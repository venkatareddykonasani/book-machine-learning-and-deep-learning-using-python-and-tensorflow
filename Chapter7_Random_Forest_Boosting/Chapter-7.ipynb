{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Chapter-7.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"969CXmyniH-7"},"source":["# Chapter-7: Random forests and Boosting"]},{"cell_type":"markdown","metadata":{"id":"0Wxb3lN5kG-F"},"source":["## Ensemble models:\n","Building several models instead of one model is called ensemble model building techniques. Ensemble models are analogous to a concept in physiology called the wisdom of crowds. If we are unsure and not able to decide the right option, then we can follow the most liked option by the\n","crowd. Given that everyone in the crowd is smart and everyone have independent opinions."]},{"cell_type":"code","metadata":{"id":"K32ZAdPQL_8C"},"source":["import pandas as pd\n","import sklearn as sk\n","import numpy as np\n","import scipy as sp\n","from sklearn  import model_selection\n","from sklearn.model_selection import train_test_split\n","from sklearn import tree\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","from sklearn.metrics import roc_curve, auc, f1_score,confusion_matrix\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","pd.set_option('display.max_columns', None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GT1wfL3KMPlK"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yujln3oP7ZLT"},"source":["github_link=\"https://raw.githubusercontent.com/venkatareddykonasani/ML_DL_py_TF/master/Chapter7_RF_Boosting/Datasets/\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FXgXUKV5k5BR"},"source":["## Bagging:\n","Bagging stands for Bootstrap Aggregating(Bootstrap Aggregating). Bagging has two steps bootstrap sampling and aggregating the classification results."]},{"cell_type":"markdown","metadata":{"id":"hAog-jOqmqaY"},"source":["**Bootstrap sampling**\n","\n","While drawing the bootstrap sample of 10 records, we do not sample all the records in one shot. We draw one sample record at a time. For example, there are ten records; we draw one record randomly, let that be record 7. We now draw one more sample, between 1 to 10, let that be 4. We now have two samples. Let us draw the third sample, let that be 7. We repeat this process ten times to create a sample of 10 records. The important point to note here is, while picking the second record, we consider all ten records of the population; this may lead to duplication of some of the data points. In a bootstrap sample, few records are repeated multiple times, and few records are never picked. The above example creates a bootstrap sample set-1. We can repeat the same process and create a bootstrap sample set-2 and more.\n","\n","The bootstrap sample is also known as a sample with replacement."]},{"cell_type":"markdown","metadata":{"id":"zqaAOP0yHOVZ"},"source":["#### Bagging Algorithm:\n","1. Draw K bootstrap samples, a higher value of K is preferred.\n","2. On each of the bootstrap samples build a model\n","3. Collate the results for the new data points based on the average for regression, maximum votes for classification models."]},{"cell_type":"markdown","metadata":{"id":"QeOntxmcIRvt"},"source":["## Random Forest\n","In the bagging algorithm, instead of building any model, if we specifically build all decision trees,\n","then it is called a Random Forest.\n","\n","**Algorithm**:\n","\n","1. Draw K bootstrap samples, a higher value of K is preferred.\n","2. On each of the bootstrap samples, build a decision tree model. While building this model, do\n","not consider all the variables for splitting\n","    \n","    a. Consider only randomly selected p-variables while splitting every node. If there are t-variables in the data then p<<t.\n","    \n","    b. Use the variable with the highest information gain out of these p-variables to split the node into child nodes\n","    c. Go to each child node and repeat the above two steps of selecting p-variables randomly and using the best variable to split the node.\n","    d. Grow the tree as long as possible without pruning\n","3. Collate the results for the new data points based on class with maximum votes."]},{"cell_type":"markdown","metadata":{"id":"jeeDa-LwVece"},"source":["**Case Study**\n","\n","The data set has data from 22 sensors and one target variable. Below are some of the basic details of\n","the data."]},{"cell_type":"code","metadata":{"id":"GaFWbOJqMaq8"},"source":["#car_train=pd.read_csv(r\"/content/drive/My Drive/DataSets/Chapter-7/datasets/car_accidents/car_sensors.csv\")\n","car_train=pd.read_csv(github_link + \"/car_accidents/car_sensors.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hwE2fBXhM5ok"},"source":["print(car_train.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LiESeWhYM8CI"},"source":["print(car_train.columns)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lt96zgcKM9zk"},"source":["print(car_train.info())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SHgmlcj4WZ3F"},"source":["From the above output, we can see that there are 33,239 records in the dataset. There are 23\n","variables in the data. The target variable name is “safe” other variables are representing the data\n","collected from 22 sensors. All the columns are numerical. We will perform basic data exploration on\n","the predictor and target variables."]},{"cell_type":"code","metadata":{"id":"eZYMkjCoNAbc"},"source":["all_cols_summary=car_train.describe()\n","print(round(all_cols_summary,2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vp3_F0dXNCqF"},"source":["print(car_train['safe'].value_counts())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oGITjZlcXt0x"},"source":["The target variable takes two values, 1-Safe, 0-Not safe. If we know the complete details about these\n","sensors, then we can perform some feature engineering tasks. We will go ahead with the model\n","building for now."]},{"cell_type":"code","metadata":{"id":"HCdidBuyNFHL"},"source":["features=car_train.columns.values[1:]\n","print(features)\n","X = car_train[features]\n","y = car_train['safe']\n","X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y ,test_size=0.2, random_state=55)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PXIOu7tjNKxE"},"source":["print(\"X_train Shape \",X_train.shape)\n","print(\"y_train Shape \", y_train.shape)\n","print(\"X_test Shape \",X_test.shape)\n","print(\"y_test Shape \", y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aTRkhgdgYuyb"},"source":["Now we will build a decision tree model on training data"]},{"cell_type":"code","metadata":{"id":"39Jel9F9NNqD"},"source":["D_tree = tree.DecisionTreeClassifier(max_depth=7)\n","D_tree.fit(X_train,y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"37fLPwAcNTAk"},"source":["tree_predict1=D_tree.predict(X_train)\n","cm1 = confusion_matrix(y_train,tree_predict1)\n","accuracy_train=(cm1[0,0]+cm1[1,1])/sum(sum(cm1))\n","print(\"Decison Tree Accuracy on Train data = \", round(accuracy_train,2) )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wqsak8JKNdjj"},"source":["tree_predict2=D_tree.predict(X_test)\n","cm2 = confusion_matrix(y_test,tree_predict2)\n","accuracy_test=(cm2[0,0]+cm2[1,1])/sum(sum(cm2))\n","print(\"Decison Tree Accuracy on Test data = \", round(accuracy_test,2) )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xln2IMZJNi_t"},"source":["false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, tree_predict1)\n","auc_train = auc(false_positive_rate, true_positive_rate)\n","print(\"Decison Tree AUC on Train data = \", round(auc_train,2) )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c1xqI9s1Nltv"},"source":["false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, tree_predict2)\n","auc_test = auc(false_positive_rate, true_positive_rate)\n","print(\"Decison Tree AUC on Test data = \", round(auc_test,2) )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J95Ju75XZs6t"},"source":["After many trials, we will arrive at the optimal max_depth for this data, which is max_depth=7.\n","The best decision tree gives us 88% accuracy and AUC of 87%. If we try a higher max_depth then we\n","will be getting into overfitting zone. We will now build a random forest model. The two important\n","hyperparameters are the number of trees and the number of features. We can set the max_depth to\n","a fixed number, say 10 in this case."]},{"cell_type":"code","metadata":{"id":"EvnsVNRGNpP6"},"source":["R_forest=RandomForestClassifier(n_estimators=300, max_features=4, max_depth=10)\n","R_forest.fit(X_train,y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7U5TgvPNcEFB"},"source":["**Code Description**\n","\n","* n_estimators = 300. We are building 300 trees here. A higher number is preferred. If the dataset size is large, then it can be a smaller number. We can try 100-500 and choose the optimal value.\n","* max_features=4 . The number of features randomly chosen. A lower number is preferred. We can try 3,4,5\n","* max_depth=10. We can fix this at a slightly higher value as compared to a standard decision tree. If we try a lower value for this, we will get very little accuracy on train and test data."]},{"cell_type":"code","metadata":{"id":"Fh8SOYF0Nsul"},"source":["forest_predict1=R_forest.predict(X_train)\n","cm1 = confusion_matrix(y_train,forest_predict1)\n","accuracy_train=(cm1[0,0]+cm1[1,1])/sum(sum(cm1))\n","print(\"Random Forest Accuracy on Train data = \", round(accuracy_train,2) )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SpO09dVTNyGR"},"source":["forest_predict2=R_forest.predict(X_test)\n","cm2 = confusion_matrix(y_test,forest_predict2)\n","accuracy_test=(cm2[0,0]+cm2[1,1])/sum(sum(cm2))\n","print(\"Random Forest Accuracy on Test data = \", round(accuracy_test,2) )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TTQtm-omN1ZD"},"source":["false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, forest_predict1)\n","auc_train = auc(false_positive_rate, true_positive_rate)\n","print(\"Random Forest AUC on Train data =  \", round(auc_train,2) )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tGXwIOvkN5RW"},"source":["false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, forest_predict2)\n","auc_test= auc(false_positive_rate, true_positive_rate)\n","print(\"Random Forest AUC on Test data =  \", round(auc_test,2) )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uRDZiSR5b9NL"},"source":["The Random Forest gives us 91% accuracy and AUC of 90% on the test data. When compared to a\n","single decision tree, we can see an improvement of 3%. When compared to a decision tree, we\n","generally observe an improvement of 1% to 5%."]},{"cell_type":"markdown","metadata":{"id":"JXjQyhQPmL4u"},"source":["## Boosting\n","In Bagging, we built multiple models in parallel. In Boosting, also we build various weak models and\n","combine them to form a robust model. Nevertheless, in Boosting, we build them sequentially, that is\n","the main difference between Bagging and Boosting. If we think bagging as the wisdom of crowds,\n","then boosting is the wisdom of crowds with some weight given to individuals based on their skill."]},{"cell_type":"markdown","metadata":{"id":"h64hVAiTo6JG"},"source":["### Ada Boosting Algorithm\n","\n","**Step-1:** Data and Weak Classifier\n","\n","**Step-2:** Error calculation and Weighted Sample\n","\n","**Step-3:** Rebuild and Repeat\n","\n","**Step-4:** Stopping Criteria"]},{"cell_type":"markdown","metadata":{"id":"oK7sDDPBsf9m"},"source":["### Gradient Boosting\n","Gradient Boosting algorithm can be easily understood with Regression. We take the whole training\n","data. We build the first regression model. This model may not be perfect. We will take the\n","predictions from this model to calculate the errors. We will build a new model that can exclusively\n","learn these errors. We will recalculate the error and repeat this process.\n","\n","**Gradient Boosting Algorithm**\n","\n","**Step-1:** Initial Model\n","\n","**Step-2:** Residuals Calculation\n","\n","**Step-3:** Build Model on Residuals\n","\n","**Step-4:** Update the Residuals and Update the Model\n","\n","**Step-5:** Stopping Criterion "]},{"cell_type":"markdown","metadata":{"id":"UimmABq0sf2-"},"source":["#### Hyperparameters in boosting\n","* **number of iterations(n)**: Boosting is an iterative algorithm. The error reduces in each iteration. The number of iterations is the first hyperparameter in boosting. A large number will lead to overfitting, and a minimal number will lead to underfitting.\n","\n","* **shrinkage or learning rate**: Instead of considering the predictions as it is from the residual models, we shrink them by factor . Shrinkage or learning rate parameter makes the whole learning process very slow. The error reduction will be very less in each iteration.\n","\n","* **Size of the tree**: While solving practical problems, it is always preferable to make the trees\n","learn slowly. The real power of the ensemble is in building weak models and collating them.\n","To make the individual trees as weak learners, we need to set the max_depth of each tree as\n","a small value."]},{"cell_type":"markdown","metadata":{"id":"XuUhGArB3bNT"},"source":["We will use a dataset to understand the boosting and learning rate parameter in-depth. Pet\n","adoption data has two columns. One is the age of the customer and the target column. The target\n","column has two classes 0’s and 1’s. 0- Not adopted the pet and 1- Adopted the pet. We will try using\n","the GBM model to predict whether the customer’s likelihood to adopt a pet based on their age."]},{"cell_type":"code","metadata":{"id":"yilnmKHWN8PH"},"source":["import pandas as pd\n","#pets_data = pd.read_csv(r\"/content/drive/My Drive/DataSets/Chapter-7/datasets/Pet_adoption/adoption.csv\")\n","pets_data = pd.read_csv(github_link+\"/Pet_adoption/adoption.csv\")\n","pets_data.columns.values\n","pets_data.head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UsuuLwK3ODwv"},"source":["X=pets_data[[\"cust_age\"]]\n","y=h=pets_data['adopted_pet']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eLsx1N1pOIUE"},"source":["for i in range (1,21):\n","    \n","    #Model and predictions \n","    boost_model=GradientBoostingClassifier(n_estimators=i,learning_rate=1, max_depth=1)\n","    boost_model.fit(X,y)\n","    pets_data[\"itaration_result\"]=boost_model.predict_proba(X)[:,1]\n","    boost_predict= boost_model.predict(X)\n","    \n","    #Graph\n","    fig = plt.figure()\n","    plt.rcParams[\"figure.figsize\"] = (7,5)\n","    plt.title(['Iteration :', i ], fontsize=20)\n","    ax1 = fig.add_subplot(111)\n","    ax1.scatter(pets_data[\"cust_age\"],pets_data[\"adopted_pet\"], s=50, c='b', marker=\"x\")\n","    ax1.scatter(pets_data[\"cust_age\"],pets_data[\"itaration_result\"], s=50, c='r', marker=\"o\")\n","    ax1.set_xlabel('cust_age')\n","    ax1.set_ylabel('adopted_pet')\n","    \n","    #SSE and Accuracy\n","    print(\"SSE : \", sum((pets_data[\"itaration_result\"] - y)**2))\n","    accuracy=f1_score(y, boost_predict, average='micro')\n","    print(\"Accuracy : \", accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r7Xg_k9D6kRd"},"source":["In the above output, we can see the actual values are shown as ‘x’ and predicted values are with ‘o’.\n","In the first iteration, there was much error. Slowly by the end of the 20 th iteration, the predictions\n","have moved almost on top of the actual values. If we try a lower learning rate then by the end of the\n","20 th iteration, we will still have much error left."]},{"cell_type":"code","metadata":{"id":"TYrzG2GbONQr"},"source":["for i in range (1,102):\n","    \n","    #Model and predictions \n","    boost_model=GradientBoostingClassifier(n_estimators=i,learning_rate=0.1, max_depth=1)\n","    boost_model.fit(X,y)\n","    pets_data[\"itaration_result\"]=boost_model.predict_proba(X)[:,1]\n","    boost_predict= boost_model.predict(X)\n","    \n","    #Graph\n","    if(np.mod(i, 10) ==1):\n","        fig = plt.figure()\n","        plt.rcParams[\"figure.figsize\"] = (7,5)\n","        plt.title(['learning_rate=0.1', 'Iteration :', i ], fontsize=20)\n","        ax1 = fig.add_subplot(111)\n","        ax1.scatter(pets_data[\"cust_age\"],pets_data[\"adopted_pet\"], s=50, c='b', marker=\"x\")\n","        ax1.scatter(pets_data[\"cust_age\"],pets_data[\"itaration_result\"], s=50, c='r', marker=\"o\")\n","        ax1.set_xlabel('cust_age')\n","        ax1.set_ylabel('adopted_pet')\n","        \n","    #SSE and Accuracy\n","    print(\"SSE : \", sum((pets_data[\"itaration_result\"] - y)**2))\n","    accuracy=f1_score(y, boost_predict, average='micro')\n","    print(\"Accuracy : \", accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GTr3pxZr61vg"},"source":["From the above output, we can see the impact of the learning rate parameter. The error reduction\n","that we could achieve in ten steps with a learning rate of 1 almost took 100 iterations with a learning\n","rate of 0.1. In general, a slow learning model with a lower learning rate is preferred."]},{"cell_type":"markdown","metadata":{"id":"95r0wilnozw7"},"source":["**Case Study- Income Prediction from Census Data**\n","\n"]},{"cell_type":"code","metadata":{"id":"ROF_WhSyOw9u"},"source":["#income = pd.read_csv(r\"/content/drive/My Drive/DataSets/Chapter-7/datasets/Adult_Census_Income/Adult_Income.csv\")\n","income = pd.read_csv(github_link+\"/Adult_Census_Income/Adult_Income.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ba-HeoWk_7mp"},"source":["print(income.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5xFXhTa5_-bM"},"source":["print(income.columns)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h8CQWhNkAAag"},"source":["print(income.info())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fDsMPreVACJm"},"source":["all_cols_summary=income.describe()\n","print(round(all_cols_summary,2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XVt4JSckAE9K"},"source":["categorical_vars=income.select_dtypes(include=['object']).columns\n","print(categorical_vars)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AjUejQJ6AHad"},"source":["for col in categorical_vars:\n","    print(\"\\n\\nFrequency Table for the column \", col )\n","    print(income[col].value_counts())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mdCqBL37AKt7"},"source":["income[\"workclass\"] = income[\"workclass\"].replace(['?','Never-worked','Without-pay'], 'Other')  \n","print(income[\"workclass\"] .value_counts())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oQHz8oAnANrG"},"source":["income[\"marital.status\"] = income[\"marital.status\"].replace(['Never-married','Divorced','Separated','Widowed'], 'Not-married')\n","print(income[\"marital.status\"] .value_counts())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fLoUZ9FKAPoo"},"source":["income[\"occupation\"] = income[\"occupation\"].replace(['?'], 'Other-service')  \n","print(income[\"occupation\"] .value_counts())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v4AwyxkZARpi"},"source":["freq_country=income[\"native.country\"].value_counts()\n","less_frequent= freq_country[freq_country <100].index\n","print(less_frequent)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RHECqmIqATwa"},"source":["income[\"native.country\"]=income[\"native.country\"].replace([less_frequent], 'Other')\n","income[\"native.country\"] = income[\"native.country\"].replace(['?'], 'Other')  \n","print(income[\"native.country\"].value_counts())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mbzYmi4dAV0B"},"source":["print(income[\"sex\"].value_counts())\n","income['sex']=income['sex'].map({'Male': 0, 'Female': 1})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dg6Um_fKAZYj"},"source":["print(income[\"income\"].value_counts())\n","income['income']=income['income'].map({'<=50K': 0, '>50K': 1})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q4ndtlXRAdzc"},"source":["one_hot_cols=['workclass','marital.status','occupation','native.country']\n","one_hot_data = pd.get_dummies(income[one_hot_cols])\n","print(one_hot_data.shape)\n","print(one_hot_data.columns.values)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jWeMM8VXAfr8"},"source":["print(income.shape)\n","income_final = pd.concat([income, one_hot_data], axis=1)\n","print(income_final.shape)\n","print(income_final.info())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EeLcHEXiAkeH"},"source":["one_hot_features=list(one_hot_data.columns.values)\n","numerical_features=['age',  'education.num', 'sex', 'capital.gain', 'capital.loss', 'hours.per.week']\n","all_features=one_hot_features+numerical_features\n","print(all_features)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nh-dSmP4AmtB"},"source":["X=income_final[all_features]\n","y=income_final['income']\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ycamCUrApji"},"source":["print(X_train.shape)\n","print(y_train.shape)\n","print(X_test.shape)\n","print(y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k0nyccW6ArsO"},"source":["gbm_model1 = GradientBoostingClassifier(learning_rate=0.01, max_depth=4,  n_estimators=100, verbose=1)\n","gbm_model1.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NbjQKlAuAuzb"},"source":["predictions=gbm_model1.predict(X_train)\n","actuals=y_train\n","cm = confusion_matrix(actuals,predictions)\n","print(\"Confusion Matrix on Train data\\n\", cm)\n","accuracy=(cm[0,0]+cm[1,1])/(sum(sum(cm)))\n","print(\"Train Accuracy\", accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xu9WBrNEAyAb"},"source":["predictions=gbm_model1.predict(X_test)\n","actuals=y_test\n","cm = confusion_matrix(actuals,predictions)\n","print(\"Confusion Matrix on Test data\\n\", cm)\n","accuracy=(cm[0,0]+cm[1,1])/(sum(sum(cm)))\n","print(\"Test Accuracy\", accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZTJclSgdA0nC"},"source":["for i in range(5,1000, 50):\n","    gbm_model1 = GradientBoostingClassifier(learning_rate=0.01, max_depth=4,  n_estimators=i)\n","    gbm_model1.fit(X_train, y_train)\n","    \n","    print(\"N_estimators=\" , i)\n","    #Train data\n","    predictions=gbm_model1.predict(X_train)\n","    actuals=y_train\n","    cm = confusion_matrix(actuals,predictions)\n","    accuracy=(cm[0,0]+cm[1,1])/(sum(sum(cm)))\n","    print(\"Train Accuracy\", accuracy)\n","\n","    #Test data\n","    predictions=gbm_model1.predict(X_test)\n","    actuals=y_test\n","    cm = confusion_matrix(actuals,predictions)\n","    accuracy=(cm[0,0]+cm[1,1])/(sum(sum(cm)))\n","    print(\"Test Accuracy\", accuracy)"],"execution_count":null,"outputs":[]}]}