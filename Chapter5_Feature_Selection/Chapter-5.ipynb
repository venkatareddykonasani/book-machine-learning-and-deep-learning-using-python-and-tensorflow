{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Chapter-5.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"bpKfBMOVJGTP"},"source":["# Chapter-5: Model Selection and Cross-Validation"]},{"cell_type":"markdown","metadata":{"id":"deT8QmAjfN-B"},"source":["## Steps in model building\n","1. Defining the objectives\n","2. Explore, validate and Prepare the data\n","3. Build the model \n","4. Validate the model\n","5. Deploy the model\n"]},{"cell_type":"markdown","metadata":{"id":"ZtxO5DBVfkZs"},"source":["Importing all the required packages and libraries"]},{"cell_type":"code","metadata":{"id":"A8fY0rRWI96f"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import math\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import r2_score\n","from sklearn import  linear_model\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import explained_variance_score\n","import statsmodels.api as sm\n","from matplotlib.pyplot import plot \n","pd.set_option('display.max_columns', None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1853A3kBLQ0Y"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-waBm-bMzKFd"},"source":["git_hub_path=\"https://raw.githubusercontent.com/venkatareddykonasani/ML_DL_py_TF/master/Chapter5_Model_Selection_Feature_engg/Datasets/\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H5iJMW0EgBwg"},"source":["## Model validating measures"]},{"cell_type":"markdown","metadata":{"id":"ok49dzaWgKRy"},"source":["### 1.Regression\n","* Mean Absolute Deviation -MAD\n","* Mean Absolute Percentage Error - MAPE\n","* Root Mean Squared Error - RMSE\n","\n","We will see these measures one by one "]},{"cell_type":"markdown","metadata":{"id":"_oBl2vmvjEav"},"source":["Firstly we will import the data and find the basic details of the data.\n","\n","This dataset contains house sale prices for King County, which includes Seattle. The data contains details of all houses sold between May 2014 to May 2015. It is available under License CC0: Public Domain. We want to predict the house price using features like the number of bedrooms, the number of bathrooms, age of construction, square feet area, location of the house, etc."]},{"cell_type":"code","metadata":{"id":"lqxBzP_uLIEh"},"source":["#kc_house_data = pd.read_csv(r'/content/drive/My Drive/DataSets/Chapter-5/datasets/kc_house_data/kc_house_data.csv')\n","kc_house_data = pd.read_csv(git_hub_path+ \"/kc_house_data/kc_house_data.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X7BkqrW1LmOW"},"source":["print(kc_house_data.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KadRXj6sLpcd"},"source":["print(kc_house_data.columns)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kh8OqxYCkFI3"},"source":["There are 21,613 rows and 21 columns in the dataset. The column names are self-explanatory. The columns try to describe the home properties like bedrooms, bathrooms, living room area, number of floors, year of construction. The target variable that we are trying to predict here is “price.” "]},{"cell_type":"code","metadata":{"id":"gM51ENDOLrtB"},"source":["print(kc_house_data.dtypes)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xN_iWlcRHhTD"},"source":["All the variables are integers except for the date variable. We will use the rest of all variables for building our model. We will keep aside the date variable"]},{"cell_type":"code","metadata":{"id":"Ovp-2P9OLuMc"},"source":["kc_house_data.info()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ezt4CJipHx0D"},"source":["All the columns have data populated. None of the columns has missing values. Now we will go though the summary of each column"]},{"cell_type":"code","metadata":{"id":"Yk8xfkU4LxXk"},"source":["all_cols_summary=kc_house_data.describe()\n","print(round(all_cols_summary,2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n3-92pgbIHFF"},"source":["Overall the data seem to be in good shape. There are a few columns with outliers.  We will go-ahead with model building.\n","\n","* Till now, we have used the “statsmodels” package to perform regression tasks. We will try to use an alternative package in this exercise. The syntax will be different, and the results will be the same.  \n","* A lot of data scientists use the “sklearn” package. We will try to understand the “sklearn” package as well. The interpretation of R-squared and all the related regression measures remain the same. Only syntax changes.\n","* The summary() function is available in statsmodels package. That is why we started with that package. However, there is no summary function in sklearn package. Now we know what values are important in the regression output. We can fetch all those values from the model object individually in this package.\n","\n","First, we will write the code to create train data and test data\n","\n"]},{"cell_type":"code","metadata":{"id":"l2GjhXpJMKc5"},"source":["X = kc_house_data[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15']]\n","\n","y = kc_house_data['price']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-6bSHVN4NC0U"},"source":["from sklearn  import model_selection\n","X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y ,test_size=0.2, random_state=55)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C-jcRdwjNJVd"},"source":["print(X_train.shape)\n","print(y_train.shape)\n","print(X_test.shape)\n","print(y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1ZaKjGjiLztQ"},"source":["80% of the overall data is considered for training, and the rest of the data is considered for testing. Now we are ready to build the model. Below is the code for building the model using sklearn package. "]},{"cell_type":"code","metadata":{"id":"1mFed9GHN8iv"},"source":["import sklearn \n","model_1 = sklearn.linear_model.LinearRegression()\n","model_1.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WZpYu2g7T5G-"},"source":["When we execute the above code, the model will be fit and stored in model_1. There is no summary() function in this package. We can fetch the coefficients and r-squared values using the below code. "]},{"cell_type":"code","metadata":{"id":"ZUurGxBNOBZY"},"source":["print(model_1.intercept_)\n","print(model_1.coef_)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rpqj3Ku4cf3d"},"source":["#### R-squarred\n","R-Squared value is a good measure. It talks about the overall accuracy of the model and the total variance explained by the model. If we want to get an intuitive idea of how close or far away from the predicted values from the actual values, then we can look at these measures.  "]},{"cell_type":"code","metadata":{"id":"fpiTGd9yQ8Os"},"source":["from sklearn import metrics\n","y_pred_train=model_1.predict(X_train)\n","print(metrics.r2_score(y_train,y_pred_train))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kzHIwGedRkwr"},"source":["y_pred_test=model_1.predict(X_test)\n","print(metrics.r2_score(y_test,y_pred_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yuAflDF5Y67j"},"source":["Intercept is a large value. Mainly due to the scale of the target. The rest of the coefficients are shown in scientific number format. "]},{"cell_type":"code","metadata":{"id":"1aCkIhueRnlT"},"source":["round(kc_house_data.price.describe())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"avPoeM-3Rp-E"},"source":["print(\"R-Squared on Train data : \", metrics.r2_score(y_train,y_pred_train))\n","print(\"R-Squared on Test data : \", metrics.r2_score(y_test,y_pred_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"seurFUI7ccA5"},"source":["The R-squared value of the model on train data is 0.700 i.e., 70%, and the R-squared value on the test data is 0.696. i.e. 69.6% . "]},{"cell_type":"markdown","metadata":{"id":"nVKKiEv7eLlj"},"source":["#### Mean Absolute Deviation(MAD)\n","\n","MAD = $\\sum_{i=1}^{n}\\frac{|y_i-y|}{n}$\n","* Find the deviation of predicted value from the actual value. Both negative and positive deviations are errors. Calculate the absolute deviation.\n","* The average of all these absolute deviations is known as Mean Absolute Deviation – MAD. \n","* Calculate MAD on train and Test data. \n","* A good model will have near to zero MAD on train and test data.\n","* MAD gives tries to tell us the average deviation of predictions from the actual values. \n","* While working with MAD, we are not sure if a value of 1000 is higher or lower, unless we see the scale of target variable y.  If y is in millions, then MAD of 1000 is very less if y value is in thousands then MAD of 1000 is considered high\n"," "]},{"cell_type":"code","metadata":{"id":"NzzGtggERwZS"},"source":["print(\"MAD on Train data : \", round(np.mean(np.abs(y_train - y_pred_train)),2))\n","print(\"MAD on Test data : \", round(np.mean(np.abs(y_test - y_pred_test)),2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ylcNs5M8gmWv"},"source":["#### Mean Absolute Percentage Error(MAPE)\n","\n","MAPE = $\\frac{100}{n}$$\\sum_{i=1}^{n}\\frac{|y_i-y|}{n}$\n","* We tweak the MAD formula and convert each deviation into the percentage of actual value. \n","* If we are interested in knowing the deviation percentage instead of actual deviation, then we can use MAPE.\n","* In MAPE, we don’t need to worry about the scale of the variable. A MAPE value of 2%, is always lower than MAPE of 10%, no matter what is the scale of Y. \n"]},{"cell_type":"code","metadata":{"id":"Y9V8R-0QRyzx"},"source":["print(\"MAPE on Train data : \", round(np.mean(np.abs(y_train - y_pred_train)/y_train),2))\n","print(\"MAPE on Test data : \", round(np.mean(np.abs(y_test - y_pred_test)/y_test),2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3BhuezFshImq"},"source":["#### Root Mean Squared Error(RMSE)\n","\n","RMSE = $\\sqrt(\\sum_{i=1}^{n}\\frac{(y_i-y)^2}{n})$\n","\n","* Root mean squared error is another alternative. \n","* All these measures are trying to explain the error using different formulas. \n","* When we are comparing two modes, A lower value of RMSE is preferred. "]},{"cell_type":"code","metadata":{"id":"up-rehM8R1Vr"},"source":["print(\"RMSE on Train data : \", round(math.sqrt(np.mean(np.abs(y_train - y_pred_train)**2)),2))\n","print(\"RMSE on Test data : \", round(math.sqrt(np.mean(np.abs(y_test - y_pred_test)**2)),2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dmRbsMsfhzAi"},"source":["Here from above results we can note few things. The above model is just a benchmark model. Here we have not cleaned the data for outliers. We have not put any effort into improving the model. We have used some variables like zipcode as it is. If we use data cleaning and feature engineering techniques, then we can improve the overall accuracy of the model with the same data and with the same model building algorithm. "]},{"cell_type":"markdown","metadata":{"id":"TK_VcHc3imD0"},"source":["### 2 . Classification\n","While reading with regression models, we have used R-squared and other deviation based validation measures. When it comes to classification, we have 0 and 1 in the output. The deviation value actual-predicted may not work here. We will create a confusion matrix and derive accuracy from actual and predicted classes. "]},{"cell_type":"markdown","metadata":{"id":"2Zwp462tjJgX"},"source":["We will first import the data and find the basic details about the data.  This dataset is created from the “Give me some credit” competition on the kaggle.com website.  A bank wants to predict a customer is a good or bad customer. The bank has collected two years of historical data. We will build a model on this historical data and use it for predicting the defaults in the new data."]},{"cell_type":"code","metadata":{"id":"CORA-C_uR4Q9"},"source":["import pandas as pd\n","#credit_risk_data = pd.read_csv(r'/content/drive/My Drive/DataSets/Chapter-5/datasets/loans_data/credit_risk_data_v1.csv')\n","credit_risk_data = pd.read_csv(git_hub_path+\"/loans_data/credit_risk_data_v1.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SBwdfZrqShk2"},"source":["print(credit_risk_data.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JbftpyDqSk6X"},"source":["print(credit_risk_data.columns)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ilgqmu8HSnGE"},"source":["print(credit_risk_data.dtypes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JP2yOLFTSqa7"},"source":["credit_risk_data.info()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lbGS9PL5hgNo"},"source":["From the output, we can see that there are 150,008 records and eight columns. The columns are self-explanatory. All the columns are numerical. Below table quickly explains the columns\n","\n","Column_name | Description\n","--- | ---\n","Cust_num | Customer id or number\n","Bad | Bad indicator. Target variable. Defaulters are denoted with 1\n","Credit_Limit | The credit limit on their card. \n","Late_Payments_Count | Number of times customer was late in paying the bill \n","Card_Utilization_Percent | Customer credit line average utilization \n","Age | Age of the customer\n","Debt_to_income_ratio | Debt to income ratio\n","Monthly_Income | Monthly income\n","Num_loans_personal_loans | Number of personal loans\n","Family_dependents | Number of dependents\n"]},{"cell_type":"code","metadata":{"id":"YL4XaeCAS85m"},"source":["pd.set_option('display.max_columns', None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E4m_ZcfIS_u9"},"source":["all_cols_summary=credit_risk_data.describe()\n","print(round(all_cols_summary,2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AIrheeoMlASy"},"source":["This data is in good shape. No missing values. No noticeable outliers. We will directly go ahead with model building. Before that, we will create a train and test data. "]},{"cell_type":"code","metadata":{"id":"D4LjYwhBTCs1"},"source":["X = credit_risk_data[['Credit_Limit', 'Late_Payments_Count',\n","       'Card_Utilization_Percent', 'Age', 'Debt_to_income_ratio',\n","       'Monthly_Income', 'Num_loans_personal_loans', 'Family_dependents']]\n","\n","y = credit_risk_data['Bad']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_2HTp0CXTMPK"},"source":["from sklearn  import model_selection\n","X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y ,test_size=0.2, random_state=55)\n","\n","print(X_train.shape)\n","print(y_train.shape)\n","print(X_test.shape)\n","print(y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FxSaNw2cnpbU"},"source":["Now we will build the logistic regression model and find the accurcy of the model "]},{"cell_type":"code","metadata":{"id":"StP4hIxxTRFj"},"source":["from sklearn.linear_model import LogisticRegression\n","model_2= LogisticRegression()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gYQgpQUHTWYo"},"source":["model_2.fit(X_train,y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e6DkNPNJTYV3"},"source":["print(model_2.intercept_)\n","print(model_2.coef_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s72CALgNTbXS"},"source":["from sklearn.metrics import confusion_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9OaMdkwYTdzX"},"source":["y_pred_train=model_2.predict(X_train)\n","cm1 = confusion_matrix(y_train,y_pred_train)\n","print(cm1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dvbX7PoETga6"},"source":["accuracy1=(cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n","print(accuracy1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mi3LCJs8TjYK"},"source":["y_pred_test=model_2.predict(X_test)\n","cm2 = confusion_matrix(y_test,y_pred_test)\n","print(cm2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tjSAjfRcToKv"},"source":["accuracy2=(cm2[0,0]+cm2[1,1])/(cm2[0,0]+cm2[0,1]+cm2[1,0]+cm2[1,1])\n","print(accuracy2)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OwvgjN3VoWLG"},"source":["We can see similar results on the train and test data. Overall accuracy is 93% on train data and test data. If we look at only accuracy, then the model is good. Is accuracy a sufficient measure here? \n","\n","As discussed earlier, in the case of credit risk models, accuracy may not be the right measure. One bad customer is not the same as one good customer. In this data, more than 90% are good customers; less than 10% are bad customers. We can confirm the same by looking at the frequency of good customers(0’s) and bad customers(1’s) in the data. \n"]},{"cell_type":"code","metadata":{"id":"FI71IaK1TqtG"},"source":["credit_risk_data['Bad'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hq_PjWABo1eU"},"source":["Out of 150,000 records, we can see that the number of 1’s is less than 15,000 which means class-1 percentage is less than 10%. We can see the class imbalance. In this example, bad customers are denoted with class-1. We need to focus on class-1 accuracy instead of overall accuracy."]},{"cell_type":"markdown","metadata":{"id":"wv-OReNMo3vj"},"source":["#### Sensitivity\n","\n","Sensitivity is the accuracy of the first class. We usually denote it with a class-0 or positive class. A model has high sensitivity when it has predicted many records related to class-0 accurately. \n","\n","Sensitivity = $\\frac{Number~of~times~0~is~predicted~as~0}{Overall~occurances~of~0}$\n","\n","Sensitivity=$\\frac{cm[0,0]}{(cm[0,0]+cm[0,1])}$\n","\n","Sensitivity=$\\frac{True~Positives(TP)}{True~Positives(TP)+ Flase~Negatives(FN)}$"]},{"cell_type":"markdown","metadata":{"id":"kILpPVpf1kxP"},"source":["#### Specificity\n","The accuracy of the second class, i.e., class-1 accuracy, is specificity. Out of all records in class-1, how many times our model has predicted them correctly. \n","\n","Specificity = $\\frac{Number~of~times~1~is~predicted~as~1}{Overall~occurances~of~1}$\n","\n","Specificity = $\\frac{cm[1,1]}{cm[1,0]+cm[1,1]}$\n","\n","Specificity = $\\frac{True~Negatives(TN)}{False~Positives(FP)+True~Negatives(TN)}$\n"]},{"cell_type":"markdown","metadata":{"id":"4Q018RMK2dfO"},"source":["***Example:***\n","\n","Let us look at this below confusion matrix. We are trying to predict whether a customer is a good or bad customer before giving him a personal loan. Bad customers are known as defaulters and good customers are non-defaulters. These models are known as credit risk models\n","\n","\n","\n",". | 0 – Bad customer | 1 – Good Customer | Class-wise Accuracy\n","---|---|---|---\n","0 – Bad customer | Model is predicting bad customer as bad | Model is predicting bad customer as good | Sensitivity\n","1 – Good Customer | Model is predicting good customer as bad | Model is predicting good customer as good | Specificity\n","\n","In the above matrix, what is important for us? We are not worried about the diagonal elements.  The model predicting bad customers as bad and predicting good customers as good are the right predictions. There are two types of errors here. The model predicting bad customers as good customers is one type of error. The second type of error is the model predicting good customers as bad customers. Let us look at the business implications of all these cells.\n","\n",". | 0 – Bad customer | 1 – Good Customer | Class-wise Accuracy\n","---|---|---|---\n","0 – Bad customer | Reject the loan | Approve the loan | Sensitivity\n","1 – Good Customer | Reject the loan | Approve the loan | Specificity\n"]},{"cell_type":"code","metadata":{"id":"h3f8FMKcTtkY"},"source":["Sensitivity1=cm1[0,0]/(cm1[0,0]+cm1[0,1])\n","print(round(Sensitivity1,3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vjr7ViwbYkuq"},"source":["Specificity1=cm1[1,1]/(cm1[1,0]+cm1[1,1])\n","print(round(Specificity1,3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CS_-TLEKZUg-"},"source":["Sensitivity2=cm2[0,0]/(cm2[0,0]+cm2[0,1])\n","print(round(Sensitivity2,3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Lt2DgXXZXLF"},"source":["Specificity2=cm2[1,1]/(cm2[1,0]+cm2[1,1])\n","print(round(Specificity2,3))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fMaTXeKm6W2g"},"source":["Sensitivity on the train and test data us 99.5%., that is nearly perfect. The measure that matters is specificity. Specificity on both train and test data is very poor.  Train data has a specificity of 6.8%, and test data shows 5.7% specificity. The model needs much improvement in this class. "]},{"cell_type":"markdown","metadata":{"id":"3v4WUlpr-4k-"},"source":["How do we improve the specificity? We have already built the model. There a small trick that can help boost the specificity. We can experiment with the threshold. By default, logistic regression gives us the probability as the prediction. We then convert it into class by taking 0.5 as a threshold. If the predicted value is less than 0.5, then the predicted class is “0” else it is 1.If we do not mention any threshold, then any predicted value by default will be considered as class-0. \n","\n","In the case of class imbalance, we can lower the threshold to 0.2 or 0.25 or 0.3 to increase the chances of finding class-1. In that process, we may misclassify some of the class-0 as class-1. Since we are interested in one class, we can afford to misclassify a few records. "]},{"cell_type":"code","metadata":{"id":"OqJCQ2m1ZZIA"},"source":["y_pred_prob=model_2.predict_proba(X_train)\n","print(y_pred_prob.shape)\n","print(y_pred_prob)\n","print(y_pred_prob[0,])\n","print(y_pred_prob[0,0])\n","print(y_pred_prob[0,1])\n","print(y_pred_prob[0:5,1])\n","print(y_pred_prob[:,1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bs8HsKaGZgH8"},"source":["y_pred_prob_1=y_pred_prob[:,1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2NQsuo_OBsuE"},"source":["Now we will create the confusion matrix and recalculate sensitivity and specificity for different thresholds. "]},{"cell_type":"markdown","metadata":{"id":"by4PcH4OAE4Y"},"source":["* ***Threshold = 0.5***"]},{"cell_type":"code","metadata":{"id":"-uKQjpwFZi3e"},"source":["threshold=0.5\n","y_pred_class=y_pred_prob_1*0\n","y_pred_class[y_pred_prob_1>threshold]=1\n","print(y_pred_class)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pa9HX1ACZlm_"},"source":["cm3 = confusion_matrix(y_train,y_pred_class)\n","print(\"confusion Matrix with Threshold \",  threshold,  \"\\n\",cm3)\n","accuracy3=(cm3[0,0]+cm3[1,1])/(cm3[0,0]+cm3[0,1]+cm3[1,0]+cm3[1,1])\n","print(\"Accuracy is \", round(accuracy3,3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"80mE-dRzZwNl"},"source":["Sensitivity3=cm3[0,0]/(cm3[0,0]+cm3[0,1])\n","print(\"Sensitivity is\", round(Sensitivity3,3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o-oKxvTgZzsO"},"source":["Specificity3=cm3[1,1]/(cm3[1,0]+cm3[1,1])\n","print(\"Specificity is \", round(Specificity3,3))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i5sPPZ-2Cq-x"},"source":["We will get the same results as default settings for the threshold 0.5. We can compare these results with the train data results in the previous section, and they are the same. In this problem statement, class-1 is important for us. We need to maximize the probability of detecting class-1. We will lower the threshold and try to lift the specificity from 6.8% to a higher number. We can compromise a little on sensitivity. We use the same code, and we just need to change the threshold. "]},{"cell_type":"markdown","metadata":{"id":"vWBX_1YKCsPG"},"source":["* ***Threshold = 0.2***"]},{"cell_type":"code","metadata":{"id":"4t7o9_81Z2rf"},"source":["threshold=0.2\n","y_pred_class=y_pred_prob_1*0\n","y_pred_class[y_pred_prob_1>threshold]=1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gs6IW5u9Z6Kz"},"source":["cm3 = confusion_matrix(y_train,y_pred_class)\n","print(\"confusion Matrix with Threshold \",  threshold,  \"\\n\",cm3)\n","accuracy3=(cm3[0,0]+cm3[1,1])/(cm3[0,0]+cm3[0,1]+cm3[1,0]+cm3[1,1])\n","print(\"Accuracy is \", round(accuracy3,3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NonOMFswZ-V4"},"source":["Sensitivity3=cm3[0,0]/(cm3[0,0]+cm3[0,1])\n","print(\"Sensitivity is\", round(Sensitivity3,3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XjksrSrTaGFb"},"source":["Specificity3=cm3[1,1]/(cm3[1,0]+cm3[1,1])\n","print(\"Specificity is \", round(Specificity3,3))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6bxxS9xKC3TN"},"source":["By changing the threshold to 0.2, we have lifted specificity from 6.8% to 32.9%. Let us further reduce the threshold to 0.1"]},{"cell_type":"markdown","metadata":{"id":"n3URWPViC4zB"},"source":["* ***Threshold = 0.1***"]},{"cell_type":"code","metadata":{"id":"EhHNFCRnaN5l"},"source":["threshold=0.1\n","y_pred_class=y_pred_prob_1*0\n","y_pred_class[y_pred_prob_1>threshold]=1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LjtRLGxxaQ2N"},"source":["cm3 = confusion_matrix(y_train,y_pred_class)\n","print(\"confusion Matrix with Threshold \",  threshold,  \"\\n\",cm3)\n","accuracy3=(cm3[0,0]+cm3[1,1])/(cm3[0,0]+cm3[0,1]+cm3[1,0]+cm3[1,1])\n","print(\"Accuracy is \", round(accuracy3,3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VDUbpQ-maTZm"},"source":["Sensitivity3=cm3[0,0]/(cm3[0,0]+cm3[0,1])\n","print(\"Sensitivity is\", round(Sensitivity3,3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"umSTnh1naXol"},"source":["Specificity3=cm3[1,1]/(cm3[1,0]+cm3[1,1])\n","print(\"Specificity is \", round(Specificity3,3))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vJzETp8nDdU9"},"source":["By changing the threshold to 0.1, we have further lifted specificity from 6.8% to 59.6%. However, sensitivity decreased from 99.5% to 79.7%. If we further reduce threshold specificity will further increase, but sensitivity will decrease. We will be losing much business if we keep on classifying good customers as bad customers. There has to be a tradeoff between sensitivity and specificity."]},{"cell_type":"markdown","metadata":{"id":"nMirIqwDDg1N"},"source":["* ***Threshold = 0.01***"]},{"cell_type":"code","metadata":{"id":"0KKymOrhaapL"},"source":["threshold=0.01\n","y_pred_class=y_pred_prob_1*0\n","y_pred_class[y_pred_prob_1>threshold]=1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bhz5eMQHaeLL"},"source":["cm3 = confusion_matrix(y_train,y_pred_class)\n","print(\"confusion Matrix with Threshold \",  threshold,  \"\\n\",cm3)\n","accuracy3=(cm3[0,0]+cm3[1,1])/(cm3[0,0]+cm3[0,1]+cm3[1,0]+cm3[1,1])\n","print(\"Accuracy is \", round(accuracy3,3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nMlo0FvrajEb"},"source":["Sensitivity3=cm3[0,0]/(cm3[0,0]+cm3[0,1])\n","print(\"Sensitivity is\", round(Sensitivity3,3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pslpS8fdaoPr"},"source":["Specificity3=cm3[1,1]/(cm3[1,0]+cm3[1,1])\n","print(\"Specificity is \", round(Specificity3,3))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4ciy2CE2Dnf0"},"source":["By changing the threshold to 0.01, we have further lifted specificity from 6.8% to 97.8%. However, sensitivity decreased from 99.5% to 13.1%. We will be losing much business if we keep on classifying good customers as bad customers. There has to be a tradeoff between sensitivity and specificity. In the next section, we will see how to choose the optimal threshold where we are satisfied with both sensitivity and specificity. "]},{"cell_type":"markdown","metadata":{"id":"gD_yvVDjDyOl"},"source":["#### ROC and AUC\n","In some cases, sensitivity is important, and in some cases, specificity is important. By lowering the threshold, we can increase the specificity. We have seen that in the previous example. Similarly, by increasing the threshold, we can increase the sensitivity. The important question is when we lower the threshold, specificity increases; at the same time, sensitivity decreases. While we are focusing on increasing the accuracy of one class, the other class accuracy decreases. Is there a risk in it? \n","\n","\n","Sensitivity and specificity move in opposite directions. If one increases, the other decreases. First, we decide which one of these two is our priority. We will try to maximize that at the same time, we will try to minimize the loss associated with the other one. ROC curve helps us in choosing that optimal pair of sensitivity and specificity. \n"]},{"cell_type":"code","metadata":{"id":"-lZypgLpaquZ"},"source":["from sklearn.metrics import roc_curve, auc\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MlQxWggPavIi"},"source":["false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, y_pred_prob_1)\n","plt.title('ROC Curve')\n","plt.plot(false_positive_rate, true_positive_rate)\n","plt.plot([0,1],[0,1],'r--')\n","plt.ylabel('True Positive Rate(Sensitivity)')\n","plt.xlabel('False Positive Rate(1-Specificity)')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jBITNgkKazFS"},"source":["auc = auc(false_positive_rate, true_positive_rate)\n","print(auc)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LJkJYgzRFnu4"},"source":["ROC curve is created by taking different sensitivity on Y-axis and 1-Specificity on X-axis. \n","ROC curve stands for the Receiver Operating Characteristic curve. The dotted line in the middle is the no-discrimination line. On this line, both sensitivity and 1- specificity is equal. This line is created from a model that gives a 50-50 chance to both the classes. \n","\n","**AUC**\n"," is a better validation measure in case of class imbalance. We know that accuracy changes when threshold value changes. AUC is calculated from the ROC curve, and it considers all the threshold values. If AUC value is near to 1, then the model is considered to be good. \n","\n"," The important function in the above code is **roc_curve()**. It takes actual values and predicted probabilities of y as input. This function considers all the threshold values and returns a table that has threshold value, false-positive rate, and true positive rate."]},{"cell_type":"markdown","metadata":{"id":"81W0ZjaNIrwF"},"source":["#### F1 Score\n","F1 score can be considered as an extension of sensitivity and specificity. In Sensitivity, we tend to focus on a single class. The F1 score is also calculated for individual classes. \n","\n","Sensitivity is the True Positive Rate. It is also known as recall. Out of all the records in the positive class, how many are predicted correctly. If we are focusing on a single class, sensitivity helps us in predicting its probability. There is one more angle to this single class. Out of all the predicted values as positive, how many are actually positive. This is accuracy in the first column. This measure is known as precision.\n","\n","F1 score is the harmonic mean of recall and precision. The harmonic mean is a different type of average. It is preferred when we are dealing with fractions. The harmonic mean is the inverse of the arithmetic mean. \n","\n","F1 Score=harmonic mean(recall,precision)\n","\n","F1 Score=$\\frac{2}{\\frac{1}{recall}+\\frac{1}{precision}}$\n","\n","F1 Score=2*($\\frac{precision*recall}{precison+recall}$)"]},{"cell_type":"code","metadata":{"id":"YLq3JvkQa2CW"},"source":["from sklearn.metrics import f1_score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9dYBtIpoa4Pp"},"source":["threshold=0.5\n","y_pred_class=y_pred_prob_1*0\n","y_pred_class[y_pred_prob_1>threshold]=1\n","print(f1_score(y_train, y_pred_class))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"repj-iQUa7V9"},"source":["threshold=0.2\n","y_pred_class=y_pred_prob_1*0\n","y_pred_class[y_pred_prob_1>threshold]=1\n","print(f1_score(y_train, y_pred_class))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zaIeeHQqV0Ta"},"source":["### Bias-Variance tradeoff\n","* While building the models, we focus on bringing the best out of the data. We want to have the best model with high accuracy. While we concentrate a lot on increasing accuracy, we may run into two types of problems. The problem of overfitting and the problem of underfitting. \n","- The model should neither be overfitted or under fitted; in other words, the model should neither have variance nor bias. \n","+ The overall error in a model can be divided into three parts: the irreducible error, bias, and variance. Not every model will be 100% accurate. There will always be some error inherent in data that can not be reduced. That component is known as an irreducible error. Bias component happens due to underfitting, and variance component happens due to overfitting. \n","* Bias and variance move it opposite directions. If we increase the complexity of the model, then variance increases and bias reduces. If we decrease the complexity, then variance decreases and bias increases. To reduce Bias and Variance, we need to build models with optimal complexity. \n"]},{"cell_type":"markdown","metadata":{"id":"sGcCGAuJWaDv"},"source":["This data set was originally shared by the National Institute of Diabetes and Digestive and Kidney Diseases. The goal is to predict whether a person has diabetes or not based on several diagnostic measurements. The diagnostic measurements are  Pregnancies,    Glucose    BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction and Age. The target variable name is “outcome,” and It takes two values 0 and 1. Class-1 indicates diabetes. Below table gives us details of all the columns\n","\n","Column Name | Details\n","--- | ---\n","Pregnancies  | Number of times pregnant\n","Glucose | Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n","blood pressure | Diastolic blood pressure (mm Hg)\n","skin thickness | Triceps skinfold thickness (mm)\n","Insulin | 2-Hour serum insulin (mu U/ml)\n","BMI | Body mass index (weight in kg/(height in m)^2)\n","DiabetesPedigreeFunction | Diabetes pedigree function\n","Age | Age (years)\n","Outcome | It takes two values 0 and 1. Class-1 indicates diabetes\n","\n","We will import this data into python and get some basic statistics before proceeding with model building."]},{"cell_type":"code","metadata":{"id":"rxPmvnyRa-aW"},"source":["#diabetes_data= pd.read_csv(r'/content/drive/My Drive/DataSets/Chapter-5/datasets/pima/diabetes.csv')\n","diabetes_data= pd.read_csv(git_hub_path+\"/pima/diabetes.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ffIrawJtbMHl"},"source":["print(diabetes_data.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TnXrAeKJbROn"},"source":["print(diabetes_data.columns)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ur_zTcGmbWPW"},"source":["print(diabetes_data.dtypes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wKOu6_JTbZbO"},"source":["diabetes_data.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nrUrZEkxbzDr"},"source":["pd.set_option('display.max_columns', None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rKwdZO5Xb20I"},"source":["all_cols_summary=diabetes_data.describe()\n","print(round(all_cols_summary,2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hFEArYiWX6g2"},"source":["From the summary, we can see that the data is clean enough to go ahead with model building. The provider already cleaned this data. We will define the train and test data."]},{"cell_type":"code","metadata":{"id":"DtIJCkf-b5Z_"},"source":["X = diabetes_data[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']]\n","y = diabetes_data[['Outcome']]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aNpYJai5b-9k"},"source":["from sklearn  import model_selection\n","X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y ,test_size=0.2, random_state=33)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hPC0oIV6cGIV"},"source":["print(X_train.shape)\n","print(y_train.shape)\n","print(X_test.shape)\n","print(y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"96yHi9WBcJgK"},"source":["from sklearn.tree import DecisionTreeClassifier\n","diabetes_tree1= DecisionTreeClassifier()\n","diabetes_tree1.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TBVzD_IAnBM0"},"source":["score() function helps us directly calculating the accuracy value. This function internally creates the confusion matrix and gives us the accuracy value. "]},{"cell_type":"code","metadata":{"id":"a0BmXG3IceCC"},"source":["print(\"Max Depth = None\")\n","print(\"Train data Accuracy\", diabetes_tree1.score(X_train, y_train))\n","print(\"Test data Accuracy\", diabetes_tree1.score(X_test, y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NZcwRQW3nTbE"},"source":["We can see from the output that the model is overfitted. The above model is an example of a high variance model. We will try to decrease the model complexity and simplify it. In this case, we need to reduce the size of the tree. If that model is under-fitted, then we may have to increase the complexity slightly. Finally, we need to choose our model with optimal complexity. To check whether the model that we built is optimal or not, we need to perform cross-validation. "]},{"cell_type":"markdown","metadata":{"id":"Bfu76uvMrLjo"},"source":["### Cross-Validation\n","Building the model on train data and validating it on test data is called cross-validation. \n"]},{"cell_type":"markdown","metadata":{"id":"wR8ozwKjr6xI"},"source":["#### Train-Test cross-validation\n","Build the model on train data, find the accuracy, and validate it on test data. If the model shows high accuracy on train data and low accuracy on test data, then the model is overfitted. If the model shows low accuracy on train data, then the model is under fitted.  \n","\n","We will build different models by changing the value of pruning parameter \"max_depth\"."]},{"cell_type":"markdown","metadata":{"id":"cbv3mjG0vZ3B"},"source":["***max_depth = 6***"]},{"cell_type":"code","metadata":{"id":"3q8PHe0Pcib5"},"source":["print(\"Max Depth = 6\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qwZBhsTLcu2z"},"source":["from sklearn.tree import DecisionTreeClassifier\n","diabetes_tree1= DecisionTreeClassifier(max_depth=5)\n","diabetes_tree1.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6bNey-cvcxso"},"source":["print(\"Train data Accuracy\", diabetes_tree1.score(X_train, y_train))\n","print(\"Test data Accuracy\", diabetes_tree1.score(X_test, y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ssWwBPymvVzG"},"source":["***max_depth = 3***"]},{"cell_type":"code","metadata":{"id":"iO91aCaZc2VK"},"source":["print(\"Max Depth = 3\")\n","from sklearn.tree import DecisionTreeClassifier\n","diabetes_tree1= DecisionTreeClassifier(max_depth=3)\n","diabetes_tree1.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pcgUFFAyc5ai"},"source":["print(\"Train data Accuracy\", diabetes_tree1.score(X_train, y_train))\n","print(\"Test data Accuracy\", diabetes_tree1.score(X_test, y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8HT7kiO9vO64"},"source":["***max_depth = 2***"]},{"cell_type":"code","metadata":{"id":"REwXBqoNc9Dz"},"source":["print(\"Max Depth = 2\")\n","from sklearn.tree import DecisionTreeClassifier\n","diabetes_tree1= DecisionTreeClassifier(max_depth=2)\n","diabetes_tree1.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dJ5CoQqIdAzu"},"source":["print(\"Train data Accuracy\", diabetes_tree1.score(X_train, y_train))\n","print(\"Test data Accuracy\", diabetes_tree1.score(X_test, y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dy6-0iHkvEg_"},"source":["From the output, we can see that the max_depth=6 tree is overfitted, and the max_depth=1 tree is slightly under fitted. As discussed earlier, detecting underfitting is a little tricky. We need to look at only train data accuracy to decide to under-fitting. Out of these results, we can take either max_depth=3 or max_depth=2 as the final decision tree. "]},{"cell_type":"markdown","metadata":{"id":"noWf32nNxFFV"},"source":["#### k-fold cross-validation\n","**step 1**:Take the whole dataset. Divide it into K subsets(K-folds). Usually, K is taken as a number between 5 and 10.\n","\n","**step 2**: Build K models. While building the first model, take first K-1 folds as the train data and take the last part as test data. Build and finetune the model that best suites for the pair of train and test data. Repeat the same by changing the test data. Every fold will be used as a test dataset for one model. \n","\n","**step 3**: Find the accuracy of all models. Note that all the above K models were built and finetuned for the combination of train and test data. It was NOT a single model applied on several datasets.  We will take the average accuracy of all those K models, that will be the final result.\n","\n","K-fold cross-validation repeats the train-test scenario K-times. Since the K-fold method is taking average as the final result,  it gives us the optimal value for accuracy. We should see K-fold cross-validation as a model validation method, not a model building method. \n","Below is the code for K-fold cross-validation"]},{"cell_type":"code","metadata":{"id":"oZO05eztdKgV"},"source":["diabetes_tree_KF = DecisionTreeClassifier(max_depth=3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H0XgHxP_dNjl"},"source":["from sklearn.model_selection import KFold\n","kfold = KFold(n_splits=10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-4MC3go00oyZ"},"source":["We need to use the function KFold() and mention n_splits that mentions the number of folds. In this example, we are trying tenfold cross-validation. Here we are using a static model with max_depth=3. In reality, we have to finetune each model and arrive at this value. Since we already tried different values of max_depth, we will go ahead with max_depth=3."]},{"cell_type":"code","metadata":{"id":"fhNvInHGdQl5"},"source":["from sklearn import model_selection\n","acc10 = model_selection.cross_val_score(diabetes_tree_KF,X, y,cv=kfold)\n","print(acc10)\n","print(acc10.mean())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wVu7DnDw058Z"},"source":["From the above output, we can infer that the optimal accuracy of the model is 74.4%. Any model above 74.4% accuracy is overfitted, and below that value is under fitted. "]},{"cell_type":"markdown","metadata":{"id":"7iqHK8cUbsy8"},"source":["#### Train-Validation-Holdout\n","K-fold cross-validation is computationally expensive. It takes a lot of iterations and much time to arrive at the optimal accuracy value. There is one more reliable method that we can follow to arrive at the optimal accuracy.  Divide the data into three parts. Call them train data, validation data, and holdout data\n","* Train data - The dataset used for building the model. We learn the patterns from this data\n","* Validation data – While building the model, use this as the data for validating the model hyperparameters. We use this dataset to finetune parameters and finalize our model\n","* Holdout data – This is like our final test data. We have not used it in finetuning the parameters. We will use this model to test the finalized model. This data should NOT be used for finetuning the parameters. \n","\n","**The issue the train-test cross-validation was overfitting on both train data and test data. The issue with K-fold cross-validation was computations. This approach of Train-Validation-Holdout is in between the two methods.** \n"]},{"cell_type":"code","metadata":{"id":"wfZl9wEgdS2Q"},"source":["from sklearn  import model_selection"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jTkUSjibd_IY"},"source":["**Split overall data into train and test split**"]},{"cell_type":"code","metadata":{"id":"_QarN64pdV5S"},"source":["X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y ,test_size=0.3, random_state=99)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qBjFGDxIee-l"},"source":["**Split test data into holdout and validation data split**"]},{"cell_type":"code","metadata":{"id":"4CishkWZdYL2"},"source":["X_val, X_hold, y_val, y_hold = model_selection.train_test_split(X_test, y_test ,test_size=0.5 , random_state=11)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d1fi92CpdbTj"},"source":["print(X_train.shape)\n","print(y_train.shape)\n","print(X_val.shape)\n","print(y_val.shape)\n","print(X_hold.shape)\n","print(y_hold.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nbuXQkPtftt-"},"source":["The above code first splits the overall data into two parts - 70 % of the data for the train data and the remaining 30% for the test data. In the next line test data was further split into two parts 50% of the test data was stored in the validation and the remaining 50% in the holdout data. This split gives 15% of the overall data in the validation data."]},{"cell_type":"markdown","metadata":{"id":"XfJlOffhijhu"},"source":["We will try different values of pruning parameter max_depth"]},{"cell_type":"markdown","metadata":{"id":"D9kNdwCPi2bL"},"source":["**max_depth = 6**"]},{"cell_type":"code","metadata":{"id":"Ki1toiRDdftA"},"source":["print(\"Max Depth 6\")\n","from sklearn.tree import DecisionTreeClassifier\n","diabetes_tree1= DecisionTreeClassifier(max_depth=6)\n","diabetes_tree1.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RebkNHPZdiBA"},"source":["print(\"Train data Accuracy\", diabetes_tree1.score(X_train, y_train))\n","print(\"Validation data Accuracy\", diabetes_tree1.score(X_val, y_val))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tz1fiayBitT9"},"source":["**max_depth = 1**"]},{"cell_type":"code","metadata":{"id":"ekRfyMsgdkiW"},"source":["print(\"Max Depth 1\")\n","from sklearn.tree import DecisionTreeClassifier\n","diabetes_tree1= DecisionTreeClassifier(max_depth=1)\n","diabetes_tree1.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xI_T-VqEdwbB"},"source":["print(\"Train data Accuracy\", diabetes_tree1.score(X_train, y_train))\n","print(\"Validation data Accuracy\", diabetes_tree1.score(X_val, y_val))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MmWYhDZ1i9ku"},"source":["**max_depth = 3**"]},{"cell_type":"code","metadata":{"id":"4kVdx21zdzYU"},"source":["print(\"Max Depth 3\")\n","from sklearn.tree import DecisionTreeClassifier\n","diabetes_tree1= DecisionTreeClassifier(max_depth=3)\n","diabetes_tree1.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vlRsqJR7d13K"},"source":["print(\"Train data Accuracy\", diabetes_tree1.score(X_train, y_train))\n","print(\"Validation data Accuracy\", diabetes_tree1.score(X_val, y_val))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ctsSMmxNjJoP"},"source":["We can finalize max_depth as three. Finally, we can test this model on holdout data.The below code gives us the result on holdout data. "]},{"cell_type":"code","metadata":{"id":"TlzAs7BEd4oN"},"source":["print(\"Max Depth 3\")\n","print(\"Train data Accuracy\", diabetes_tree1.score(X_train, y_train))\n","print(\"Validation data Accuracy\", diabetes_tree1.score(X_val, y_val))\n","print(\"Holdout data Accuracy\", diabetes_tree1.score(X_hold, y_hold))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nzwL1X0Njivh"},"source":["We can see that that the model validates well on the holdout data. We may not always see higher accuracy on holdout data. \n","The above-discussed methods are the most widely used methods of cross-validation."]},{"cell_type":"markdown","metadata":{"id":"9uqAOiNxmJay"},"source":["For choosing the optimal value of pruning parameter till now we were building different models one by one and comparing them with each other. So instead of doing this we can use GridSearchCV. We just need to mention the values of different pruning parameters and model you want to build. "]},{"cell_type":"code","metadata":{"id":"OcX-sdwZd7B1"},"source":["from sklearn.model_selection import GridSearchCV\n","grid_param={'max_depth': range(1,10,1), 'max_leaf_nodes': range(2,30,1)}\n","clf_tree=DecisionTreeClassifier()\n","clf=GridSearchCV(clf_tree,grid_param)\n","clf.fit(X_train,y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7zUcNnRpeAK2"},"source":["print(clf.best_score_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"luIQ0p4aeDAO"},"source":["print(clf.best_params_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GmsLVy6ieFQe"},"source":["print(clf.best_estimator_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WHOaJTlueHW9"},"source":["grid_result_tree= clf.best_estimator_\n","print(\"Train data Accuracy\", grid_result_tree.score(X_train, y_train))\n","print(\"Validation data Accuracy\", grid_result_tree.score(X_val, y_val))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ijOzeEng3OPF"},"source":["### Feature engineering tips and tricks\n","* Manually adding new features that are derived from the existing features is called feature engineering.\n","* Feature engineering requires both statistical knowledge and business knowledge.\n","* Sometimes there is some information hidden in dates. Sometimes hidden information is in latitude and longitude; sometimes, there is some information for a particular region.\n","* Feature engineering methods are not common or standard across all datasets and industries. We should carefully study the data and business to create these new features. * We will gain the intuition and knowledge about feature engineering with practice and experience. In this section, we will discuss some tips and tricks for feature engineering."]},{"cell_type":"markdown","metadata":{"id":"0gu5KNgUbki2"},"source":["We will revisit the case study: House Sales in King County, USA. We have to predict the price of the house based on certain features. A regression model was built and its R-squared value was 70%. We used the data as it is. Now the question is, can we use the same data and increase the R-square value using feature engineering techniques? \n","\n","Defining X data"]},{"cell_type":"code","metadata":{"id":"qZYDnAR1eKYr"},"source":["X = kc_house_data[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15']]\n","y = kc_house_data['price']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wesj7ztfeN0s"},"source":["from sklearn  import model_selection\n","X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y ,test_size=0.2, random_state=55)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l7QWnwC0eTlX"},"source":["print(X_train.shape)\n","print(y_train.shape)\n","print(X_test.shape)\n","print(y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fdtbfovQeV6A"},"source":["import sklearn \n","model_1 = sklearn.linear_model.LinearRegression()\n","model_1.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QrZxsE5ueYQJ"},"source":["print(model_1.intercept_)\n","print(model_1.coef_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X-4hFVh4eaPl"},"source":["from sklearn import metrics\n","y_pred_train=model_1.predict(X_train)\n","print(metrics.r2_score(y_train,y_pred_train))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fvifWnq9egq4"},"source":["y_pred_test=model_1.predict(X_test)\n","print(metrics.r2_score(y_test,y_pred_test))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KIrVNNWGei-G"},"source":["print(\"RMSE on Train data : \", round(math.sqrt(np.mean(np.abs(y_train - y_pred_train)**2)),2))\n","print(\"RMSE on Test data : \", round(math.sqrt(np.mean(np.abs(y_test - y_pred_test)**2)),2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ObSKrYhccixF"},"source":["From the above output, we can see that the R-squared value on train and test data is around 70%. RMSE value is around 200,000. "]},{"cell_type":"markdown","metadata":{"id":"Nm5bLjcScn3Z"},"source":["#### The Dummy Variable Creation or One-hot encoding \n","Dummy variable creation is one of the basic and easiest ways to extract hidden information.  Dummy variable creation is used for non-numerical or categorical variables.\n","\n","The below code tries to draw the boxplots for all the categorical variables vs. the target variable."]},{"cell_type":"code","metadata":{"id":"yGwuMSjRelWH"},"source":["import matplotlib.pyplot as plt\n","import seaborn as sns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3aR9cCcSeotf"},"source":["plt.figure(figsize=(10,10))\n","sns.boxplot( x=kc_house_data[\"bedrooms\"],y=kc_house_data[\"price\"])\n","plt.title('bedrooms vs House Price', fontsize=20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NTXNVol2c80p"},"source":["Inference from the above graph – As the number of bedrooms increases the price of the house increases\n"]},{"cell_type":"code","metadata":{"id":"_04JFrvYerEe"},"source":["plt.figure(figsize=(10,10))\n","sns.boxplot( x=kc_house_data[\"bathrooms\"],y=kc_house_data[\"price\"])\n","plt.title('bathrooms vs House Price', fontsize=20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zDq3eKCUdBvQ"},"source":["Inference from the above graph – As the number of bathrooms increases the price of the house increases\n"]},{"cell_type":"code","metadata":{"id":"a7QrtYXeeuH4"},"source":["plt.figure(figsize=(10,10))\n","sns.boxplot( x=kc_house_data[\"floors\"],y=kc_house_data[\"price\"])\n","plt.title('floors vs House Price', fontsize=20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r8hr8UYSdI2p"},"source":["Inference from the above graph – The number of floors does not have a direct relation with the price. One hot encoding may help. "]},{"cell_type":"code","metadata":{"id":"gtXOKWlbew8q"},"source":["plt.figure(figsize=(10,10))\n","sns.boxplot( x=kc_house_data[\"waterfront\"],y=kc_house_data[\"price\"])\n","plt.title('waterfront vs House Price', fontsize=20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xnaQOnZRdOMv"},"source":["Inference from the above graph – The number of floors does not have a direct relation with the price. One hot encoding may help. "]},{"cell_type":"code","metadata":{"id":"Ww5smRyzezLY"},"source":["plt.figure(figsize=(10,10))\n","sns.boxplot( x=kc_house_data[\"view\"],y=kc_house_data[\"price\"])\n","plt.title('view vs House Price', fontsize=20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G7CgmxEAdT6-"},"source":["Inference from the above graph – House price increases as view increases. "]},{"cell_type":"code","metadata":{"id":"tHtQPDb3e2Xs"},"source":["plt.figure(figsize=(10,10))\n","sns.boxplot( x=kc_house_data[\"condition\"],y=kc_house_data[\"price\"])\n","plt.title('condition vs House Price', fontsize=20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A79lVgz-ddRN"},"source":["Inference from the above graph – Condition does not show a direct strong relation with house price. "]},{"cell_type":"code","metadata":{"id":"ziwYPZpne5Vj"},"source":["plt.figure(figsize=(10,10))\n","sns.boxplot( x=kc_house_data[\"grade\"],y=kc_house_data[\"price\"])\n","plt.title('grade vs House Price', fontsize=20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Acbi05FWdnMP"},"source":["Inference from the above graph - House price increases as grade increases\n"]},{"cell_type":"code","metadata":{"id":"1Kx1veJue8Zw"},"source":["plt.figure(figsize=(10,10))\n","sns.boxplot( x=kc_house_data[\"zipcode\"],y=kc_house_data[\"price\"])\n","plt.title('zipcode vs House Price', fontsize=20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IO4-F6mndtYr"},"source":["Inference from the above graph – House price has no apparent relation with zip code. \n"]},{"cell_type":"markdown","metadata":{"id":"O-cIzFcZerCg"},"source":["We have already used all these variables directly in the model. It is always a good idea to create dummy variables and check the impact of these variables on the model. \n"]},{"cell_type":"code","metadata":{"id":"IxJKn7CAe-z7"},"source":["from sklearn.preprocessing import OneHotEncoder"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z7v6P0BlfCWA"},"source":["print(kc_house_data.shape)\n","categorical_vars=['bedrooms', 'bathrooms', 'floors', 'waterfront', 'view', 'condition', 'grade', 'zipcode']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Uu-LaIGfHnN"},"source":["encoding=OneHotEncoder()\n","encoding.fit(kc_house_data[categorical_vars])\n","onehotlabels = encoding.transform(kc_house_data[categorical_vars]).toarray()\n","onehotlabels_data=pd.DataFrame(onehotlabels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"id-7B0v1f8Ij"},"source":["The OneHotEncoder() function converts the categorical columns to one-hot encoded columns. In fit() function, we need to mention the column names. The transform function will transform the columns to one-hot encoded columns. The number of columns will depend on the number of unique values in the column.  We then drop actual columns and update the dataset with one hot encoded column. "]},{"cell_type":"code","metadata":{"id":"9rtrzoRsfJ8N"},"source":["print(kc_house_data.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fuVjl18lfL8f"},"source":["kc_house_data1 = kc_house_data.drop(categorical_vars,axis = 1)\n","print(kc_house_data1.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RiusVlYifO5h"},"source":["kc_house_data_onehot=kc_house_data1.join(onehotlabels_data)\n","print(kc_house_data_onehot.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VnGl1ykQhhEe"},"source":["Now we will use this updated dataset to build the regression line. Below code is used for creating train and test data."]},{"cell_type":"code","metadata":{"id":"HgHdThAcfRs-"},"source":["col_names = kc_house_data_onehot.columns.values\n","print(col_names)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rahZSM2OfVd9"},"source":["x_col_names=col_names[3:]\n","print(x_col_names)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gv_c240RfXXT"},"source":["X = kc_house_data_onehot[x_col_names]\n","y = kc_house_data_onehot['price']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GwEAq5z2fZ_L"},"source":["from sklearn  import model_selection\n","X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y ,test_size=0.2, random_state=55)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ktwvOml5fdEd"},"source":["print(X_train.shape)\n","print(y_train.shape)\n","print(X_test.shape)\n","print(y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CsMk6lGHhvZG"},"source":["Now we are ready to build the model. Remember the previous model R-square value was 70% and the RMSE value was around 200,000"]},{"cell_type":"code","metadata":{"id":"pY3S-pwxffTc"},"source":["import sklearn \n","model_1 = sklearn.linear_model.LinearRegression()\n","model_1.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v7tuWRF3fhoI"},"source":["print(model_1.intercept_)\n","print(model_1.coef_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hKHqk_2afkCw"},"source":["from sklearn import metrics\n","y_pred_train=model_1.predict(X_train)\n","print(\"Train data R-Squared : \", metrics.r2_score(y_train,y_pred_train))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T4TQgEHJfm5t"},"source":["y_pred_test=model_1.predict(X_test)\n","print(\"Test data R-Squared : \" , metrics.r2_score(y_test,y_pred_test))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0MoABaiHfqDA"},"source":["print(\"RMSE on Train data : \", round(math.sqrt(np.mean(np.abs(y_train - y_pred_train)**2)),2))\n","print(\"RMSE on Test data : \", round(math.sqrt(np.mean(np.abs(y_test - y_pred_test)**2)),2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oj67a3-akpUi"},"source":["R-squared jumped from 70% to 84%. RMSE dropped from 200,000 to 145,000. This improvement is huge. It is so huge that it raises doubts about our methodology. The only variable that is fishy here is zipcode. It has too many distinct values. However, it looks perfectly fine after a second verification. Even if we look at additional checks like adjusted R-square, this model passes all those tests.  By using the same data and same model building technique, we have achieved far better accuracy."]},{"cell_type":"markdown","metadata":{"id":"jCYNs4N25Xzi"},"source":["#### Handlling Longitude and Latitude\n","House price varies based on the location of the house. The location of the house is captured in longitude and latitude.  The model may not be able to learn from the numerical values of longitude and latitude directly. We have to derive new features from longitude and latitude. \n","\n","In this example, we will try to see the relation between price and longitude latitude values.\n"]},{"cell_type":"code","metadata":{"id":"dm_rl9JnfsNd"},"source":["bubble_col= kc_house_data[\"price\"] > kc_house_data[\"price\"].quantile(0.7)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fg3eyCVq6j3e"},"source":["The below code tries to draw a scatter plot between longitude and latitude. The bubble color is filled with green if the house price is in the top 30 percentile."]},{"cell_type":"code","metadata":{"id":"6TDbjHdsfupd"},"source":["import matplotlib.pyplot as plt\n","plt.figure(figsize=(12,12))\n","plt.scatter(kc_house_data[\"long\"],kc_house_data[\"lat\"], c=bubble_col,cmap=\"RdYlGn\",s=10)\n","plt.title('House Price vs Longitude and Latitude', fontsize=20)\n","plt.xlabel('Longitude', fontsize=15)\n","plt.ylabel('Latitude', fontsize=15)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ki94fOt17FS5"},"source":["We can see that the high price houses are clustered around the top left side. It looks like there is no significant impact on longitude and latitude. We can still create a feature to extract the maximum information out of these two features. We will create a center value for high priced houses. We will calculate the distance of each house from that center"]},{"cell_type":"code","metadata":{"id":"aaTl3dCyfzCt"},"source":["high_long_mean=kc_house_data[\"long\"][bubble_col].mean()\n","high_lat_mean=kc_house_data[\"lat\"][bubble_col].mean()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6QRDn8duf2MQ"},"source":["import matplotlib.pyplot as plt\n","plt.figure(figsize=(12,12))\n","plt.scatter(kc_house_data[\"long\"],kc_house_data[\"lat\"], c=bubble_col,cmap=\"RdYlGn\",s=10)\n","plt.scatter(high_long_mean,high_lat_mean, c=\"blue\", s=1000)\n","plt.title('House Price vs Longitude and Latitude', fontsize=20)\n","plt.xlabel('Longitude', fontsize=15)\n","plt.ylabel('Latitude', fontsize=15)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xkb5KXti7rUy"},"source":["The point is (high_long_mean , high_lat_mean) is the center of high priced houses. We can see the center included in the map. We will now create a new column distance from this center of high priced houses. We will later use it in model building. We will see whether that variable can lift the accuracy of the standard model. "]},{"cell_type":"code","metadata":{"id":"5dcX6JO-f5hr"},"source":["kc_house_data[\"High_cen_distance\"]=np.sqrt((kc_house_data[\"long\"] - high_long_mean) ** 2 + (kc_house_data[\"lat\"] - high_lat_mean) ** 2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OeeIepiyf8-k"},"source":["plt.figure(figsize=(15,15))\n","plt.scatter(kc_house_data[\"High_cen_distance\"],np.log(kc_house_data[\"price\"]))\n","plt.title('House Price vs Distance from center', fontsize=20)\n","plt.xlabel('Distance from center', fontsize=15)\n","plt.ylabel('log(house price)', fontsize=15)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"awyPjGB68hue"},"source":["From the output, we can see that as the distance from the center increases overall, the price goes down. It is not a strong pattern, but it is a hidden pattern nonetheless. We will now use this variable in our initial standard model. "]},{"cell_type":"code","metadata":{"id":"D6HpN51wf_Ov"},"source":["col_names = kc_house_data.columns.values\n","print(col_names)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iTJByrwTgCE1"},"source":["x_col_names=col_names[3:]\n","print(x_col_names)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lf00zQJzgETk"},"source":["X = kc_house_data[x_col_names]\n","y = kc_house_data['price']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lhHL-3dvgHXu"},"source":["from sklearn  import model_selection\n","X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y ,test_size=0.2, random_state=55)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QsOCLuWugKXC"},"source":["print(X_train.shape)\n","print(y_train.shape)\n","print(X_test.shape)\n","print(y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vggnaP4FgMcY"},"source":["import sklearn \n","model_1 = sklearn.linear_model.LinearRegression()\n","model_1.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aC2qVhmygOXY"},"source":["print(model_1.intercept_)\n","print(model_1.coef_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ty7q8ZqYgQsz"},"source":["from sklearn import metrics\n","y_pred_train=model_1.predict(X_train)\n","print(\"Train data R-Squared : \", metrics.r2_score(y_train,y_pred_train))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mBI3j8z3gTtn"},"source":["y_pred_test=model_1.predict(X_test)\n","print(\"Test data R-Squared : \" , metrics.r2_score(y_test,y_pred_test))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6mAcp8iYgWnc"},"source":["print(\"RMSE on Train data : \", round(math.sqrt(np.mean(np.abs(y_train - y_pred_train)**2)),2))\n","print(\"RMSE on Test data : \", round(math.sqrt(np.mean(np.abs(y_test - y_pred_test)**2)),2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yBTn6-0q88v0"},"source":["The r-squared value increased by 1%, and RMSE reduced by 5000. This improvement is not huge in this case. Sometimes these features add great value. Apart from this, if we have any additional relevant knowledge about king county, then we can add it as another feature. "]},{"cell_type":"markdown","metadata":{"id":"ih16GWwk9Obo"},"source":["#### Handlling Date variables\n","The date variables should always be considered for feature engineering. Date and DateTime variables have fixed sets of formats like DD-MM-YY-HH-MM-SS. This format makes it very difficult for the model to learn the pattern from this single column."]},{"cell_type":"code","metadata":{"id":"qMidkW6rgZ_h"},"source":["print(kc_house_data.columns)\n","date_vars = ['date', 'yr_built', 'yr_renovated']\n","kc_house_dates=kc_house_data[date_vars]\n","kc_house_dates.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fg2mOVrP-ApY"},"source":["We have a ‘date’ variable; this is the sale date. We will derive year of sales, the month of sales and day of sales from this.  We will also derive the age of construction from year_built. We will derive a new indicator renovation_ind. That will indicate all the houses that are renovated. Less than 10% of homes were renovated. Hence we are not considering time since renovation."]},{"cell_type":"code","metadata":{"id":"1jBFY1Sigcv3"},"source":["kc_house_dates['sale_year'] = np.int64([d[0:4] for d in kc_house_dates[\"date\"]])\n","kc_house_dates['sale_month'] = np.int64([d[4:6] for d in kc_house_dates[\"date\"]])\n","kc_house_dates['day_sold'] = np.int64([d[6:8] for d in kc_house_dates[\"date\"]])\n","kc_house_dates['age_of_house'] = kc_house_dates['sale_year'] - kc_house_dates['yr_built']\n","kc_house_dates['Ind_renovated'] = kc_house_dates['yr_renovated']>0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SM7PHsYt-S09"},"source":["We will draw the relevant graphs to see the relation of all these new columns with the price variable. \n"]},{"cell_type":"code","metadata":{"id":"V7szbP2Cggj5"},"source":["plt.figure(figsize=(10,10))\n","sns.boxplot( x=kc_house_dates['sale_year'],y=kc_house_data[\"price\"])\n","plt.title('sale_year vs House Price', fontsize=20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mbwf3_c7-bPY"},"source":["Inference from the above graph- Sale year has no direct relation with house prices. \n"]},{"cell_type":"code","metadata":{"id":"JLcJok1vgjab"},"source":["plt.figure(figsize=(10,10))\n","sns.boxplot( x=kc_house_dates['sale_month'],y=kc_house_data[\"price\"])\n","plt.title('sale_month vs House Price', fontsize=20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Au6-VaTEAPHB"},"source":["Inference from the above graph- Sale month has no direct relation with house prices.\n"]},{"cell_type":"code","metadata":{"id":"ZjMuVB_1gl16"},"source":["plt.figure(figsize=(10,10))\n","sns.boxplot( x=kc_house_dates['day_sold'],y=kc_house_data[\"price\"])\n","plt.title('day_sold vs House Price', fontsize=20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b7zpk4zGAVRN"},"source":["Inference from the above graph- Sale day has no direct relation with house prices.\n"]},{"cell_type":"code","metadata":{"id":"v5-Ip_ITgotx"},"source":["plt.figure(figsize=(10,10))\n","plt.scatter(kc_house_dates[\"age_of_house\"],kc_house_data[\"price\"])\n","plt.title('age_of_house vs House Price', fontsize=20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NGvhBYcFAczh"},"source":["Inference from the above graph- age of the constuction has no direct relation with house prices. This result is surprising. "]},{"cell_type":"code","metadata":{"id":"BQ5MdQrvgtMu"},"source":["plt.figure(figsize=(10,10))\n","sns.boxplot( x=kc_house_dates['Ind_renovated'],y=kc_house_data[\"price\"])\n","plt.title('Ind_renovated vs House Price', fontsize=20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"thRiTpPTAlm7"},"source":["Inference from the above graph- Rennovation has an impact on house prices. This result is inline with our intuition.  "]},{"cell_type":"code","metadata":{"id":"A48eJYkQgvEs"},"source":["kc_house_dates1=kc_house_dates.drop(date_vars, axis=1) #keep only newly derived variables\n","kc_house_with_dates=kc_house_data.join(kc_house_dates1)\n","print(kc_house_with_dates.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xK1EnbcFgxLi"},"source":["col_names = kc_house_with_dates.columns.values\n","print(col_names)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bvCjUpktg0Om"},"source":["x_col_names=col_names[3:]\n","print(x_col_names)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pBeU_Iemg2kP"},"source":["X = kc_house_with_dates[x_col_names]\n","y = kc_house_with_dates['price']\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4IHqjsB_hMKU"},"source":["from sklearn  import model_selection\n","X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y ,test_size=0.2, random_state=55)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GeBxdv35hQTq"},"source":["print(X_train.shape)\n","print(y_train.shape)\n","print(X_test.shape)\n","print(y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"37bFaJkIhARE"},"source":["import sklearn \n","model_1 = sklearn.linear_model.LinearRegression()\n","model_1.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L0GzDFOEhC1K"},"source":["import sklearn \n","model_1 = sklearn.linear_model.LinearRegression()\n","model_1.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pUWmkTxohTs5"},"source":["print(model_1.intercept_)\n","print(model_1.coef_)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uElV6mo-hV_S"},"source":["from sklearn import metrics\n","y_pred_train=model_1.predict(X_train)\n","print(\"Train data R-Squared : \", metrics.r2_score(y_train,y_pred_train))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xy6Mx6XJhYCv"},"source":["y_pred_test=model_1.predict(X_test)\n","print(\"Test data R-Squared : \" , metrics.r2_score(y_test,y_pred_test))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FQnmRqNKhaXm"},"source":["print(\"RMSE on Train data : \", round(math.sqrt(np.mean(np.abs(y_train - y_pred_train)**2)),2))\n","print(\"RMSE on Test data : \", round(math.sqrt(np.mean(np.abs(y_test - y_pred_test)**2)),2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FWaK2meiAyJA"},"source":["The r-squared value increased by 1%, and RMSE reduced by 5000. This improvement is not huge in this case. We can try some more derived features like a quarter, day of the week. "]},{"cell_type":"markdown","metadata":{"id":"PDH3bRWEA1kw"},"source":["#### Transformations\n","We discussed categorical variables, and we can apply one-hot encoding on categorical variables. We also discussed the date variables. What about the continuous variables? We can apply transformations to these variables. If some variables take exponential values, then we can apply log transformation for better predictions. We can always handle the outliers by replacing them with mean or median. Alternatively, we can try the transformations. Sometimes we can derive polynomial terms from the existing data. If the data is skewed, then log transformation normalizes it. We can even apply a transformation to the target column.  We have to make sure that we do not have negative values in the column before we apply log or square root transformation. \n"]},{"cell_type":"code","metadata":{"id":"_AR8gZAPheOd"},"source":["grid_plot1= sns.PairGrid(kc_house_data, y_vars=[\"price\"], x_vars=[\"sqft_living\", \"sqft_lot\"], height=5)\n","grid_plot1.map(sns.regplot)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"letce9sBhg0D"},"source":["grid_plot2 = sns.PairGrid(kc_house_data, y_vars=[\"price\"], x_vars=[\"sqft_above\", \"sqft_basement\"], height=5)\n","grid_plot2.map(sns.regplot)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NjwGrs9phi25"},"source":["grid_plot3 = sns.PairGrid(kc_house_data, y_vars=[\"price\"], x_vars=[\"sqft_living15\",\"sqft_lot15\"], height=5)\n","grid_plot3.map(sns.regplot)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sU0kvcOaBJcE"},"source":["From the above graphs, we can see that some variables like sqft_living, sqft_above and sqft_living15 have a direct relation with the price variable. The price itself has some extreme values. Let us draw the distribution of the price variable. \n"]},{"cell_type":"code","metadata":{"id":"Bt83x09Xhkt8"},"source":["plt.figure(figsize=(10,10))\n","sns.distplot(kc_house_data[\"price\"])\n","plt.title('House Price distribution', fontsize=20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dOElStA5BRRI"},"source":["We can see the distribution is skewed. We can perform outlier treatment or apply log transformation on this data. Below code creates log_price variable and drawas the distribution chart for the transformed variable. \n"]},{"cell_type":"code","metadata":{"id":"w14K6drhhnaY"},"source":["kc_house_data[\"log_price\"]=np.log(kc_house_data[\"price\"])\n","plt.figure(figsize=(10,10))\n","sns.distplot(kc_house_data[\"log_price\"])\n","plt.title('log(House Price) distribution', fontsize=20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PEoHRzwaBXu8"},"source":["The graph shows negligible skewness. We will try to build the model by taking this log transformation on the target variable. As usual, we will compare these results with our initial model. The below code is used for building the model after log transformation. \n"]},{"cell_type":"code","metadata":{"id":"oVgn2zT1hsoF"},"source":["X = kc_house_data[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15']]\n","\n","y = kc_house_data['log_price']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"STBFsOuahwfD"},"source":["from sklearn  import model_selection\n","X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y ,test_size=0.2, random_state=55)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BRFjkp3Ih03V"},"source":["print(X_train.shape)\n","print(y_train.shape)\n","print(X_test.shape)\n","print(y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k4EdnF_ah2jv"},"source":["import sklearn \n","model_1 = sklearn.linear_model.LinearRegression()\n","model_1.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Ym4cYiNh40F"},"source":["print(model_1.intercept_)\n","print(model_1.coef_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c2Tr_o5Wh7Ta"},"source":["from sklearn import metrics\n","y_pred_train=model_1.predict(X_train)\n","print(\"Train data R-Squared : \", metrics.r2_score(y_train,y_pred_train))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pzCHlecnh9qw"},"source":["y_pred_test=model_1.predict(X_test)\n","print(\"Test data R-Squared : \" , metrics.r2_score(y_test,y_pred_test))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ooX3wGa8h_uS"},"source":["print(\"RMSE on Train data : \", round(math.sqrt(np.mean(np.abs(y_train - y_pred_train)**2)),2))\n","print(\"RMSE on Test data : \", round(math.sqrt(np.mean(np.abs(y_test - y_pred_test)**2)),2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x_yZ5Bj3BmXi"},"source":["We can see that R-squared valued increased significantly. It has gone up to 77%. Since we did the log transformation, we can not compare RMSE value. This log transformation is just one example. We can apply a transformation on predictor variables also. Square root, Square, Cube root, Cube, inverse and binning are other examples of transformation.\n","\n","There is no guarantee that every new feature increases the accuracy of the model. In our example, one-hot encoding and log transformation worked well. Feature engineering on the date, longitude and latitude variables did not show much improvement. For a certain type of datasets, certain types of feature engineering tricks work. What works best for our dataset needs to be discovered manually. \n"]},{"cell_type":"markdown","metadata":{"id":"qcO-HPhbB5jj"},"source":["### Dealing with class imbalance\n","While discussing sensitivity and specificity, we discussed class imbalance.  In some classification problems, the classes in the target have this problem of class imbalance. In those cases, the overall accuracy is driven mainly by a single class. If we are not interested in that class, then overall accuracy looks good but the model fails to fulfill its basic objective.  We then looked at individual class accuracy called sensitivity and specificity.  In that section, we discussed the model validation measures in case of class imbalance. Here we will discuss the adjustments that we need to do before building the model so that the model can learn the patterns related to rare events. "]},{"cell_type":"markdown","metadata":{"id":"j9LAB5YGCfu4"},"source":["#### Oversampling and Undersampling\n","Taking a subset of majority class is known as undersampling. Taking duplicate copies of the minority class is known as oversampling. We will try to create a balanced data from the imbalanced dataset. We expect the model to pick patterns associated with minority class from the balanced data. In a way, we are sending skewed data to the model so that it can focus on minority class. "]},{"cell_type":"code","metadata":{"id":"o7DwQKQCiCF8"},"source":["import pandas as pd\n","#credit_risk_data = pd.read_csv(r'/content/drive/My Drive/DataSets/Chapter-5/datasets/loans_data/credit_risk_data_v1.csv')\n","credit_risk_data = pd.read_csv(git_hub_path+\"/loans_data/credit_risk_data_v1.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hxBa0QPuiNxC"},"source":["print(\"Actual Data :\", credit_risk_data.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NTqG7sKwiPug"},"source":["print(\"Overall Data - Frquency\")\n","freq=credit_risk_data['Bad'].value_counts()\n","print(freq)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I3M6KNr41xt9"},"source":["print(\"Percentage\")\n","print((freq/freq.sum())*100)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o-wOYfj0iSP9"},"source":["credit_risk_class0 = credit_risk_data[credit_risk_data['Bad'] == 0]\n","credit_risk_class1 = credit_risk_data[credit_risk_data['Bad'] == 1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tBCbGSSyiUQY"},"source":["print(\"Class0 Actual :\", credit_risk_class0.shape)\n","print(\"Class1 Actual  :\", credit_risk_class1.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NfLDY3NbDC6K"},"source":["**Undersamling of class 0**"]},{"cell_type":"code","metadata":{"id":"Ug7K-PKYiWWP"},"source":["credit_risk_class0_under = credit_risk_class0.sample(int(0.5*len(credit_risk_class0)))\n","print(\"Class0 Undersample :\", credit_risk_class0_under.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kNTfZMyPDGkF"},"source":["**Oversampling of class 1**"]},{"cell_type":"code","metadata":{"id":"RjV6mMyTicuT"},"source":["credit_risk_class1_over = credit_risk_class1.sample(4*len(credit_risk_class1),replace=True)\n","print(\"Class1 Oversample :\", credit_risk_class1_over.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZBe8hip0DQOO"},"source":["In the above code, we used the sample() function to fetch a sample from the data. For the under-sample, we choose 50% of the records from class-0. In the case of oversample, we increased the records by four times. We need to use replace=True option for oversampling"]},{"cell_type":"code","metadata":{"id":"6J-IDDVxifGu"},"source":["credit_risk_balanced=pd.concat([credit_risk_class0_under,credit_risk_class1_over])\n","print(\"Final Balannced Data :\", credit_risk_balanced.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vfuv3GJmihQ7"},"source":["print(\"Balanced Data\")\n","freq=credit_risk_balanced['Bad'].value_counts()\n","print(freq)\n","print((freq/freq.sum())*100)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z5kljJRcDbDt"},"source":["We can see from the output that class-1 was just 6% in the overall data and the balanced data class-1 is 36%. We will build a model with balanced data. We expect the updated model to have a better specificity."]},{"cell_type":"code","metadata":{"id":"9WllQUMwijPu"},"source":["print(\"Actual Data :\", credit_risk_data.shape)\n","print(\"Class0 Actual :\", credit_risk_class0.shape)\n","print(\"Class1 Actual  :\", credit_risk_class1.shape)\n","print(\"Class0 Undersample :\", credit_risk_class0_under.shape)\n","print(\"Class1 Oversample :\", credit_risk_class1_over.shape)\n","print(\"Final Balannced Data :\", credit_risk_balanced.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KuRXaRJoimUw"},"source":["X = credit_risk_balanced[['Credit_Limit', 'Late_Payments_Count',\n","       'Card_Utilization_Percent', 'Age', 'Debt_to_income_ratio',\n","       'Monthly_Income', 'Num_loans_personal_loans', 'Family_dependents']]\n","\n","y = credit_risk_balanced['Bad']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oyesyDEPio7F"},"source":["from sklearn  import model_selection\n","X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y ,test_size=0.2, random_state=55)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c9McCyhcir3s"},"source":["print(X_train.shape)\n","print(y_train.shape)\n","print(X_test.shape)\n","print(y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wt5JLE6tiur9"},"source":["from sklearn.linear_model import LogisticRegression\n","model_2= LogisticRegression()\n","model_2.fit(X_train,y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ATkY2hDAiy3M"},"source":["print(model_2.intercept_)\n","print(model_2.coef_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QjyArWAhi0bV"},"source":["from sklearn.metrics import confusion_matrix\n","\n","y_pred_train=model_2.predict(X_train)\n","cm1 = confusion_matrix(y_train,y_pred_train)\n","print(\"Confusion Matrix  on Train Data\")\n","print(cm1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LSPrJ1DCi2jW"},"source":["accuracy1=(cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n","print(\"Accuracy on Train data \",accuracy1)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OiUIRe6Gi4sO"},"source":["Sensitivity1=cm1[0,0]/(cm1[0,0]+cm1[0,1])\n","print(\"Sensitivity Train data \", round(Sensitivity1,3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rHnETLaOi6Vm"},"source":["Specificity1=cm1[1,1]/(cm1[1,0]+cm1[1,1])\n","print(\"Specificity Train data \",round(Specificity1,3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7-zTP4LZi8t5"},"source":["y_pred_test=model_2.predict(X_test)\n","cm2 = confusion_matrix(y_test,y_pred_test)\n","print(\"Confusion Matrix  on Test Data\")\n","print(cm2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OM9PgrrZi-QQ"},"source":["accuracy2=(cm2[0,0]+cm2[1,1])/(cm2[0,0]+cm2[0,1]+cm2[1,0]+cm2[1,1])\n","print(\"Accuracy on Test data \", accuracy2)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nkWwkZMIjAQ5"},"source":["Sensitivity2=cm2[0,0]/(cm2[0,0]+cm2[0,1])\n","print(\"Sensitivity Test data \",round(Sensitivity2,3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_M-_b9mgjCSZ"},"source":["Specificity2=cm2[1,1]/(cm2[1,0]+cm2[1,1])\n","print(\"Specificity Test data \", round(Specificity2,3))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SylvK-5TDmE8"},"source":["We want a model with high specificity. By creating balanced data, we lifted the specificity of the model from 6.8% to 55.8%. Over Sampling and Undersampling is one method of handling class imbalance. There are other methods like synthetic sampling, cluster centroids. We can explore them if the above-metioned technique does not work well on our data. "]}]}