{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Chapter-11.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"92HAv6cs8lEM"},"source":["# Chapter-11: CNN- Convolutional Neural Networks"]},{"cell_type":"code","metadata":{"id":"dbPqgkNOIvTq"},"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from PIL import Image\n","from matplotlib.pyplot import imshow, imsave\n","import imageio\n","import random\n","import urllib.request  \n","\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Se6zmvQYJE1v"},"source":["Data_path=\"/content/drive/My Drive/DataSets/Chapter-11/Datasets/\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"45I7PenM_7cf"},"source":["## ANN's for images\n","The standard ANNs have the flexibility to learn any patterns in the data. In ANNs, we take each pixel as input while building the model. Considering each pixel as an input is not practically viable. As part of the first step, we flatten the images. Flattening the images removes a fundamental property of\n","images called special dependency. Images have a local correlation."]},{"cell_type":"markdown","metadata":{"id":"xU-em3Y3BmkY"},"source":["### Spatial dependence\n","In ANN, the first step that we perform is flattening the image to a single row. While building the ANN model, we do not use this image as it is, we convert it onto a row and pass it\n","on to model as input."]},{"cell_type":"code","metadata":{"id":"swxR_aE8S5Mk"},"source":["(X_train, Y_train), (X_test, Y_test) = keras.datasets.mnist.load_data()\n","print(\"X_train shape\", X_train.shape)\n","print(\"X_test shape\", X_test.shape)\n","\n","x_train = X_train.reshape(60000, 784)\n","x_test = X_test.reshape(10000, 784)\n","\n","print(\"X_train new shape\", x_train.shape)\n","print(\"X_test new shape\", x_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MF7pNHQyB_zK"},"source":["In MNIST data, our images were of size 28X28. We have 60,000 images in train data. The first step was to flatten the 28X28 matrix-shaped images to 756X1 array. We generally use the reshape function before starting our ANN model building."]},{"cell_type":"markdown","metadata":{"id":"if_nUWQoCToA"},"source":["The original data has 60,000 images and each image has 28X28 size. Original data is a three-dimensional tensor. We used reshape to flatten the 28X28 image into a single row of 784 pixels. The reshaped tensor has only two dimensions now. We then use this reshaped data and build the model."]},{"cell_type":"code","metadata":{"id":"uImyuyFVS8e6"},"source":["num_classes=10\n","x_train = x_train.astype('float32')\n","x_test = x_test.astype('float32')\n","x_train /= 255\n","x_test /= 255\n","\n","## Convert class vectors to binary class matrices\n","y_train = keras.utils.to_categorical(Y_train, num_classes)\n","y_test = keras.utils.to_categorical(Y_test, num_classes)\n","\n","model = keras.Sequential()\n","model.add(layers.Dense(20, activation='sigmoid', input_shape=(784,)))\n","model.add(layers.Dense(20, activation='sigmoid'))\n","model.add(layers.Dense(10, activation='softmax'))\n","model.summary()\n","model.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n","model.fit(x_train, y_train,epochs=2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f1-s-JbfCmK4"},"source":["By converting images to single rows, which is also known as flatting the images removes the spatial\n","correlation between the surrounding pixels. Somehow we need to preserve the local correlation.\n","Loss of spatial dependency is the first issue that we observed with standard ANNs."]},{"cell_type":"markdown","metadata":{"id":"GIU3kyQDJr3t"},"source":["## Filters\n","We can think filters as subregions on the image. Till now we have considered pixels inside the image. A pixel is a most primitive subregion. Instead of taking the input information from the pixels we try to take the information from the sub-regions of the image. We use filters to get the information on image subregions. "]},{"cell_type":"code","metadata":{"id":"8A04pSljTCe5"},"source":["(X_train, Y_train), (X_test, Y_test) = keras.datasets.mnist.load_data()\n","print(\"X_train shape\", X_train.shape)\n","print(\"X_test shape\", X_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y7rPme8OLmSr"},"source":["### Kernal matrix for detecting features\n","The values of the kernel matrix decide the resultant convoluted features. In the above example, we\n","considered a kernel matrix that has all 1’s in it. Depending on the values of the matrix each kernel\n","captures a certain type of features. Some kernels capture straight lines in the image. Some kernels\n","capture the circles, some kernels capture the sharp edges and some kernels capture the curves."]},{"cell_type":"code","metadata":{"id":"vLor3ZBMTJos"},"source":["# Plot 4 images as gray scale\n","\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","plt.subplot(221)\n","plt.imshow(X_train[1], cmap=plt.get_cmap('gray'))\n","plt.subplot(222)\n","plt.imshow(X_train[6], cmap=plt.get_cmap('gray'))\n","plt.subplot(223)\n","plt.imshow(X_train[7], cmap=plt.get_cmap('gray'))\n","plt.subplot(224)\n","plt.imshow(X_train[9], cmap=plt.get_cmap('gray'))\n","\n","# show the plot\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RVgNFmpOMxZA"},"source":["## The convolution layer\n","In ANN we have an input layer, and the first layer after the input layer is a fully connected hidden layer. But here, we are applying a filter on the input layer and creating convoluted features. The filter is nothing but a matrix of weights. These weights need to be determined while training. The\n","first layer in the convolutional neural network is the convolution layer. The convolution layer keeps the local correlation intact. The convolution layer captures all the features from the input."]},{"cell_type":"markdown","metadata":{"id":"4ZQtrP3jO3Xu"},"source":["### Convolution layer in keras\n","We use the function conv2D(). This function moves\n","the kernel matrix along the rows and columns of the image. We usually initialize random weights in the kernel. Below is the code is used for adding the convolution layer to the model."]},{"cell_type":"code","metadata":{"id":"zXzDqKrlTb8l"},"source":["#i=int(np.random.rand(1,1)*60000)\n","#634 #924 #952 #3611  #4458\n","import numpy as np\n","x=X_train[3611]\n","\n","\n","print(\"Actual Image\")\n","plt.imshow(x, cmap=plt.get_cmap('gray'))\n","plt.show()\n","\n","\n","print(\"Random Weights Kennel\")\n","from tensorflow.keras.layers import Conv2D\n","model=Sequential()\n","model.add(Conv2D(filters=1,\n","                 kernel_size=7,\n","                 input_shape=(28,28,1),\n","                 kernel_initializer='random_uniform'))\n","\n","img_reshape=np.expand_dims(x, axis=0)\n","img_reshape=np.expand_dims(img_reshape, axis=3)\n","img_reshape=model.predict(img_reshape)\n","pixels = np.matrix(img_reshape[:][:][:][0])\n","plt.imshow(pixels,cmap=plt.get_cmap('gray'))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C_SLMYp8PF-e"},"source":["Below are the parameters from the above code\n","* **Conv2D()** we are moving the kernel matrix in two dimensions- rows and columns\n","* **filters** – Number od filters or kernel matrices. We need to declare sufficient filters to capture\n","all the features. Here we mentioned1. Usually, in practical problems, it is 8 or 16 or 32\n","* **kernel_size** – Size of the kernel matrix. Here we mentioned 7 which gives us 7X7 matrix.\n","Usually, it is 3X3 or 5X5 or 7X7\n","* **input_shape** – Required parameter for the first layer only. It will be derived on its own from\n","the second convolution layer onwards. In our example, the input image shape is (28,28,1)\n","* **kernel_initializer** – Initial values of the kernel matrix. Usually, it is randomly initialized."]},{"cell_type":"markdown","metadata":{"id":"Oz5-P80UP4t2"},"source":["In the above output we saw the result of a randomly initialized kernel. During the training process\n","those weights in the matrix will be adjusted. Finally, each kernel matrix will identify a feature. Now\n","we will create own kernels for detecting horizontal and vertical lines. These kernels are known as\n","constant initializers in Keras. Below is the code to create the two kernel matrices."]},{"cell_type":"code","metadata":{"id":"5F2M5qMuTfMO"},"source":["import numpy as np\n","filter1=np.array([[1,1,1,1,1,1,1],\n","           [1,1,1,1,1,1,1],\n","           [100,100,100,100,100,100,100],\n","           [100,100,100,100,100,100,100],\n","           [100,100,100,100,100,100,100],\n","           [1,1,1,1,1,1,1],\n","           [1,1,1,1,1,1,1]])\n","print(\"filter1 \\n\", filter1)\n","\n","filter2=np.transpose(filter1)\n","print(\"filter2 \\n\",filter2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NAFSSxEzP_9v"},"source":["We will now apply these two kernel functions using the below code."]},{"cell_type":"code","metadata":{"id":"_YFb2lHpTh9Q"},"source":["#Try #634 #924 #952 #3611  #4458\n","x=X_train[3611]\n","\n","print(\"Actual Image\")\n","plt.imshow(x, cmap=plt.get_cmap('gray'))\n","plt.show()\n","\n","print(\"Horizontal Line\")\n","\n","model=Sequential()\n","model.add(Conv2D(1,\n","                 kernel_size=7,\n","                 input_shape=(28,28,1),\n","                 kernel_initializer=keras.initializers.Constant(filter1)))\n","\n","img_reshape=np.expand_dims(x, axis=0)\n","img_reshape=np.expand_dims(img_reshape, axis=3)\n","img_reshape=model.predict(img_reshape)\n","pixels = np.matrix(img_reshape[:][:][:][0])\n","plt.imshow(pixels,cmap=plt.get_cmap('gray'))\n","plt.show()\n","\n","print(\"Vertical Line\")\n","model=Sequential()\n","model.add(Conv2D(1,\n","                 kernel_size=7,\n","                 input_shape=(28,28,1),\n","                 kernel_initializer=keras.initializers.Constant(filter2)))\n","\n","img_reshape=np.expand_dims(x, axis=0)\n","img_reshape=np.expand_dims(img_reshape, axis=3)\n","img_reshape=model.predict(img_reshape)\n","pixels = np.matrix(img_reshape[:][:][:][0])\n","plt.imshow(pixels,cmap=plt.get_cmap('gray'))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rNxZbt3AQLnz"},"source":["From the result, we can see the highlighted portions of horizontal lines and vertical lines. Usually, it\n","is not easy to observe such patterns in the images just by looking at the plots."]},{"cell_type":"markdown","metadata":{"id":"1dU_i7y2QS9P"},"source":["### Filters for colour images\n","The color images have the depth the filter will also have the depth. The filter is applied just like the back and white images but the dot product will be considering the depth also. The whole calculation will end up in a single number in the convoluted image."]},{"cell_type":"markdown","metadata":{"id":"rvLn6MpDTuoJ"},"source":["We will first import the image"]},{"cell_type":"code","metadata":{"id":"ttPmHiJ3NfRk"},"source":["github_link=\"https://raw.githubusercontent.com/venkatareddykonasani/ML_DL_py_TF/master/Chapter11_CNN/Datasets/\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jrVlDkrUTkls"},"source":["#imp_path=Data_path+\"43534.png\"\n","#print(\"imp_path\",imp_path)\n","\n","#Image importing\n","import matplotlib.pyplot as plt\n","import urllib.request  \n","urllib.request.urlretrieve((github_link+\"43534.png\"), \"43534.png\")\n","x=plt.imread('43534.png')\n","\n","%matplotlib inline\n","#x=plt.imread(imp_path)\n","print(x.shape)\n","y=x[10:15,10:15]\n","print(y*20)\n","print(y.shape)\n","plt.imshow((y*255).astype(np.uint8))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GWaznhBvO1zN"},"source":["## Zero Padding "]},{"cell_type":"markdown","metadata":{"id":"ey6dkoEzTliO"},"source":["Below is the code for creating convolution layers."]},{"cell_type":"code","metadata":{"id":"euTYmKeWTnM8"},"source":["#i=int(np.random.rand(1,1)*60000)\n","#634 #924 #952 #3611  #4458\n","import numpy as np\n","x=X_train[3611]\n","\n","print(\"Actual Image Shape \" ,  x.shape)\n","plt.imshow(x, cmap=plt.get_cmap('gray'))\n","plt.show()\n","\n","\n","print(\"Random Weights Kennel siz3 7X7\")\n","from tensorflow.keras.layers import Conv2D\n","model=Sequential()\n","model.add(Conv2D(filters=1,\n","                 kernel_size=7,\n","                 input_shape=(28,28,1),\n","                 kernel_initializer='random_uniform'))\n","\n","img_reshape=np.expand_dims(x, axis=0)\n","img_reshape=np.expand_dims(img_reshape, axis=3)\n","img_reshape=model.predict(img_reshape)\n","pixels = np.matrix(img_reshape[:][:][:][0])\n","print(\"Output Shape \" ,pixels.shape)\n","plt.imshow(pixels,cmap=plt.get_cmap('gray'))\n","plt.show()\n","\n","\n","print(\"Random Weights Kennel siz3 5X5\")\n","from tensorflow.keras.layers import Conv2D\n","model=Sequential()\n","model.add(Conv2D(filters=1,\n","                 kernel_size=5,\n","                 input_shape=(28,28,1),\n","                 kernel_initializer='random_uniform'))\n","\n","img_reshape=np.expand_dims(x, axis=0)\n","img_reshape=np.expand_dims(img_reshape, axis=3)\n","img_reshape=model.predict(img_reshape)\n","pixels = np.matrix(img_reshape[:][:][:][0])\n","print(\"Output Shape \" ,pixels.shape)\n","plt.imshow(pixels,cmap=plt.get_cmap('gray'))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RbndOSgwggZ_"},"source":["## CNN code\n","The network we used for calculating weights is not a random CNN model. It is a model used for the classification of images in CIFAR10 data. CIFAR10 and CIFAR100 are of the 80 million tiny images dataset collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. CIFAR10 is available as part of\n","Keras sample datasets library. The CIFAR-10 data consists of 60,000 images, each image size is 32x32x3 and there are ten classes in the output. The output classes are airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck\n","\n","Below code is used for downloading the data and visualizing a few images"]},{"cell_type":"code","metadata":{"id":"kchOCnasVMgq"},"source":["from tensorflow.keras import datasets, layers, models\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aICIwXe-VPhg"},"source":["(X_train, y_train), (X_test, y_test) = datasets.cifar10.load_data()\n","\n","# Normalize pixel values to be between 0 and 1\n","X_train=X_train/255\n","X_test=X_test/255"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vVchhhHzVRRi"},"source":["print(\"X_train.shape\", X_train.shape)\n","print(\"y_train.shape\", y_train.shape)\n","print(\"X_test.shape\", X_test.shape)\n","print(\"y_test.shape\", y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hHxg9nOGVVr_"},"source":["#Drawing Few images\n","class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","plt.figure(figsize=(10,10))\n","for i in range(16):\n","    plt.subplot(4,4,i+1)\n","    plt.imshow(X_train[i], cmap=plt.cm.binary)\n","    plt.xlabel(class_names[y_train[i][0]])\n","    plt.xticks([])\n","    plt.yticks([])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CRwLs2TgipBn"},"source":["The dataset has such low-quality\n","pixelated images. We can now go ahead and build the model."]},{"cell_type":"markdown","metadata":{"id":"QQIilDy0ip7e"},"source":["Below is the code for creating the above CNN model."]},{"cell_type":"code","metadata":{"id":"JJCTCcgNVYON"},"source":["model = models.Sequential()\n","model.add(layers.Conv2D(32, (5, 5), activation='relu', input_shape=(32, 32, 3)))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n","model.add(layers.Flatten())\n","model.add(layers.Dense(64, activation='relu'))\n","model.add(layers.Dense(10))\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZEKLyXt_jMtx"},"source":["Most often, we use ReLU activation in image processing examples. Image processing networks are\n","usually very deep; ReLu works better than sigmoid and tanh activation.\n","\n","Now we will go ahead and train the model. Below is the code training the model"]},{"cell_type":"code","metadata":{"id":"OdmNkG9nVavf"},"source":["import time #To measure the execution time \n","start = time.time()\n","\n","model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.1),\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","model.fit(X_train, y_train, \n","          batch_size=16,\n","          epochs=12, \n","          validation_data=(X_test, y_test))\n","\n","end = time.time()\n","print(\"Execution time is\", int(end - start), \"seconds\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lijPnWooj3XP"},"source":["In the above code, we used the SGD optimizer. Loss is usually categorical cross-entropy. Here in this\n","data, we did not do the one-hot encoding for the target variable. The target variable has integers in\n","it. We have to use SparseCategoricalCrossentropy() for such cases where one-hot encoding is not\n","performed. form_logits=True will consider the logits for they-predicted values instead of\n","probabilities. This option works better for faster execution."]},{"cell_type":"markdown","metadata":{"id":"NxWeI4Srkxhc"},"source":["From the above output, we can see that the accuracy of test data is around 70% accuracy after 10\n","epochs. If we run a few more epochs the model will be exceedingly overfitted. We can finetune the\n","parameters and add regularization, we can further increase the accuracy."]},{"cell_type":"markdown","metadata":{"id":"syPs5RXxlssx"},"source":["## Case study: Sign Language Reading from Images\n","This case study is about predicting the number based on the sign shown with fingers. We are going\n","to use the sign-language dataset in this case study. It is publically available under CC BY-SA 4.0\n","License. This dataset originally prepared by Turkey Ankara Ayrancı Anadolu High School Students."]},{"cell_type":"markdown","metadata":{"id":"iCIG9hklmCEC"},"source":["### Background and objective\n","The samples are collected from handmade gestures of the digits from 218 participants. Each image\n","includes a one-handed display of digits 0 to 9. These are color images wth size: 100 x 100 pixels. The\n","objective is to build a model that will predict the hand gestures."]},{"cell_type":"code","metadata":{"id":"C-37U9GgVdDK"},"source":["#Let us keep all the libraries ready\n","import tensorflow.keras\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D,  Activation\n","from tensorflow.keras.layers import Reshape, Input, Lambda\n","from tensorflow.keras.layers import Conv2D, Convolution2D, MaxPooling2D, BatchNormalization\n","from tensorflow.keras.layers import Concatenate, GlobalAveragePooling2D\n","from tensorflow.keras.optimizers import Adam, SGD\n","from tensorflow.keras import regularizers, initializers\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","import pandas as pd\n","import numpy as np\n","\n","from sklearn.metrics import confusion_matrix\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","from PIL import Image\n","from matplotlib.pyplot import imshow, imsave\n","import imageio\n","import random"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F6azXy-FmZ5d"},"source":["Before going ahead with data importing. We will verify some random images from the data using the\n","below code."]},{"cell_type":"code","metadata":{"id":"tNnthAiKO-r-"},"source":["auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","downloaded = drive.CreateFile({'id':\"1gGisF40WaqB3uWSBSTW9Y9CPERtoBT2X\"})   \n","downloaded.GetContentFile('Datasets.zip') "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oGQhXLScQ3pm"},"source":["!unzip -qq 'Datasets.zip'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"THcpc_lqQz_V"},"source":["Data_path = \"./\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Fq991p7Zk4f"},"source":["#Few random Images\n","fig, ax = plt.subplots(2,2)\n","location=Data_path+'Sign_Language_Digits/Sign-Language-Digits-Dataset-master/Dataset/'\n","i=random.randint(0, 9)\n","img_id=18+i\n","img=imageio.imread(location+str(i)+\"/IMG_11\"+str(img_id)+\".JPG\")\n","ax[0,0].imshow(img)\n","\n","i=random.randint(0, 9)\n","img_id=18+i\n","img=imageio.imread(location+str(i)+\"/IMG_11\"+str(img_id)+\".JPG\")\n","ax[0,1].imshow(img)\n","\n","i=random.randint(0, 9)\n","img_id=18+i\n","img=imageio.imread(location+str(i)+\"/IMG_11\"+str(img_id)+\".JPG\")\n","ax[1,0].imshow(img)\n","\n","i=random.randint(0, 9)\n","img_id=18+i\n","img=imageio.imread(location+str(i)+\"/IMG_11\"+str(img_id)+\".JPG\")\n","ax[1,1].imshow(img)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X_KjuJusmqom"},"source":["### Data importing\n","The image data importing is different from the standard numerical tabular data importing. Usually,\n","the collection of images are stored in their respective folders. In our input data set, there are ten\n","folders, and each folder has around 200 sample images, here the folder names are the labels. The\n","function flow_from_directory() iterates through the data directory and creates random batches for\n","us. There are three major steps.\n","* tf.keras.preprocessing.image.ImageDataGenerator – for generating batches of\n","tensor image data. It has image scaling, reshaping, and various other commands\n","* Flow_from_directory : Used for creating train, validation, and test data.\n","* fit_generator: This function takes input as data and fits the model.\n","\n","Let us see how to write the code for it. The syntax is new. This is just a new way of importing."]},{"cell_type":"code","metadata":{"id":"eijeN_sEZnJI"},"source":["########################\n","# Generators\n","########################\n","\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","batch_size = 256\n","target_size = (100,100)\n","\n","\n","########################\n","# Data Directory\n","########################\n","\n","data_dir = location  # this is the image datasets directory\n","location_1=Data_path+\"Sign_language_digits_dataset_64_64\\\\\"\n","########################\n","# Data generator : Any preprocessing options/steps can be  defined here\n","########################\n","datagen = ImageDataGenerator(rescale = 1./255,  # scaling the images matrix(standard preprocessing step)\n","                             validation_split=0.2) # set validation split\n","\n","########################\n","# Train generator\n","########################\n","train_generator = datagen.flow_from_directory(\n","    data_dir,\n","    target_size=target_size,   # resizing the input images to a specific size\n","    batch_size=batch_size,     # Batch size, iterator will generate a random batch with this size\n","    color_mode = 'grayscale',  # keeping the channel to grayscale for easy calculations\n","    class_mode='categorical',\n","    shuffle=True,\n","    subset=\"training\") \n","\n","########################\n","# Validation generator\n","########################\n","\n","validation_generator = datagen.flow_from_directory(\n","    data_dir, # same directory as training data\n","    target_size=target_size,\n","    batch_size=batch_size,\n","    color_mode = 'grayscale', \n","    class_mode='categorical',\n","    shuffle=True,\n","    subset=\"validation\") # set as validation data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OwNTcwSjnMUk"},"source":["Code explanation\n","\n","**data_dir = location**  This is the image datasets directory\n","\n","**Data generator** Any preprocessing options/steps can be defined here\n","\n","rescale- scaling the images matrix(standard preprocessing step)\n","\n","validation_split - set validation split\n","\n","**train_generator()** target_size - resizing the input images to a specific size\n","\n","color_mode - keeping the channel to grayscale for easy\n","calculations\n","\n","batch_size- Batch size, the iterator will generate a random batch\n","with this size\n","\n","subset – Set as train or test data\n","\n","The output shows the number of images from train data and test data."]},{"cell_type":"markdown","metadata":{"id":"F6MLFq6Qnq5R"},"source":["### Model building and validation\n","Below is the code for building the CNN model."]},{"cell_type":"code","metadata":{"id":"agajo431Z213"},"source":["model1 = Sequential()\n","\n","# Convolution layer\n","model1.add(Conv2D(64, (3, 3), input_shape = (100, 100, 1), activation = 'relu'))\n","\n","# Pooling layer\n","model1.add(MaxPooling2D(pool_size = (2, 2)))\n","\n","# Adding second convolutional layer\n","model1.add(Conv2D(64, (3, 3), activation = 'relu'))\n","model1.add(MaxPooling2D(pool_size = (2, 2)))\n","\n","# Adding third convolutional layer\n","model1.add(Conv2D(64, (3, 3), activation = 'relu'))\n","model1.add(MaxPooling2D(pool_size = (2, 2)))\n","\n","# Flattening\n","model1.add(Flatten())\n","\n","# Step 4 - Fully connected dense layers\n","model1.add(Dense(units = 256, activation = 'relu'))\n","model1.add(Dense(units = 10, activation = 'softmax'))\n","\n","model1.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XTzy5k52PqQ4"},"source":["In the above code, we have added three convolution layers. Each layer has 64 filters and each filter size id 3X3. We also added a few pooling layers. The final dense layer has 256 hidden nodes. Below is the summary of the above model and we need to take a look at the number of weights before going ahead with model building"]},{"cell_type":"markdown","metadata":{"id":"bjEI_SxoP4R4"},"source":["There are nearly 1.7 million parameters. We will now compile and fit the model.  Training 1.7 million parameters will take much time. We will measure the time and we will also save the final model."]},{"cell_type":"code","metadata":{"id":"aAs2hh0JZ7Gg"},"source":["# model1 compilation\n","model1.compile(optimizer =SGD(lr=0.01, momentum = 0.9), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n","\n","########################\n","# fit model and train\n","########################\n","\n","import time\n","start = time.time()\n","\n","model1.fit_generator(\n","        train_generator, \n","        steps_per_epoch = len(train_generator), #total number of batches in one train epoch(train observation/batch size; also called iterations per epoch)\n","        epochs=20,\n","        validation_data = validation_generator,\n","        validation_steps = len(validation_generator), #total number of batches in validation(validation observation/batch size)\n","        verbose=1)\n","\n","model1.save_weights('m1_Sign_Language_20epochs.h5')\n","\n","end = time.time()\n","print(\"Execution time is\", int(end - start), \"seconds\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BseGChggQCEb"},"source":["We supply the train data, validation data, and epochs. Steps per epoch is the total number of batches in one train epoch. It is training records/batch size; also called iterations per epoch. \n","We can see from the output that the model is overfitted. The model takes around 40 minutes to execute 20 epochs. "]},{"cell_type":"markdown","metadata":{"id":"WWQvDOxKTKUf"},"source":["Using the below code, we can load that model and execute two epochs on top of it. "]},{"cell_type":"code","metadata":{"id":"KqTg7qIkZ95Y"},"source":["model1.load_weights(Data_path+\"/Pre_trained_models/m1_Sign_Language_20epochs.h5\")\n","\n","model1.fit_generator(\n","        train_generator, \n","        steps_per_epoch = len(train_generator), \n","        epochs=2,\n","        validation_data = validation_generator,\n","        validation_steps = len(validation_generator), \n","        verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aRAVr6RnTN0p"},"source":["Using the below code, we can load the model with 50 epochs and execute two epochs on top of it. "]},{"cell_type":"code","metadata":{"id":"tfXyoGtReNwO"},"source":["model1.load_weights(Data_path+\"/Pre_trained_models/m1_Sign_Language_50epochs.h5\")\n","\n","model1.fit_generator(\n","        train_generator, \n","        steps_per_epoch = len(train_generator), \n","        epochs=2,\n","        validation_data = validation_generator,\n","        validation_steps = len(validation_generator), \n","        verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2YaxYzpmTdYg"},"source":["From the above results, it is evident that the model is overfitted. We can follow specific rules and build an optimal CNN model. In the next section, we will see how to configure the CNN model. "]},{"cell_type":"markdown","metadata":{"id":"jhqeJ-PTFMu-"},"source":["## Scheming the ideal CNN Architecture\n","While building the CNN model in the previous example, we randomly took some filters and added three convolution layers. We have no logic or rationale for choosing the number of convolution layers and the number of filters. We can follow a useful trick while constructing the CNN model"]},{"cell_type":"code","metadata":{"id":"9lpXFm3hj4Ww"},"source":["########################\n","# Generators\n","########################\n","\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","batch_size = 256\n","target_size = (64,64)\n","\n","\n","########################\n","# Data Directory\n","########################\n","\n","data_dir = location  # this is the image datasets directory\n","\n","########################\n","# Data generator : Any preprocessing options/steps can be  defined here\n","########################\n","datagen = ImageDataGenerator(rescale = 1./255,  # scaling the images matrix(standard preprocessing step)\n","                             validation_split=0.2) # set validation split\n","\n","########################\n","# Train generator\n","########################\n","train_generator = datagen.flow_from_directory(\n","    data_dir,\n","    target_size=target_size,   # resizing the input images to a specific size\n","    batch_size=batch_size,     # Batch size, iterator will generate a random batch with this size\n","    color_mode = 'grayscale',  # keeping the channel to grayscale for easy calculations\n","    class_mode='categorical',\n","    shuffle=True,\n","    subset=\"training\") # set as training data\n","\n","########################\n","# Validation generator\n","########################\n","\n","validation_generator = datagen.flow_from_directory(\n","    data_dir, # same directory as training data\n","    target_size=target_size,\n","    batch_size=batch_size,\n","    color_mode = 'grayscale', \n","    class_mode='categorical',\n","    shuffle=True,\n","    subset=\"validation\") # set as validation data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gr82-40wNHLM"},"source":["Now we will build a CNN model"]},{"cell_type":"code","metadata":{"id":"HlW15CbFkRem"},"source":["model2 = Sequential()\n","\n","# Convolution and Pooling layers\n","model2.add(Conv2D(16, (3, 3), input_shape = (64, 64, 1), activation = 'relu'))\n","model2.add(Conv2D(32, (3, 3), activation = 'relu'))\n","model2.add(MaxPooling2D(pool_size = (2, 2)))\n","\n","\n","model2.add(Conv2D(64, (3, 3), activation = 'relu'))\n","model2.add(Conv2D(64, (3, 3), activation = 'relu'))\n","model2.add(MaxPooling2D(pool_size = (2, 2)))\n","\n","model2.add(Conv2D(128, (3, 3), activation = 'relu'))\n","model2.add(Conv2D(128, (3, 3), activation = 'relu'))\n","model2.add(MaxPooling2D(pool_size = (2, 2)))\n","\n","\n","# Flattening and  Fully connected dense layers\n","model2.add(Flatten())\n","model2.add(Dense(units = 32, activation = 'relu'))\n","model2.add(Dense(units = 10, activation = 'softmax'))\n","\n","model2.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y4E3JM5kNR09"},"source":["There are 347,562 parameters in this model. We have now reduced the parameters from 1.7 million\n","to 0.34 million. We will compile and build the model using the below code"]},{"cell_type":"code","metadata":{"id":"eTrtpl8IkVZK"},"source":["# model compilation\n","model2.compile(optimizer =SGD(lr=0.01, momentum = 0.9), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n","\n","########################\n","# fit model and train\n","########################\n","\n","import time\n","start = time.time()\n","\n","model2.fit_generator(\n","        train_generator, \n","        steps_per_epoch = len(train_generator), #total number of batches in one train epoch(train observation/batch size; also called iterations per epoch)\n","        epochs=50,\n","        validation_data = validation_generator,\n","        validation_steps = len(validation_generator), #total number of batches in validation(validation observation/batch size)\n","        verbose=1)\n","\n","model2.save_weights('m2_Receptive_field_50epochs.h5')\n","\n","end = time.time()\n","print(\"Execution time is\", int(end - start), \"seconds\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H9WmaSjRdV-c"},"source":["Using the below code, we can load the model with 50 epochs and execute two epochs on top of it. "]},{"cell_type":"code","metadata":{"id":"gnc_BqTBkYWr"},"source":["model2.load_weights(Data_path+\"Pre_trained_models/m2_Receptive_field_50epochs.h5\")\n","\n","model2.fit_generator(\n","        train_generator, \n","        steps_per_epoch = len(train_generator), #total number of batches in one train epoch(train observation/batch size; also called iterations per epoch)\n","        epochs=2,\n","        validation_data = validation_generator,\n","        validation_steps = len(validation_generator), #total number of batches in validation(validation observation/batch size)\n","        verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9u8B03cpNhLm"},"source":["From the output, we can see that the above model is also overfitted(100% accuracy on train data and 74% on test data). We have two options now. We can either reduce the number of convolution layers and nodes; alternatively, we can keep the same architecture and introduce regularization. Usually, regularization is the preferred option. The below code introduces regularization in the above network diagram. We can also decrease the batch size to 128 or 64 to increase the iterations per epoch, which will help us in reducing the overall epochs."]},{"cell_type":"markdown","metadata":{"id":"RaGx55koPQ-b"},"source":[" Below is the code for the model with\n","regularization"]},{"cell_type":"code","metadata":{"id":"feimLrU7kt7A"},"source":["########################\n","# Generators\n","########################\n","\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","batch_size = 64\n","target_size = (64,64)\n","\n","\n","########################\n","# Data Directory\n","########################\n","\n","data_dir = location  # this is the image datasets directory\n","\n","########################\n","# Data generator : Any preprocessing options/steps can be  defined here\n","########################\n","datagen = ImageDataGenerator(rescale = 1./255,  # scaling the images matrix(standard preprocessing step)\n","                             validation_split=0.2) # set validation split\n","\n","########################\n","# Train generator\n","########################\n","train_generator = datagen.flow_from_directory(\n","    data_dir,\n","    target_size=target_size,   # resizing the input images to a specific size\n","    batch_size=batch_size,     # Batch size, iterator will generate a random batch with this size\n","    color_mode = 'grayscale',  # keeping the channel to grayscale for easy calculations\n","    class_mode='categorical',\n","    shuffle=True,\n","    subset=\"training\") # set as training data\n","\n","########################\n","# Validation generator\n","########################\n","\n","validation_generator = datagen.flow_from_directory(\n","    data_dir, # same directory as training data\n","    target_size=target_size,\n","    batch_size=batch_size,\n","    color_mode = 'grayscale', \n","    class_mode='categorical',\n","    shuffle=True,\n","    subset=\"validation\") # set as validation data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0d2w2zF_kz0-"},"source":["model2 = Sequential()\n","\n","# Convolution and Pooling layers\n","model2.add(Conv2D(16, (3, 3), input_shape = (64, 64, 1), activation = 'relu'))\n","model2.add(Conv2D(32, (3, 3), activation = 'relu'))\n","model2.add(MaxPooling2D(pool_size = (2, 2)))\n","model2.add(Dropout(0.5))\n","\n","model2.add(Conv2D(64, (3, 3), activation = 'relu'))\n","model2.add(Conv2D(64, (3, 3), activation = 'relu'))\n","model2.add(MaxPooling2D(pool_size = (2, 2)))\n","model2.add(Dropout(0.5))\n","\n","model2.add(Conv2D(128, (3, 3), activation = 'relu'))\n","model2.add(Conv2D(128, (3, 3), activation = 'relu'))\n","model2.add(MaxPooling2D(pool_size = (2, 2)))\n","model2.add(Dropout(0.5))\n","\n","\n","# Flattening and  Fully connected dense layers\n","model2.add(Flatten())\n","model2.add(Dense(units = 32, activation = 'relu'))\n","model2.add(Dropout(0.5))\n","\n","model2.add(Dense(units = 10, activation = 'softmax'))\n","\n","model2.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CRfnEut8PZl1"},"source":["We can see the dropout layers in the above model code. The same can be observed in its output.\n","\n","We can now compile the model and execute it. We can also track the accuracy change in each epoch\n","by saving the model epochs in the history object."]},{"cell_type":"code","metadata":{"id":"A04qcjaSk20s"},"source":["# model compilation\n","model2.compile(optimizer =SGD(lr=0.01, momentum = 0.9), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n","\n","########################\n","# fit model and train\n","########################\n","\n","import time\n","start = time.time()\n","\n","history=model2.fit_generator(\n","        train_generator, \n","        steps_per_epoch = len(train_generator), \n","        epochs=50,\n","        validation_data = validation_generator,\n","        validation_steps = len(validation_generator), \n","        verbose=1)\n","\n","model2.save_weights('m2_Dropout_Rec_fld_50epochs.h5')\n","\n","end = time.time()\n","print(\"Execution time is\", int(end - start), \"seconds\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c73LmMWik6eK"},"source":["plt.plot(history.history['accuracy'], label='accuracy')\n","plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n","plt.title(\"Train and Valid Accuracy by Epochs\")\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.ylim([0,1])\n","plt.legend(loc='lower right')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YIsK9XH-k9j7"},"source":["model2.load_weights(Data_path+\"/Pre_trained_models/m2_Dropout_Rec_fld_50epochs.h5\")\n","\n","history=model2.fit_generator(\n","        train_generator, \n","        steps_per_epoch = len(train_generator), \n","        epochs=2,\n","        validation_data = validation_generator,\n","        validation_steps = len(validation_generator), \n","        verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eTLj1-qllF9P"},"source":["model2.load_weights(Data_path+\"/Pre_trained_models/m2_Dropout_Rec_fld_100epochs.h5\")\n","\n","history=model2.fit_generator(\n","        train_generator, \n","        steps_per_epoch = len(train_generator), \n","        epochs=2,\n","        validation_data = validation_generator,\n","        validation_steps = len(validation_generator), \n","        verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XvbLn8uVlMPB"},"source":["model2.load_weights(Data_path+\"/Pre_trained_models/m2_Dropout_Rec_fld_100epochs.h5\")\n","\n","history=model2.fit_generator(\n","        train_generator, \n","        steps_per_epoch = len(train_generator), \n","        epochs=2,\n","        validation_data = validation_generator,\n","        validation_steps = len(validation_generator), \n","        verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mHobJzHSf4Jw"},"source":["### Batch Normalization\n","Before building the model, we usually normalize or scale all the values of input values. For example, if some of the inputs are on the scale of thousands and others are in decimals, then we normalize the input. Different distributions for the inputs is generally considered as a problem, we normalize the inputs to avoid it. The formula for normalization is simple . This normalization of input will help us in faster calculations, and it also avoids the dominance of a few inputs. Input normalization is\n","applied for almost all the models. In some cases, the input normalization is done by default in the training data.\n","\n","Below is a sample code to understand the number of parameter calculations."]},{"cell_type":"code","metadata":{"id":"iPE8M8UisFzy"},"source":["model = Sequential()\n","model.add(Conv2D(1, (3, 3), input_shape = (32, 32, 1)))\n","model.add(BatchNormalization())\n","\n","model.add(MaxPooling2D(pool_size = (2, 2)))\n","model.add(BatchNormalization())\n","\n","model.add(Conv2D(2, (3, 3)))\n","model.add(BatchNormalization())\n","\n","model.add(MaxPooling2D(pool_size = (2, 2)))\n","model.add(BatchNormalization())\n","\n","model.add(Conv2D(3, (3, 3)))\n","model.add(BatchNormalization())\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lMKucAxgilUM"},"source":["Now that we understand batch normalization, we will get back to our case study and include batch normalization layers. After adding batch normalization, we can afford to reduce the depth of the network. Below is the updated code with batch normalization layers"]},{"cell_type":"code","metadata":{"id":"WCu3m0jysHHG"},"source":["model3 = Sequential()\n","\n","model3.add(Conv2D(16, (3, 3), input_shape = (64, 64, 1), activation = 'relu'))\n","model3.add(BatchNormalization())\n","model3.add(Dropout(0.5))\n","\n","model3.add(Conv2D(16, (3, 3), activation = 'relu'))\n","model3.add(MaxPooling2D(pool_size = (2, 2)))\n","model3.add(BatchNormalization())\n","model3.add(Dropout(0.5))\n","\n","model3.add(Conv2D(32, (3, 3), activation = 'relu'))\n","model3.add(MaxPooling2D(pool_size = (2, 2)))\n","model3.add(BatchNormalization())\n","model3.add(Dropout(0.5))\n","\n","model3.add(Conv2D(32, (3, 3), activation = 'relu'))\n","model3.add(MaxPooling2D(pool_size = (2, 2)))\n","model3.add(BatchNormalization())\n","model3.add(Dropout(0.5))\n","\n","model3.add(Conv2D(64, (3, 3), activation = 'relu'))\n","model3.add(BatchNormalization())\n","model3.add(Dropout(0.5))\n","\n","model3.add(Flatten())\n","model3.add(Dense(units = 16, activation = 'relu'))\n","model3.add(Dropout(0.5))\n","model3.add(Dense(units = 10, activation = 'softmax'))\n","\n","model3.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BnXPYGXYiwkf"},"source":["We can observe the significant reduction of convolution layers in this code. The last model has 347,562 weights. This new model will have very less number of weights since we have removed two convolution layers with 128 nodes each.\n","\n","From the model summary, we can see a considerable reduction in the number of parameters. Here we are attempting to build a simpler model that can give the same accuracy as the previous model.\n","Below is the code for training the model and plotting the results."]},{"cell_type":"code","metadata":{"id":"NGQ7eI0tsMvP"},"source":["model3.compile(optimizer =SGD(lr=0.03, momentum = 0.9), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n","\n","########################\n","# fit model and train\n","########################\n","\n","import time\n","start = time.time()\n","\n","history=model3.fit_generator(\n","        train_generator, \n","        steps_per_epoch = len(train_generator), \n","        epochs=200,\n","        validation_data = validation_generator,\n","        validation_steps = len(validation_generator), \n","        verbose=1)\n","\n","model3.save_weights('m3_BatchNorm_200epochs.h5')\n","\n","end = time.time()\n","print(\"Execution time is\", int(end - start), \"seconds\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"EJWN3ax2sQIl"},"source":["## Plotting the results \n","plt.plot(history.history['accuracy'], label='accuracy')\n","plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n","plt.title(\"Train and Valid Accuracy by Epochs\")\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.ylim([0,1])\n","plt.legend(loc='lower right')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qWSbAWuJjk4V"},"source":["From the output we can see that accuracy has not improved a lot, we got the same accuracy of 80%\n","on train and test data. The point to note here is the simplicity of the model. We achieved the same\n","accuracy with almost seven times lesser parameters. As discussed earlier, batch normalization stand-\n","alone may not give us perfect results. Batch normalization along with other optimal parameters\n","gives us the best results. That brings us to the next topic on choosing the right optimizer"]},{"cell_type":"markdown","metadata":{"id":"8So0jxEskzYf"},"source":["### Choosing the optimizers\n","We can try optimizers on a given dataset, but we can not be very sure that the accuracy will increase. These optimizers help us in reducing the overall execution time. In practice, we use almost all the optimizers. There is no preference. For a particular type of datasets, a certain type of optimizer works the best way. Below is the code for applying the adam optimizer on our data."]},{"cell_type":"code","metadata":{"id":"KoG0450LsSnC"},"source":["model3.load_weights(Data_path+\"/Pre_trained_models/m3_BatchNorm_200epochs.h5\")\n","\n","history=model3.fit_generator(\n","        train_generator, \n","        steps_per_epoch = len(train_generator), \n","        epochs=2,\n","        validation_data = validation_generator,\n","        validation_steps = len(validation_generator), \n","        verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8WpmA8ImmFww"},"source":["We can see from the above code that the model is kept the same. We changed the optimization\n","function from SGD to Adam."]},{"cell_type":"code","metadata":{"id":"YzvmvtzPsWXf"},"source":["model3.compile(optimizer =Adam(learning_rate=0.005, beta_1=0.9, beta_2=0.999), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n","\n","########################\n","# fit model and train\n","########################\n","\n","import time\n","start = time.time()\n","\n","history=model3.fit_generator(\n","        train_generator, \n","        steps_per_epoch = len(train_generator), \n","        epochs=100,\n","        validation_data = validation_generator,\n","        validation_steps = len(validation_generator), \n","        verbose=1)\n","\n","model3.save_weights('m3_BatchNorm_and_Adam_100epochs.h5')\n","\n","end = time.time()\n","print(\"Execution time is\", int(end - start), \"seconds\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"891Nu4eTsZ3t"},"source":["## Plotting the results \n","plt.plot(history.history['accuracy'], label='accuracy')\n","plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n","plt.title(\"Train and Valid Accuracy by Epochs\")\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.ylim([0,1])\n","plt.legend(loc='lower right')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xrzBiHQ1mcJ-"},"source":["From the output, we can see that the adam optimizer is working almost the same as SGD in our example. But we could achieve the same 80% accuracy within 100 epochs using Adam. There is a massive difference in execution time as well. In some cases, Adam gives a better result compared to SGD and RMSprop."]},{"cell_type":"code","metadata":{"id":"Ru193op1sbuE"},"source":["model3.load_weights(Data_path+\"/Pre_trained_models/m3_BatchNorm_and_Adam_100epochs.h5\")\n","\n","history=model3.fit_generator(\n","        train_generator, \n","        steps_per_epoch = len(train_generator), \n","        epochs=3,\n","        validation_data = validation_generator,\n","        validation_steps = len(validation_generator), \n","        verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TUz4aorEd8sd"},"source":[""],"execution_count":null,"outputs":[]}]}