{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Chapter-3.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"xAvGQOCZlQRw"},"source":["# Chapter-3: Regression and Logistics Regression"]},{"cell_type":"code","metadata":{"id":"5CNI7LyvSqaJ"},"source":["import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IVacVgl5lM5f"},"source":["## Regression"]},{"cell_type":"code","metadata":{"id":"mhGN9GqrNhbs"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5j1C_TQlNgZO"},"source":["We have this employee profile data. We have three variables in it. Import the data, draw the scatter plot and try to answer the below questions.\n","1. Is there any association between monthly income and monthly expense, if there is any association, then is it positive or negative? \n","2. Is there any association between monthly income and time spent on reading books, if there is any association, then is it positive or negative? \n"]},{"cell_type":"code","metadata":{"id":"g3-G_6Usk_n9"},"source":["#emp_profile=pd.read_csv(r\"/content/drive/My Drive/DataSets/Chapter-3/Datasets/employee_profile.csv\")\n","emp_profile=pd.read_csv(r\"https://raw.githubusercontent.com/venkatareddykonasani/ML_DL_py_TF/master/Chapter3_Regression_Logistic/Datasets/employee_profile.csv\")\n","\n","#First few rows\n","emp_profile.head()\n","\n","#Column names \n","print(emp_profile.columns)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EV_KsSHYODqO"},"source":["### Drawing the scatter plot"]},{"cell_type":"code","metadata":{"id":"ymzkOisrNyOw"},"source":["import matplotlib.pyplot as plt\n","plt.scatter(emp_profile[\"Monthly_Income\"], emp_profile[\"Monthly_Expenses\"])\n","plt.title('Income vs Expesnes Plot')\n","plt.xlabel('Monthly Income')\n","plt.ylabel('Monthly Expenses')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BQCh2yjRN86e"},"source":["From the above scatter plot, It is very evident that there is a strong association between monthly income and monthly expenses. Higher the income higher expenses, lower the income lower the expenses. This is a clear indication of a strongly positive relationship. "]},{"cell_type":"code","metadata":{"id":"lTUKxY9cPABK"},"source":["import matplotlib.pyplot as plt\n","plt.scatter(emp_profile[\"Monthly_Income\"], emp_profile[\"Time_Spent_Reading_Books\"])\n","plt.title('Income vs Time_Spent_Reading_Books')\n","plt.xlabel('Monthly Income')\n","plt.ylabel('Time_Spent_Reading_Books')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xh5BCc84PKjU"},"source":["From the above scatter plot, it is clear that there is no relation between 'Monthly Income' and 'Time_Spent_Reading_Books'. The graph between these two variables is very scattered."]},{"cell_type":"markdown","metadata":{"id":"dDBuFgEpQnvN"},"source":["### Regression Model Building\n","Import the data and start fitting regression line. We don't need to draw the data, we can directly start building the regression model.Here we are not doing data exploration or data cleaning since the data is already clean and well-formatted before starting the model building."]},{"cell_type":"code","metadata":{"id":"beoGv7l0N2nw"},"source":["#air_pass=pd.read_csv(r\"/content/drive/My Drive/DataSets/Chapter-3/Datasets/Air_Passengers.csv\")\n","air_pass=pd.read_csv(\"https://raw.githubusercontent.com/venkatareddykonasani/ML_DL_py_TF/master/Chapter3_Regression_Logistic/Datasets/Air_Passengers.csv\")\n","print(air_pass.columns)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LIZ4KODeTmQ8"},"source":["We can use two packages for building a regression model, 'sklearn' and 'statsmodels'. We will start with “statsmodels” package. We need to import subpackage from statsmodels package. "]},{"cell_type":"code","metadata":{"id":"hcY_faJON9DX"},"source":["import statsmodels.formula.api as sm"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IZ3iqSIyUMKu"},"source":["####Code description\n","* model = sm.ols(formula='y~x',data = data_name),where \n","      \n","1. y : target variable\n","2. x : predictor variables\n","3. sm.ols – sm is our package name, ols function is used for minimizing the error squares to give us the values of regression beta coefficients. Ols stands for “ordinary least squares”.\n","\n","* fitted = model.fit()\n","1. The previous step is model configuration and this step is model building.\n","2. Here the actual data will be submitted, and optimization will be performed,  the model will be built.\n","3. After this step we are ready with the beta coefficients, they will be stored in fitted. \n","\n","* fitted.summary()\n","1. This command gives the output summary."]},{"cell_type":"code","metadata":{"id":"f3fwWs-oN__A"},"source":["model1 = sm.ols(formula='Passengers_count ~ marketing_cost', data=air_pass)\n","fitted1 = model1.fit()\n","print(fitted1.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oS_Cs5JNWrUc"},"source":["Now the model is fitted so we will find the predictions using this model.We can predict Passengers_count at any given point of marketing_cost.\n","\n","EXAMPLE:\n","When marketing_cost=4500, we will find predicted value of Passengers_count. "]},{"cell_type":"code","metadata":{"id":"zSMRJDpiOCrF"},"source":["new_data=pd.DataFrame({\"marketing_cost\":[4500]})\n","print(fitted1.predict(new_data))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GLN_J6hAYBWJ"},"source":["Similarly we can predict Passengers_count at more than one point in one attempt.\n","\n","EXAMPLE: marketing_cost=4500,3600,3000,5000"]},{"cell_type":"code","metadata":{"id":"S1QXhHMTOGUG"},"source":["new_data1=pd.DataFrame({\"marketing_cost\":[4500,3600, 3000,5000]})\n","print(fitted1.predict(new_data1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZRG-fUjZtktU"},"source":["### R-squarred"]},{"cell_type":"code","metadata":{"id":"ryfpfv-kOIrj"},"source":["air_pass[\"passengers_count_pred\"]=round(fitted1.predict(air_pass))\n","keep_cols=[\"marketing_cost\", \"Passengers_count\", \"passengers_count_pred\"]\n","air_pass[keep_cols]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TNuDaF2dyozC"},"source":["If the model is good, then the predicted values will be very close to actual values.  Any gap between actual values and predicted values should be considered as an error. For a perfect model, the actual values are exactly equal to predicted values, and error will be zero in that case. \n","\n","R-Squared is used for measuring the accuracy or the goodness of fit of the model. R-squared is also known as an explained variance by the model. For good model,   R-squarred value should be near to 1.\n"]},{"cell_type":"code","metadata":{"id":"wSS8-iQhOLOs"},"source":["model2 = sm.ols(formula='Passengers_count ~ customer_ratings', data=air_pass)\n","fitted2 = model2.fit()\n","print(fitted2.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MteN0IldDPpL"},"source":["Looking at the output summary, we can identify that model-1 has an R-squared value of 0.761 and model-2 has an R-squared value of 0.102. Using model-1, we have explained 76% of the variance in the target variable, whereas using model-2, we could explain only 10% variance in the target variable.  Hence, model-1 is better than model-2 when it comes to the accuracy of predictions.  "]},{"cell_type":"markdown","metadata":{"id":"76CLCE-mDk1c"},"source":["## Multiple regression\n","The model building process and interpretation of the R-squared value remain the same. In fact, in multiple regression line, we are utilizing multiple variables information for predicting the target variable. "]},{"cell_type":"code","metadata":{"id":"tZQbCgKLOOau"},"source":["import statsmodels.formula.api as sm\n","model3 = sm.ols(formula='Passengers_count ~ marketing_cost+percent_delayed_flights+number_of_trips+customer_ratings+poor_weather_index+percent_female_customers+Holiday_week+percent_male_customers', data=air_pass)\n","fitted3 = model3.fit()\n","print(fitted3.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TStM2cYlENAu"},"source":["As you can observe, we need to mention all the predictor variables on the right-hand side. Here we are not summing the columns; it is just the syntax to supply the independent variables.We can see the R-squarred value has gone beyond 90%. We have added multiple predictor variables, and we expect the model to get better. It doesn’t necessarily mean that all the predictor variables are really important in the model."]},{"cell_type":"markdown","metadata":{"id":"4UybLUAEFp3v"},"source":["## Multicollinearity in Regression"]},{"cell_type":"code","metadata":{"id":"kjO_kob5-Qxz"},"source":["#income_expenses=pd.read_csv(r\"/content/drive/My Drive/DataSets/Chapter-3/Datasets/customer_income_expenses.csv\")\n","income_expenses=pd.read_csv(\"https://raw.githubusercontent.com/venkatareddykonasani/ML_DL_py_TF/master/Chapter3_Regression_Logistic/Datasets/customer_income_expenses.csv\")\n","\n","print(income_expenses.columns)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lvCrv9g--w4B"},"source":["import statsmodels.formula.api as sm\n","model4=sm.ols(formula='Monthly_Expenses ~ Monthly_Income_in_USD+Number_of_Credit_cards+Number_of_personal_loans+Monthly_Income_in_Euro', data=income_expenses)\n","fitted4 = model4.fit()\n","print(fitted4.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4-nUbfxscTkZ"},"source":["This model is a standard multiple regression model with four predictor variables.  The model gave us a 96% R-squared value. Let us look at the individual coefficients. \n","\n","* “Monthly_Income_in_USD” has a positive impact on overall expenses. Which means higher the variable, higher the target variable i.e monthly expenses. \n","* “Number_of_Credit_cards” has a positive impact on overall expenses. Which means higher the variable, higher the target variable i.e monthly expenses. \n","* “Number_of_personal_loans” has a positive impact on overall expenses. Which means higher the variable, higher the target variable i.e monthly expenses. \n","* “Monthly_Income_in_Euro” has a negative impact on overall expenses. Which means higher the variable, lower the target variable i.e monthly expenses. \n"]},{"cell_type":"markdown","metadata":{"id":"CGWXRKRujFSw"},"source":["The monthly expenses should be directly proportional to monthly income whether the income is measured in dollars or euros. Monthly income in euros is just 0.9 times monthly income in dollars. Here one dollar=0.9 euros.But here the target variable show negative relation with Monthly_Income_in_Euro and positive relation with Monthly_Income_in_USD.In the next example, we will remove the dollars variable from the model and rebuild the whole model with just three variables. \n"]},{"cell_type":"code","metadata":{"id":"NBmfkUUd-ztO"},"source":["model5=sm.ols(formula='Monthly_Expenses ~Number_of_Credit_cards+Number_of_personal_loans+Monthly_Income_in_Euro', data=income_expenses)\n","fitted5 = model5.fit()\n","print(fitted5.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WZGZ7qkSjwjx"},"source":["Here are a few observations from the above output.\n","* The model has the same R-squared value. So, dropping Monthly_Income_in_USD didn’t have a significant impact on the overall accuracy of the model.\n","* “Number_of_Credit_cards” has a positive impact on overall expenses. Which means higher the variable, higher the target variable, i.e., monthly expenses. \n","* “Number_of_personal_loans” has a positive impact on overall expenses. Which means higher the variable, higher the target variable, i.e., monthly expenses. \n","* “Monthly_Income_in_Euro” has a positive impact on overall expenses. Which means higher the variable, higher the target variable, i.e., monthly expenses. \n"]},{"cell_type":"markdown","metadata":{"id":"RVqoTQm9j7QR"},"source":["**The above example is a classical illustration of the adverse effects of “Multicollinierity.”**\n"]},{"cell_type":"markdown","metadata":{"id":"SXz4bx8ikMBW"},"source":["##  VIF-Variance Inflation Factor calculation\n","To detect multicollinearity, we follow a very simple technique. First of all, multicollinearity is related to only predictor variables, and the target variable has nothing to do while detecting multicollinearity. \n","\n","The actual measure for detecting multicollinearity is the Variance Inflation Factor known as VIF. This measure is derived from the individual model R-squared values.\n"]},{"cell_type":"markdown","metadata":{"id":"AmP-uN6DmKNu"},"source":["Firstly, we need to write our VIF calculation function. A function that takes all the predictor variables and builds individual models. Here is the VIF function and code explanation"]},{"cell_type":"code","metadata":{"id":"nR3uF8Xn-4zA"},"source":["def vif_cal(x_vars):\n","    xvar_names=x_vars.columns\n","    for i in range(0,xvar_names.shape[0]):\n","        y=x_vars[xvar_names[i]] \n","        x=x_vars[xvar_names.drop(xvar_names[i])]\n","        rsq=sm.ols(formula=\"y~x\", data=x_vars).fit().rsquared  \n","        vif=round(1/(1-rsq),2)\n","        print (xvar_names[i], \" VIF = \" , vif)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Q6maQia-705"},"source":["vif_cal(x_vars=income_expenses.drop([\"Monthly_Expenses\"], axis=1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jjeewvdfmol9"},"source":["* This output shows high VIF values for all the variables. All the variables have more than 5 VIF. Shall we drop all the four variables from the model? Then how can we build a model? \n","* This output shows high VIF values for Monthly_Income_in_USD and Monthly_Income_in_Euro. It does NOT mean that we drop both of these variables. In the presence of Monthly_Income_in_USD, the other variable Monthly_Income_in_Euro is redundant. Similarly, in the presence of Monthly_Income_in_Euro, the other variable Monthly_Income_in_USD is redundant. We should not drop both of them. Drop any one of them. \n","* This is important while dealing with multicollinearity, and we should not drop all the variables with  VIF>5 in one iteration. We should drop one variable at a time. That may auto-correct the VIF value of several other variables.  \n","* We will start with the variable that has the highest VIF. Here we will drop Monthly_Income_in_Euro first. That will leave three variables in the model. Then we will check multicollinearity among these three variables. \n"]},{"cell_type":"code","metadata":{"id":"PGfSyOdW--XH"},"source":["vif_cal(x_vars=income_expenses.drop([\"Monthly_Expenses\",\"Monthly_Income_in_Euro\"], axis=1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KOEb8gnRm4nN"},"source":["From the above output we can observe that the VIF value of Monthly_Income_in_USD has reduced significantly, and it happened due to the elimination of the Monthly_Income_in_Euro variable. There is still multicollinearity present in the system. Number_of_credit_cards and Number_of_personal_loans both the variables have higher than five VIF values. We need to drop the highest VIF variable and re-calculate the VIF values for the remaining variables.  "]},{"cell_type":"code","metadata":{"id":"y9gLnGcq_As2"},"source":["vif_cal(x_vars=income_expenses.drop([\"Monthly_Expenses\",\"Monthly_Income_in_Euro\",\"Number_of_personal_loans\"], axis=1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gpoHWayHq3Mj"},"source":["Now both the variables have VIF values less than five, and it indicates that both of them carry independent information. So we can conclude that we don't need four variables for building this model, two variables are sufficient. "]},{"cell_type":"markdown","metadata":{"id":"zAmnB1CSrJOC"},"source":["Now let us build a final model after removing nulticollinearity."]},{"cell_type":"code","metadata":{"id":"wTISyLzi_Cpo"},"source":["model6=sm.ols(formula='Monthly_Expenses ~ Monthly_Income_in_USD+Number_of_Credit_cards', data=income_expenses)\n","fitted6 = model6.fit()\n","print(fitted6.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SRTRRIeArXfM"},"source":["We can observe that there is no significant change in overall R-square. Since we have dropped only redundant information, the model won’t get impacted adversely. While building multiple regression models, one of the important steps is to check for multicollinearity. "]},{"cell_type":"code","metadata":{"id":"4PkuM4f_sthg"},"source":["vif_cal(x_vars=air_pass.drop([\"Passengers_count\",\"passengers_count_pred\"], axis=1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w7L4FLPns0ww"},"source":["vif_cal(x_vars=air_pass.drop([\"Passengers_count\",\"passengers_count_pred\", \"percent_male_customers\"], axis=1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Po7W-AwBs0eQ"},"source":["vif_cal(x_vars=air_pass.drop([\"Passengers_count\",\"passengers_count_pred\",\"percent_male_customers\", \"percent_delayed_flights\"], axis=1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LJS2FDVXs53F"},"source":["import statsmodels.formula.api as sm\n","model7 = sm.ols(formula='Passengers_count ~ marketing_cost+number_of_trips+customer_ratings+poor_weather_index+percent_female_customers+Holiday_week', data=air_pass)\n","fitted7 = model7.fit()\n","print(fitted7.summary())\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O_w4Ic0vs5dP"},"source":["The model is not suffering from multicollinearity now.Two variables are independent or not does not tell us anything about their impact on the target? We check for independence among the predictor variables. The impact is measured by comparing the predictor variable, against the target variable. "]},{"cell_type":"markdown","metadata":{"id":"6ipCmnSm_nIc"},"source":["## **Individual impact of the variables in regression**\n","While building multiple regression lines, we try to add as many dimensions(predictor variables) as possible to increase the model prediction accuracy.If there are 30 variables in the model that don’t necessarily mean all the variables have a significant impact on the target variable. To find the impact of variables we will use specific measure. \n"]},{"cell_type":"markdown","metadata":{"id":"1PYLWUMZDAHq"},"source":["### **p-value**\n","There is a specific measure to identify the impact of the individual variables. The measure name is p-value. To measure the impact of the variables, we perform a test. The test finally gives us a p-value. If P-value is less than 0.05, then that variable is significantly impactful on the target. If the P-value of a variable more than or equal to 0.05, then that variable is not impactful on the target variable. \n","\n","Let us do one example and find individual impact of variables.Build a model using all the variables."]},{"cell_type":"markdown","metadata":{"id":"wdDBoZ_wDj1b"},"source":["You can check the individual variable P-values under the heading P>|t |.\n","\n","For example, in model7 Two variables have higher than 0.05 P-value i.e percent_female_customers and number_of_trips have p-value greater than 0.05. The rest of the variable's P-value is less than 0.05. Their values might have been very small. Those values are rounded-off to 0.000. We can drop these variables from the model. "]},{"cell_type":"code","metadata":{"id":"9M5uju2e_HZO"},"source":["import statsmodels.formula.api as sm\n","model8 = sm.ols(formula='Passengers_count ~ marketing_cost+customer_ratings+poor_weather_index+Holiday_week', data=air_pass)\n","fitted8 = model8.fit()\n","print(fitted8.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B0gbouhcEcK9"},"source":["We can see from the output that R-squared has not dropped significantly. Since we dropped non-impactful variables, they will not have any adverse effects on the model and its accuracy. On the other hand, what will happen if we drop an impactful variable? We will see a significant drop in un R-squared value."]},{"cell_type":"markdown","metadata":{"id":"7WGUq9GvEtIr"},"source":["Let us see what happens if we drop an impactful variable. Suppose we drop marketing_cost and build a model on remaining variables."]},{"cell_type":"code","metadata":{"id":"QnjECRq__Kfo"},"source":["import statsmodels.formula.api as sm\n","model9 = sm.ols(formula='Passengers_count ~  customer_ratings+poor_weather_index+Holiday_week', data=air_pass)\n","fitted9 = model9.fit()\n","print(fitted9.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3T9JrdkCFWao"},"source":["We dropped an impactful variable, and the output shows a significant drop in R-squared value. The value of R-squared dropped from 90% to 62%. We should keep this variable in the model. So we can safely conclude that if P-value is more than more equal to 0.05, we can drop such variables. This 0.05 is the most widely used industry-standard value."]},{"cell_type":"markdown","metadata":{"id":"2UpK1i4NFr8i"},"source":["The variable impact has no relation to variable independence. We started with six independent variables. There are only four impactful variables, and two are unimpactful. So, a variable is impactful or not can not be estimated based on their independence. "]},{"cell_type":"markdown","metadata":{"id":"krmNdmrk_SMw"},"source":["## Logistic Regression\n","We have discussed linear regression in detail till now. Let us look at this below example where we are trying to predict whether a customer will buy a product or not based on the income. A very simple example where income is the predictor and buying is the target. "]},{"cell_type":"code","metadata":{"id":"f-xbLEFR_PWH"},"source":["#product_sales=pd.read_csv(r\"/content/drive/My Drive/DataSets/Chapter-3/Datasets/Product_sales.csv\")\n","product_sales=pd.read_csv(\"https://raw.githubusercontent.com/venkatareddykonasani/ML_DL_py_TF/master/Chapter3_Regression_Logistic/Datasets/Product_sales.csv\")\n","print(product_sales.columns)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-LRDiM30_Zud"},"source":["import statsmodels.formula.api as sm\n","model10 = sm.ols(formula='Bought ~  Income', data=product_sales)\n","fitted10 = model10.fit()\n","print(fitted10.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y3Gnb7wHGcEI"},"source":["There seem to be no major issues in the model. We will go ahead with the predictions. This model accepts income as input and predicts whether a customer will buy or not. Below is the code for obtaining the predicted values from the model"]},{"cell_type":"code","metadata":{"id":"DX_xks7p_cg1"},"source":["new_data=pd.DataFrame({\"Income\":[4000]})\n","print(fitted10.predict(new_data))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_7AcIMa9_f6-"},"source":["new_data1=pd.DataFrame({\"Income\":[85000]})\n","print(fitted10.predict(new_data1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G_cDdEebHB-Q"},"source":["The above output shows that when income is 4,000, the predicted value is -0.096753 when income is 85,000, then the predicted value is 1.599893. \n","* There is something wrong with these predictions. There is something wrong with this model itself. \n","* The target variable bought takes two values 0 and 1. Here class-0 means not-buying, and class-1 mean buying. There is no other value it takes. \n","* The predicted value for 4,000 income is  -0.096753. What does a negative value mean? We know the meaning of class-0. \n","* There is no such negative class in output. \n","Similarly, the predicted value for 85,000 income is  1.599893. What is the meaning of 1.59? We know the meaning of class-1. There is no definition for 1.59.  \n","\n","Let us have a look at some sample data points and draw the scatter plot between predictor and target variables to get a better idea of the data."]},{"cell_type":"code","metadata":{"id":"gEr9TM92_jA-"},"source":["print(product_sales.sample(10))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ycR8yHFa_r1v"},"source":["import matplotlib.pyplot as plt\n","plt.scatter(product_sales[\"Income\"], product_sales[\"Bought\"])\n","plt.title('Income vs Bought Plot')\n","plt.xlabel('Income')\n","plt.ylabel('Bought')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5Lj5lAdzHgGC"},"source":["From the output, it is clear that the data is not in the format that we generally assume for a regression line. The whole data is concentrated at two places, class-0 and class-1. For this data, we are trying to fit a linear regression line. A straight line is not a good representation of this data. There is no way that a straight line can go through all the points in this data. \n","\n","Let us try to draw the regression line on top of this data to get a better idea."]},{"cell_type":"code","metadata":{"id":"UrZDc3zC_7bE"},"source":["pred_values= fitted10.predict(product_sales[\"Income\"]) \n","plt.scatter(product_sales[\"Income\"], product_sales[\"Bought\"])\n","plt.plot(product_sales[\"Income\"], pred_values, color='green')\n","plt.title('Income vs Bought Plot')\n","plt.xlabel('Income')\n","plt.ylabel('Bought')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Skj85fWxH3Ua"},"source":["We see the regression line. It is not going through all the points in this data. It is not possible to build a linear regression model for this data. We can conclude that linear regression I not suitable for classification. The target variable in our classification takes limited classes. In our example, we are trying to predict buying or not buying."]},{"cell_type":"markdown","metadata":{"id":"M-ayiQ_0INH4"},"source":["In fact, in most of the business cases, we find classification problems. For example, in the fraud detection model, we give more preference to the prediction of Fraud vs. No-Fraud. The amount of fraud in a transaction is secondary. Most of the problem statements in the real-world are related to classification. Regression works for continuous output or point predictions. Regression can not be used for solving classification problems. "]},{"cell_type":"markdown","metadata":{"id":"fz6tKc2gIoGc"},"source":["### Logistic Regression model building\n","Since the logistic function is the best option for our data, we will be building a logistic regression line instead of a linear regression line.The syntax for building a logistic regression is not very different from building a linear regression line. Below is the code for building a logistic regression line. "]},{"cell_type":"code","metadata":{"id":"Oes2ItKtHqDM"},"source":["import statsmodels.api as sm\n","logit_model=sm.Logit(product_sales[\"Bought\"],product_sales[\"Income\"])\n","#Model with intercept\n","logit_model1=sm.Logit(product_sales[\"Bought\"],sm.add_constant(product_sales[\"Income\"]))\n","logit_fit1=logit_model1.fit()\n","print(logit_fit1.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MI-EudV5Ltiq"},"source":["From this output, we can extract the logistic regression coefficients 0 and 1 and, we can complete the logistic regression line equation. This equation can further be used in prediction. \n","\n","In the previous example, we build a linear regression line for this data. And we got the predictions for the x-values 4000 and 85000. We will use the above logistic regression line and try to get the predictions for the same values. We expect this line to give meaningful and accurate predictions. \n"]},{"cell_type":"code","metadata":{"id":"0Nq-W1wfHs2n"},"source":["new_data=pd.DataFrame({\"Constant\":[1,1],\"Income\":[4000, 85000]})\n","print(logit_fit1.predict(new_data))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EtuEumurMCm4"},"source":["In the code, we need to mention the constant value as 1, and this is to accommodate for the intercept term. We have added an intercept using the function sm.add_constant(). We didn’t need to do it in the case of linear regression. But here we need to mention it separately. \n","\n","The predicted value for income 4000 is  0, and income 85000 is 1. There is a slight difference between our manual calculation and predict() function. This difference is due to rounding off the values of beta_0 and beta_1. Always use predict function to get accurate results. \n"]},{"cell_type":"markdown","metadata":{"id":"fGXCR2LPOlJd"},"source":["This below code is used for drawing the logistic regression line. You may not need to draw this while solving any business problem. This code is just for the demonstration purpose."]},{"cell_type":"code","metadata":{"id":"_N1eO733HvO4"},"source":["new_data=product_sales.drop([\"Bought\"], axis=1)\n","new_data[\"Constant\"]=1\n","new_data=new_data[[\"Constant\",\"Income\"]]\n","#Pass the variables to get the predicted values. Add actual values in a new column \n","new_data[\"pred_values\"]= logit_fit1.predict(new_data)\n","new_data[\"Actual\"]=product_sales[\"Bought\"]\n","#Sort the data and draw the graph\n","new_data=new_data.sort_values([\"pred_values\"])\n","plt.scatter(new_data[\"Income\"], new_data[\"Actual\"])\n","plt.plot(new_data[\"Income\"], new_data[\"pred_values\"], color='green')\n","#Add lables and title \n","plt.title('Predicted vs Actual Plot')\n","plt.xlabel('Income')\n","plt.ylabel('Bought')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-p6e7im0PfkU"},"source":["The above code gives us the below graph. We can observe that the logistic regression line is a much better option than a linear regression line. "]},{"cell_type":"markdown","metadata":{"id":"wbvA8v2QP099"},"source":["### Accuracy of Logistic Regression line\n","In the above exercise, we built a logistic regression line, and we got the predicted values from the model. Before going for predictions, we should get an idea of the accuracy of the model. In linear regression, we used R-squared value to get an idea of the accuracy of the model. R-square value tells us the explained variation in the target variable. A variance-based measure may not work here. The target variable takes two values 0 and 1. There is hardly any variance. We will try to define a better measure of model performance. "]},{"cell_type":"code","metadata":{"id":"pzncmOQlHyS-"},"source":["print(product_sales.head(10))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ONS0COMZQzGk"},"source":["Predict function is used for extracting the predicted values. We need to pass a “constant” column separately. Logistic regression predicted values are bounded between 0 and 1. We are rounding off the predicted values so that the final values will be either class-0 or class-1"]},{"cell_type":"code","metadata":{"id":"Sz0le1SfH1Y5"},"source":["product_sales[\"Constant\"]=1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O4Dn4KPIH4NU"},"source":["product_sales[\"pred_Bought\"]=logit_fit1.predict(product_sales[[\"Constant\",\"Income\"]])\n","product_sales[\"pred_Bought\"]=round(product_sales[\"pred_Bought\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zfKcfPopH6mW"},"source":["print(product_sales[[\"Bought\",\"pred_Bought\"]])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zoOWMdQ0SAhn"},"source":["Predicting zero as zero and one as one is the right classification by the model. The other two cases i.e predicting zero as one and one as zero are wrong classifications by the model.\n","There is one matrix which is  very famous in classification algorithms named as confusion matrix. We create a confusion matrix and calculate the accuracy from it.Below code creates confusion matrix on our data. "]},{"cell_type":"code","metadata":{"id":"fSwJde42H8oy"},"source":["from sklearn.metrics import confusion_matrix\n","cm1 = confusion_matrix(product_sales[\"Bought\"],product_sales[\"pred_Bought\"])\n","print(cm1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"duTv4ZmZIAPj"},"source":["accuracy1=(cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n","print(accuracy1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D19dk6X4WgrI"},"source":["The output shows that the model has 98% accuracy. A general industry standard for a good model is to have above 80% accuracy."]},{"cell_type":"markdown","metadata":{"id":"firkn_BmWi3O"},"source":["## **Multiple Logistic Regression line**\n","For the classification type of problems, we go for logistic regression. It is not just one predictor that helps us in predicting the target variable. There can be several factors that will impact the target class. A multiple logistic regression line is used in all such cases. It is just an extension of a simple logistic regression line. "]},{"cell_type":"code","metadata":{"id":"ZBjL75iYICqb"},"source":["#telco_cust=pd.read_csv(r\"/content/drive/My Drive/DataSets/Chapter-3/Datasets/telco_data.csv\")\n","telco_cust=pd.read_csv(\"https://raw.githubusercontent.com/venkatareddykonasani/ML_DL_py_TF/master/Chapter3_Regression_Logistic/Datasets/telco_data.csv\")\n","\n","print(telco_cust.shape)\n","print(telco_cust.columns)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jmFOI7KXXW8J"},"source":["The target variable is still class-0 and class-1. The confusion matrix calculation and accuracy measure remain the same.\n","\n","The below code is used for building the multiple logistic regression line. The code is the same as the simple logistic regression line. We mention all the rest of the variables at the place of predictor variables. \n"]},{"cell_type":"code","metadata":{"id":"fwIxL3anIFU-"},"source":["import statsmodels.discrete.discrete_model as sd\n","logit_model2=sd.Logit(telco_cust['Active_cust'],telco_cust[[\"estimated_income\"]+['months_on_network']+['complaints_count']+['plan_changes_count']+['relocated_new_place']+['monthly_bill_avg']+[\"CSAT_Survey_Score\"]+['high_talktime_flag']+['internet_time']])\n","logit_fit2=logit_model2.fit()\n","print(logit_fit2.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bexee6_SILRK"},"source":["telco_cust[\"pred_Active_cust\"]=logit_fit2.predict(telco_cust.drop([\"Id\",\"Active_cust\"],axis=1))\n","telco_cust[\"pred_Active_cust\"]=round(telco_cust[\"pred_Active_cust\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z-eIO_fOJwGW"},"source":["from sklearn.metrics import confusion_matrix\n","cm2 = confusion_matrix(telco_cust[\"Active_cust\"],telco_cust[\"pred_Active_cust\"])\n","print(cm2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9H90jP1yJyuW"},"source":["accuracy2=(cm2[0,0]+cm2[1,1])/(cm2[0,0]+cm2[0,1]+cm2[1,0]+cm2[1,1])\n","print(accuracy2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0P2gO0MlYG7M"},"source":["## Multicollinearity in Logistic Regression \n","In multiple linear regression, we have seen multicollinearity as an issue. The beta coefficients can not be trusted in the presence of interdependency of variables. Multicollinearity is an issue even in multiple logistic regression. The only notable change that happened from linear regression to logistic regression is in the target variable y. In linear regression, the target variable y was continuous. In logistic regression, the target variable is a class or categorical. \n","\n","While dealing with multicollinearity, we ignore the target variable. Multicollinearity is related to predictor variables. Multicollinearity exists in logistic regression also. The multicollinearity problem in linear regression is almost the same as the multicollinearity problem in logistic regression. The detection of multicollinearity using VIF is also exactly the same. We can use the same VIF function.\n","\n","Here also we will drop the variable if VIF value is greater than 5."]},{"cell_type":"code","metadata":{"id":"ha1KGvEOJ003"},"source":["def vif_cal(x_vars):\n","    xvar_names=x_vars.columns\n","    for i in range(0,xvar_names.shape[0]):\n","        y=x_vars[xvar_names[i]] \n","        x=x_vars[xvar_names.drop(xvar_names[i])]\n","        rsq=sm.ols(formula=\"y~x\", data=x_vars).fit().rsquared  \n","        vif=round(1/(1-rsq),2)\n","        print (xvar_names[i], \" VIF = \" , vif)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aigkU71pYwGI"},"source":["We will use the above function to detect multicollinearity in our data"]},{"cell_type":"code","metadata":{"id":"DN_d8gNXJ4En"},"source":["import statsmodels.formula.api as sm\n","vif_cal(x_vars=telco_cust.drop([\"Id\",\"Active_cust\",\"pred_Active_cust\"], axis=1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cEAh3ezwY3vu"},"source":["From the output, it is very clear that few variables are interdependent. We need to drop the variable with the highest VIF value. We will drop “CSAT_Survey_Score” and re-calculate VIF values for the rest of the variables"]},{"cell_type":"code","metadata":{"id":"5SY1jMwbJ6Lq"},"source":["vif_cal(x_vars=telco_cust.drop([\"Id\",\"Active_cust\",\"pred_Active_cust\",\"CSAT_Survey_Score\"], axis=1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q52-Mg-TY99g"},"source":["From the output, we can observe that all the variables have VIF less than five. We can conclude that all these variables are independent. We can use all of them in the model. Below is the model after removing the multicollinearity. "]},{"cell_type":"code","metadata":{"id":"KQnrv3zkKLKt"},"source":["logit_model3=sd.Logit(telco_cust['Active_cust'],telco_cust[[\"estimated_income\"]+['months_on_network']+['complaints_count']+['plan_changes_count']+['relocated_new_place']+['monthly_bill_avg']+['high_talktime_flag']+['internet_time']])\n","logit_fit3=logit_model3.fit()\n","print(logit_fit3.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qCOd4WY3LoPI"},"source":["telco_cust[\"pred_Active_cust\"]=logit_fit3.predict(telco_cust.drop([\"Id\",\"Active_cust\",\"pred_Active_cust\",\"CSAT_Survey_Score\"],axis=1))\n","telco_cust[\"pred_Active_cust\"]=round(telco_cust[\"pred_Active_cust\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ItCsEydzL0GB"},"source":["from sklearn.metrics import confusion_matrix\n","cm3 = confusion_matrix(telco_cust[\"Active_cust\"],telco_cust[\"pred_Active_cust\"])\n","print(cm3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"88h3q0aXL3Ko"},"source":["accuracy3=(cm3[0,0]+cm3[1,1])/(cm3[0,0]+cm3[0,1]+cm3[1,0]+cm3[1,1])\n","print(accuracy3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"itsNm4HIZSkb"},"source":["This model has an accuracy of 86.4%. We have to look into individual variable impacts now. "]},{"cell_type":"markdown","metadata":{"id":"K5Pi20M2aWxD"},"source":["## Individual impact of variables in Logistic Regression\n","\n","Once we are done with the multicollinearity detection with VIF. We will be left with all the independent variables. All these independent variables may not be impactful. If we have a list of 20 variables in the model, it doesn’t necessarily mean that all of them are impactful on the target.\n","\n","Again this is same as linear regression."]},{"cell_type":"code","metadata":{"id":"Y3OmPtCkMjxk"},"source":["logit_model4=sd.Logit(telco_cust['Active_cust'],telco_cust[['months_on_network']+['complaints_count']+['plan_changes_count']+['relocated_new_place']+['monthly_bill_avg']+['internet_time']])\n","logit_fit4=logit_model4.fit()\n","print(logit_fit4.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YsWd9v0UMmzL"},"source":["telco_cust[\"pred_Active_cust\"]=logit_fit4.predict(telco_cust.drop([\"Id\",\"Active_cust\",\"pred_Active_cust\",\"CSAT_Survey_Score\",\"estimated_income\",\"high_talktime_flag\"],axis=1))\n","telco_cust[\"pred_Active_cust\"]=round(telco_cust[\"pred_Active_cust\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YDgoAxf0Mwk5"},"source":["from sklearn.metrics import confusion_matrix\n","cm4= confusion_matrix(telco_cust[\"Active_cust\"],telco_cust[\"pred_Active_cust\"])\n","print(cm3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cMgOjJ10Myn4"},"source":["accuracy4=(cm4[0,0]+cm4[1,1])/(cm4[0,0]+cm4[0,1]+cm4[1,0]+cm4[1,1])\n","print(accuracy4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"My-lmmAZwH4o"},"source":[""],"execution_count":null,"outputs":[]}]}