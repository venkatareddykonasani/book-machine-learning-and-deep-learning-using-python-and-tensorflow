{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Chapter-10.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"W_H9c5ceUdt3"},"source":["# Chapter-10: Deep Learning Hyperparameters"]},{"cell_type":"markdown","metadata":{"id":"gre3OQb5VBEQ"},"source":["## Regularization\n","In regularization, we choose a relatively large neural network; since it will lead to overfitting, we will keep some constraints on the weights so that they are always at a lower value. By this way, we have several nodes to explain the complexity, since each node is not showing its full power, the overfitting is also in check. In simple words, instead of dropping the hidden nodes, we are keeping them but with lower weights.\n"]},{"cell_type":"markdown","metadata":{"id":"wmW-iBnunshM"},"source":["### Regularization in regression\n","In Regression, we try to minimize the sum of squares of errors. The regression will lead to overfitting if we have too any polynomial terms in the predictor variables list. If we overdo the feature engineering and create too many derived variables, in that case, also it may lead to overfitting. We can still retain too many features by using the regularization method"]},{"cell_type":"markdown","metadata":{"id":"M8k_g4du936X"},"source":["Below is a worked-out example on small data set to demonstrate how regularization works on regression."]},{"cell_type":"code","metadata":{"id":"_tLrlz_WdNVb"},"source":["import pandas as pd\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3_bGDb2H-V93"},"source":["The data set"]},{"cell_type":"code","metadata":{"id":"kg5_IwV7dot2"},"source":["x=[-0.99768,-0.69574,-0.40373,-0.10236,0.22024,0.47742,0.82229]\n","y=[2.0885,1.1646,0.3287,0.46013,0.44808,0.10013,-0.32952]\n","\n","input_data = pd.DataFrame(list(zip(x, y)), columns =['x', 'y']) \n","print(input_data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r5yQ4zsw-i2Z"},"source":["Plotting the data"]},{"cell_type":"code","metadata":{"id":"PI7xNtjPdrzV"},"source":["x = np.array(input_data.x)\n","y = input_data.y\n","#scatter plot x and y\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","plt.title(\"Input data\", fontsize=20)\n","plt.scatter(x,y,s=50,c=\"g\")\n","plt.xlabel(\"X\")\n","plt.ylabel(\"Y\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WgoESQCj-RT9"},"source":["We will now build a simple regression line and a fifth-order polynomial regression. Simple regression\n","will be an under-fitted model for the above data and fifth-order polynomial will be overfitted model."]},{"cell_type":"markdown","metadata":{"id":"V7ohnLfv-otO"},"source":["Simple regression"]},{"cell_type":"code","metadata":{"id":"_BMFrQSOdvLz"},"source":["import statsmodels.api as sm\n","x1 = sm.add_constant(x)\n","m1 = sm.OLS(y,x1).fit()\n","#SSE\n","print(\"m1 SSE\", m1.ssr)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pwp022WA-7Li"},"source":["Second order polynomial regression"]},{"cell_type":"code","metadata":{"id":"_2ErQcNHdyqN"},"source":["x2 = sm.add_constant(np.column_stack([x,np.square(x)]))\n","m2 = sm.OLS(y,x2).fit()\n","print(\"m2 SSE\", m2.ssr)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8b6yHeP3_A4u"},"source":["Fifth order polynomial regression"]},{"cell_type":"code","metadata":{"id":"kq9ssAxyd1W4"},"source":["x3 = sm.add_constant(np.column_stack([x, np.power(x,2),np.power(x,3),np.power(x,4),np.power(x,5)]))\n","m3 = sm.OLS(y,x3).fit()\n","print(\"m3 SSE\", m3.ssr)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9f5Csdo0_feP"},"source":["We will take the fifth-order polynomial model, which is already overfitted.\n","\n","We will now use the regularization parameter. The weights will be newly calculated using a\n","regularized cost function. We will get all the six weights but as lambda increases, the weights reduce."]},{"cell_type":"code","metadata":{"id":"rDlOOSuSd3iU"},"source":["X = x3\n","y = np.array(y)\n","n_col = X.shape[1]\n","d = np.identity(n_col)\n","d[0,0] = 0\n","w = []\n","\n","reg =0 \n","w.append(np.linalg.lstsq(X.T.dot(X) + reg * d, X.T.dot(y))[0])\n","\n","reg =1 \n","w.append(np.linalg.lstsq(X.T.dot(X) + reg * d, X.T.dot(y))[0])\n","\n","\n","reg =10 \n","w.append(np.linalg.lstsq(X.T.dot(X) + reg * d, X.T.dot(y))[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yOLQbWwNd679"},"source":["print(\"Regularized weights  lambda=0 \\n\", w[0])\n","print(\"Regularized weights  lambda=1 \\n\", w[1])\n","print(\"Regularized weights  lambda=10 \\n\", w[2])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k88vzxZC_x2D"},"source":["With lamda = 0, we should see the same weights as the fifth-order polynomial regression. We have built that model previously. Let us compare these two models. Below is the code for fetching the old and new weights"]},{"cell_type":"code","metadata":{"id":"SYO7q8red-K5"},"source":["print(\"Regularized Weights With lambda = 0 \\n\", list(w[0]))\n","print(\"Standard Weights With inbuilt package \\n\",list(m3.params))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Zvys_RxAFNZ"},"source":["As expected the regularized weights with lambda=0 are the same as standard weights. Now we will\n","see the plot and observe the results from the three models."]},{"cell_type":"code","metadata":{"id":"Ky5BjxhUeBhg"},"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","plt.rcParams[\"figure.figsize\"] = (8,6)\n","plt.title('Model results for different lambda values', fontsize=20)\n","plt.scatter(x,y, s = 50, c = \"g\")\n","x_new = np.linspace(x.min(), x.max(), 200)\n","plt.plot(x_new, np.poly1d(np.polyfit(x, X.dot(w[0]), 5))(x_new),label='$\\lambda$ = 0', c = \"b\")\n","plt.plot(x_new, np.poly1d(np.polyfit(x, X.dot(w[1]), 5))(x_new),label='$\\lambda$ = 1', c = \"r\")\n","plt.plot(x_new, np.poly1d(np.polyfit(x, X.dot(w[2]), 5))(x_new),label='$\\lambda$ = 10', c = \"g\")\n","plt.legend(loc='upper right');\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zrPH-8wQAoEW"},"source":["Below are the observations from the above graphs. The blue line is the model with lambda=0, this\n","model is overfitted. The green line is the model with lambda=10, and this model is under fitted. The\n","red line is the model with lambda=1, this model is better than the other two models.\n","\n","In conclusion, we can say that the three models are built with fifth-order polynomial. By\n","changing the value of regularization parameter, we can avoid overfitting. From the above output we\n","can choose the model with lambda=1"]},{"cell_type":"code","metadata":{"id":"4niga6_TeHIT"},"source":["#weights\n","print(\"Final Weights \\n\", w[1])\n","#perdiction\n","pred = X.dot(w[1])\n","##SSE\n","SSE_Final = sum(np.square(y-pred))\n","print(\"Final SSE \", SSE_Final)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aWVi_z9z9c38"},"source":["importing required packages and libraries"]},{"cell_type":"code","metadata":{"id":"oOOO94U7eLep"},"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3nyOUds_9ioe"},"source":["loading the dataset"]},{"cell_type":"code","metadata":{"id":"1O0leBZfeNnQ"},"source":["(X_train, Y_train), (X_test, Y_test) = keras.datasets.mnist.load_data()\n","num_classes=10\n","x_train = X_train.reshape(60000, 784)\n","x_test = X_test.reshape(10000, 784)\n","x_train = x_train.astype('float32')\n","x_test = x_test.astype('float32')\n","x_train /= 255\n","x_test /= 255\n","\n","## Convert class vectors to binary class matrices\n","y_train = keras.utils.to_categorical(Y_train, num_classes)\n","y_test = keras.utils.to_categorical(Y_test, num_classes)\n","\n","print(x_train.shape, 'train input samples')\n","print(x_test.shape, 'test input samples')\n","\n","print(y_train.shape, 'train output samples')\n","print(y_test.shape, 'test output samples')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UqVbfL_Z9vQq"},"source":["### L1 and L2 regularization\n","* In regression adding too many polynomial terms led to overfitting, we did not reduce the number of\n","polynomial terms. We used a regularized cost function. Until now we minimized the sum of squares of weights. This method is known as L2 norm or L2 regularization. A regression model with this method is known as Ridge regression.\n","* Alternatively, we can use the cost function that minimizes the sum of absolute values of the weights.\n","This method is known as L1 norm or L1 regularization. In regression, it is known as Losso regression"]},{"cell_type":"markdown","metadata":{"id":"5DC2Ymxh-tHX"},"source":["In Kears we can use kernel_regularizer parameter inside layers.Dense() function. Below is the code\n","for building the model without regularization and with regularization.\n","\n","When we build a model with too many hidden nodes, we can see the accuracy of train data is high\n","starting from the first epoch. The same configuration model shows less accuracy in its epochs while\n","training."]},{"cell_type":"markdown","metadata":{"id":"CW2zK-qD-yYs"},"source":["* without regularization"]},{"cell_type":"code","metadata":{"id":"ZfQSiguIeQex"},"source":["model = keras.Sequential()\n","model.add(layers.Dense(256, activation='sigmoid', input_shape=(784,)))\n","model.add(layers.Dense(128, activation='sigmoid'))\n","model.add(layers.Dense(10, activation='softmax'))\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gaCs7ORxeSu9"},"source":["model.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n","model.fit(x_train, y_train,epochs=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oSOEPJL2eVRS"},"source":["#Final Results\n","loss, acc = model.evaluate(x_train,  y_train, verbose=2)\n","print(\"Train Accuracy: {:5.2f}%\".format(100*acc))\n","\n","loss, acc = model.evaluate(x_test,  y_test, verbose=2)\n","print(\"Test Accuracy: {:5.2f}%\".format(100*acc))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"297qIkxc_VYG"},"source":["We can see slight overfitting in this model. We can see above 90% accuracy from the second epoch\n","itself. We will build the same model with regularization now."]},{"cell_type":"markdown","metadata":{"id":"elISDNX-_Dli"},"source":["* with regularization"]},{"cell_type":"code","metadata":{"id":"PePPbk40elxW"},"source":["from tensorflow.keras import regularizers\n","model_r = keras.Sequential()\n","model_r.add(layers.Dense(256, activation='sigmoid', input_shape=(784,), kernel_regularizer=regularizers.l2(0.01)))\n","model_r.add(layers.Dense(128, activation='sigmoid',kernel_regularizer=regularizers.l2(0.01)))\n","model_r.add(layers.Dense(10, activation='softmax'))\n","model_r.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Sdi8lw3eon8"},"source":["model_r.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n","model_r.fit(x_train, y_train,epochs=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Eg3baNtxeqfF"},"source":["#Final Results\n","loss, acc = model_r.evaluate(x_train,  y_train, verbose=2)\n","print(\"Train Accuracy: {:5.2f}%\".format(100*acc))\n","\n","loss, acc = model_r.evaluate(x_test,  y_test, verbose=2)\n","print(\"Test Accuracy: {:5.2f}%\".format(100*acc))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UEUpTCXh_fJs"},"source":["We can now see the impact of regularization on the weights. Epoch by epoch comparison on the\n","train data reveals the impact of the penalty on weights. The final model also shows no signs of\n","overfitting. We can apply L1 regularization also in a similar manner using the parameter\n","kernel_regularizer=regularizers.l1(0.01)"]},{"cell_type":"markdown","metadata":{"id":"73KKT35TUsuI"},"source":["### Dropout regularization\n","There is one more regularization that helps us to reduce the dominance of individual hidden nodes. The dropout method is another effective way of avoiding overfitting. Ignoring a few hidden nodes while training the model is called the dropout method.\n","\n","There are three crucial points to note in the dropout method.\n","* The first point is that dropout is not applied at the overall network level; it is applied at each\n","hidden layer level. We can even apply dropout on a few layers and keep all nodes in the rest\n","of the layers.\n","* The second point is, the drop out happens at each iteration. We do not drop the nodes once\n","and train the network with alliterations. We drop the weights randomly in each epoch. We\n","can not guess what is the network architecture in a given iteration.\n","* The third point is about weights. All the weights are considered at the time of prediction.\n","Each weight is multiplied by q; here q=1-p."]},{"cell_type":"markdown","metadata":{"id":"RT6PNfT8W8v6"},"source":["We need to mention dropout as a layer. It is an imaginary layer that is applied to the hidden layers.\n","The dropout rate can vary from layer to layer"]},{"cell_type":"code","metadata":{"id":"btANNCyAeuAb"},"source":["from tensorflow.keras.layers import Dropout\n","model_rd = keras.Sequential()\n","\n","model_rd.add(layers.Dense(256, activation='sigmoid', input_shape=(784,)))\n","model_rd.add(Dropout(0.7))\n","\n","model_rd.add(layers.Dense(128, activation='sigmoid'))\n","model_rd.add(Dropout(0.6))\n","\n","model_rd.add(layers.Dense(10, activation='softmax'))\n","model_rd.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lu9ox4iCXDjq"},"source":["From the above code, we can see the dropout layer after the hidden layer. We choose p=0.7 which\n","means 70% of the nodes will be dropped from the first hidden layer and 60% of the nodes will be\n","dropped from the second hidden layer. At any given iteration, we will see only 77 nodes in the first\n","hidden layer and 51 nodes in the second layer."]},{"cell_type":"markdown","metadata":{"id":"JHmV4Yz9XJc-"},"source":["Drop out is an abstract layer with zero nodes. We can now train the model. In each epoch, we will\n","see less accuracy, as we are using only fewer nodes. Finally, when we calculate the accuracy of train\n","and test data, we will see a higher value."]},{"cell_type":"code","metadata":{"id":"4fbsia6qewig"},"source":["model_rd.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n","model_rd.fit(x_train, y_train,epochs=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ig2A__DVexw0"},"source":["#Final Results\n","loss, acc = model_rd.evaluate(x_train,  y_train, verbose=2)\n","print(\"Train Accuracy: {:5.2f}%\".format(100*acc))\n","\n","loss, acc = model_rd.evaluate(x_test,  y_test, verbose=2)\n","print(\"Test Accuracy: {:5.2f}%\".format(100*acc))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VnSIbFMUXO9B"},"source":["We can see from the output the model is free of overfitting."]},{"cell_type":"markdown","metadata":{"id":"wEvw80jyXP2P"},"source":["### Early stopping method\n","The early stopping method follows a simple approach to avoid overfitting. If there are too many\n","hidden nodes and layers, then the accuracy on train data increases as the number of epochs\n","increases. With sufficient hidden nodes and sufficient epochs, the accuracy might even reach 100%.\n","\n","First, we need to know how to store the model and its weights in each epoch. For storing the model\n","weights, we use the h5py package. Using this package, we can store the model weights in a file. The\n","model weights file will have the hdf5 extension."]},{"cell_type":"code","metadata":{"id":"eOLMMWOteza0"},"source":["model_re = keras.Sequential()\n","model_re.add(layers.Dense(256, activation='sigmoid', input_shape=(784,)))\n","model_re.add(layers.Dense(128, activation='sigmoid'))\n","model_re.add(layers.Dense(10, activation='softmax'))\n","model_re.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jXNi8cA4e1M_"},"source":["model_re.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n","#Enable saving checkpoints\n","# Checkpoint the weights when validation accuracy improves\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","import h5py\n","\n","# checkpoint\n","#dont forget to create a directory to store the checkpoints:\"early_stopping_checkpoints\"\n","checkpoint = ModelCheckpoint(\"\\early_stopping_checkpoints\\epoch-{epoch:02d}.hdf5\")\n","model_re.fit(x_train, y_train,epochs=10,validation_data=(x_test, y_test),callbacks=[checkpoint])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2cOdG5jLfY07"},"source":["The above code saves all the model weight files inside the directory that we have mentioned in the\n","code. We can load the model weights from a particular epoch. Imagine that after\n","epoch 7 the model getting into overfitting then we can load the weights from epoch7 using\n","load_weights() function."]},{"cell_type":"code","metadata":{"id":"oldQ1109fik9"},"source":["model_re.load_weights(\"\\early_stopping_checkpoints\\epoch-07.hdf5\")# change the file name to the epoch you want to load"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E305n0xIgNj5"},"source":["Either we can manually perform early stopping regularization using the above weights storing and\n","load_weights approach or we can directly use keras.callbacks.EarlyStopping() function. In this\n","function, we have to mention the validation measure and minimum improvement in the test data\n","that we want t see in each iteration."]},{"cell_type":"code","metadata":{"id":"ZmfwXsqCIDWF"},"source":["model_re = keras.Sequential()\n","model_re.add(layers.Dense(256, activation='sigmoid', input_shape=(784,)))\n","model_re.add(layers.Dense(128, activation='sigmoid'))\n","model_re.add(layers.Dense(10, activation='softmax'))\n","\n","model_re.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","es = keras.callbacks.EarlyStopping(monitor='val_accuracy',\n","                              min_delta=0.01,\n","                              patience=2)\n","\n","#train the model with call back method\n","model_re.fit(x_train, y_train, epochs=30,validation_data=(x_test, y_test), callbacks=[es])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HErxY8HLgmJP"},"source":["In the above code\n","* Monitor â€“ Monitor accuracy or loss\n","* min_delta - The minimum improvement required in each step.\n","* Patience - How many epochs to wait after the monitor termination condition has been\n","reached. Sometimes test accuracy seams to be decreased in an epoch but it increases up\n","after that; to avoid this strict rule, we can use the patience parameter. The model will wait\n","for few more epochs before exiting.\n","* Overall the above code means that terminate the model builing when the accuracy\n","improvemet is less than 0.01 in the validation data for two consecutive epochs.\n","\n","Though we have mentioned 30 epochs, the model will exit when it reaches the min_delta\n","termination criteria."]},{"cell_type":"markdown","metadata":{"id":"OsJ_xH3skI3P"},"source":["## Activation function\n","Below is the example code that shows how to configure a network with different activation\n","functions. We can mention different activation functions in different layers."]},{"cell_type":"code","metadata":{"id":"i9Ng--CdG_pU"},"source":["model2 = keras.Sequential()\n","model2.add(layers.Dense(15, activation='sigmoid', input_shape=(784,)))\n","model2.add(layers.Dense(15, activation='relu'))\n","model2.add(layers.Dense(15, activation='tanh'))\n","model2.add(layers.Dense(15, activation='relu'))\n","model2.add(layers.Dense(10, activation='softmax'))\n","model2.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V2zj-y-WHFyI"},"source":["model2.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n","model2.fit(x_train, y_train,epochs=10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6JePPOY_Yl4c"},"source":["## Learning function\n","To reach the minimum value of the error, We move the weights in that direction where overall error\n","reduces. A negative gradient of error gives us direction. We are multiplying the actual gradient term\n","with . This is known as the learning rate. By increasing or decreasing the values of the learning rate,\n","we can dictate how much should the weights move in one iteration."]},{"cell_type":"markdown","metadata":{"id":"uifsqPLtbL_9"},"source":["The learning rate is part of the gradient descent function. We need to mention the learning rate in\n","optimizes function. For example, tf.keras.optimizers.SGD() is an optimiser function. SGD is stochastic\n","Gradient Descent, and it is a specific type of gradient descent function. Below is the code to mention\n","the learning rate in optimizer function"]},{"cell_type":"code","metadata":{"id":"ySxc7HNeHHyW"},"source":["model3 = keras.Sequential()\n","model3.add(layers.Dense(20, activation='sigmoid', input_shape=(784,)))\n","model3.add(layers.Dense(20, activation='sigmoid'))\n","model3.add(layers.Dense(10, activation='softmax'))\n","model3.summary()\n","\n","#High Learning Rate\n","opt_new = tf.keras.optimizers.SGD(learning_rate=10)\n","model3.compile(optimizer=opt_new, loss='categorical_crossentropy', metrics=['accuracy'])\n","model3.fit(x_train, y_train,epochs=20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E7Rwc-6vbgao"},"source":["From the above output we can see that the model got stuck at an accuracy of 0.2. The weights must\n","have been oscillating between two points, not able to penetrate further inside a minimum. Now we\n","will try with a very low learning rate."]},{"cell_type":"code","metadata":{"id":"uKmBzs5sHK7H"},"source":["model3 = keras.Sequential()\n","model3.add(layers.Dense(20, activation='sigmoid', input_shape=(784,)))\n","model3.add(layers.Dense(20, activation='sigmoid'))\n","model3.add(layers.Dense(10, activation='softmax'))\n","model3.summary()\n","\n","#Low learning rate\n","opt_new = tf.keras.optimizers.SGD(learning_rate=0.00001)\n","model3.compile(optimizer=opt_new, loss='categorical_crossentropy', metrics=['accuracy'])\n","model3.fit(x_train, y_train,epochs=20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VbCktAARbm7w"},"source":["We can see from the above output that the model is either stuck at a local minimum or model is\n","extremely slow in learning. Now we will try building the model with medium learning rate."]},{"cell_type":"code","metadata":{"id":"Z9By6BDJHRQ6"},"source":["model3 = keras.Sequential()\n","model3.add(layers.Dense(20, activation='sigmoid', input_shape=(784,)))\n","model3.add(layers.Dense(20, activation='sigmoid'))\n","model3.add(layers.Dense(10, activation='softmax'))\n","model3.summary()\n","\n","#Optimal learning rate\n","opt_new = tf.keras.optimizers.SGD(learning_rate=0.01)\n","model3.compile(optimizer=opt_new, loss='categorical_crossentropy', metrics=['accuracy'])\n","model3.fit(x_train, y_train,epochs=20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Ef6lGskbtp7"},"source":["There is no shortcut for reaching the optimal learning rate. Since the optimal learning rate is a range\n","of values, it will not be an enormous challenge to find the optimal learning rate."]},{"cell_type":"markdown","metadata":{"id":"h0nKRoWWb6KP"},"source":["## Momentum\n","The learning rate can be made better by adding one more additional factor called momentum. In the\n","initial epochs, the weights will be changed by larger values. As the error function is reaching the\n","minimum value then the delta weights will be smaller. If we are reaching a local minimum, then the\n","momentum can help us to push out of local minima and help the algorithm to converge much faster."]},{"cell_type":"markdown","metadata":{"id":"Q2NdTM5McU8Q"},"source":["Momentum should be seen\n","as a helping aid to learning rate, it is not an alternative to the learning rate. Below is the code to\n","include momentum along with the learning rate."]},{"cell_type":"code","metadata":{"id":"Z0M0ngQWIZyX"},"source":["model3 = keras.Sequential()\n","model3.add(layers.Dense(20, activation='sigmoid', input_shape=(784,)))\n","model3.add(layers.Dense(20, activation='sigmoid'))\n","model3.add(layers.Dense(10, activation='softmax'))\n","model3.summary()\n","\n","#Optimal learning rate\n","opt_new = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.5)\n","model3.compile(optimizer=opt_new, loss='categorical_crossentropy', metrics=['accuracy'])\n","model3.fit(x_train, y_train,epochs=20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nwHAtYp1caLa"},"source":["The above code is the same as the previous model. We have included a momentum term\n","additionally.\n","\n","We can observe a faster convergence after including the momentum term. This parameter comes\n","very handily for broad and deep neural networks."]},{"cell_type":"markdown","metadata":{"id":"jMLnfH6sch72"},"source":["## Optimizers\n","We discussed the gradient descent algorithm for updating the weights. The original theory of\n","Gradient Descent uses all the data to calculate the gradients to update the weights.\n","\n","If there are millions of points, then calculating gradient for all those points in one go takes much\n","time."]},{"cell_type":"markdown","metadata":{"id":"9tv5bqAuchjY"},"source":["### SGD(Stochastic Gradient Descent) \n","Stochastic Gradient Descent formally known as SGD, uses an estimate of gradient instead of an\n","actual gradient. SGD approximates the overall gradient using a single point. This will make sure that\n","the individual gradients calculated faster and weights are getting updated rapidly.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PEZu8xJfdMAd"},"source":["### Minibatch Gradient Descent\n","We will take SGD and make a small modification to it. Instead of calculating gradients for every\n","single record, we will make a batch of records. A small subset of the data and calculate the gradient\n","to update the weights."]},{"cell_type":"code","metadata":{"id":"8RVyaq7qIggh"},"source":["model4 = keras.Sequential()\n","model4.add(layers.Dense(20, activation='sigmoid', input_shape=(784,)))\n","model4.add(layers.Dense(20, activation='sigmoid'))\n","model4.add(layers.Dense(10, activation='softmax'))\n","model4.summary()\n","\n","\n","opt_new = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.5)\n","model4.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","#Batch size=fll data(GD)\n","model4.fit(x_train, y_train,batch_size=x_train.shape[0], epochs=10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P0wzC4OzdeUK"},"source":["We need to pass the batch size parameter in the model.fit() function.\n","\n","From the above output, we can observe that the loss has not reduced drastically even after 10\n","epochs. Now we will build the model with SGD where the batch size is 1"]},{"cell_type":"code","metadata":{"id":"obc28UMQIkhW"},"source":["model4 = keras.Sequential()\n","model4.add(layers.Dense(20, activation='sigmoid', input_shape=(784,)))\n","model4.add(layers.Dense(20, activation='sigmoid'))\n","model4.add(layers.Dense(10, activation='softmax'))\n","model4.summary()\n","\n","\n","opt_new = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.5)\n","model4.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","#Batch size=1 (SGD)\n","model4.fit(x_train, y_train,batch_size=1, epochs=2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wnl-Lwk5dnrO"},"source":["From the above output, we can observe that within two epochs, we achieved high accuracy.\n","However, the problem is in execution time. Each epoch takes a significantly higher amount of time.\n","Now we will build the third model with batch size"]},{"cell_type":"code","metadata":{"id":"IvmP_Q2vInAl"},"source":["model4 = keras.Sequential()\n","model4.add(layers.Dense(20, activation='sigmoid', input_shape=(784,)))\n","model4.add(layers.Dense(20, activation='sigmoid'))\n","model4.add(layers.Dense(10, activation='softmax'))\n","model4.summary()\n","\n","\n","opt_new = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.5)\n","model4.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","#Batch size = 512\n","model4.fit(x_train, y_train,batch_size=512, epochs=10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GOhyVIuKdxAj"},"source":["This output shows better results than GS and SGD. Mini batch GD is used in solving real business\n","problems since it is better than the other two in terms of execution time and accuracy"]},{"cell_type":"code","metadata":{"id":"rOkQB8lgIsuE"},"source":[""],"execution_count":null,"outputs":[]}]}