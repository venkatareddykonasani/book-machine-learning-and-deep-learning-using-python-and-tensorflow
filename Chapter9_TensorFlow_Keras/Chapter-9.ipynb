{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Chapter-9.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"tLJjshnD8-ry"},"source":["# Tensorflow and Keras"]},{"cell_type":"markdown","metadata":{"id":"v85LQZil9Dwy"},"source":["## Deep neural networks\n","A neural network with a single layer is known as a shallow neural network. When the problem is non-linear, then we may need to add multiple hidden layers. Almost all computer vision problems require multiple hidden layers. A neural network with multiple hidden layers is known as a deep neural network."]},{"cell_type":"markdown","metadata":{"id":"W9e-2hlwB2Q_"},"source":["### Number of parameters\n","The deep neural networks take a much higher execution time and much higher computational resources compared to machine learning models. The main reason behind the overlong training time is the number of free parameters in the optimization function. The free parameters are nothing but the weights in the gradient descent algorithm. The number of weight parameters runs into thousands for deep neural networks. An optimization function has to perform millions of calculations to find the final optimal weights for these deep neural networks. We need efficiently written codes that can perform multiple calculations parallelly and give us the results in stipulated time. The standard packages like sci-kit learn can work well for machine learning problems but they fail on these deep neural networks. There are some packages in python that work very efficiently on deep learning problems.\n"]},{"cell_type":"markdown","metadata":{"id":"MtcmoA7-FvBB"},"source":["## Tensorflow\n","TensorFlow is a library that can effectively perform complex mathematical calculations. Tensor means a multidimensional vector. We can think tensor as data, and TensorFlow is data flow. As we discussed earlier, the deep learning algorithms involve complex matrix calculations. TensorFlow works best when it comes to matrix computations. Tensorflow is scalable to multi-CPUs and even GPUs(Graphics Processing Units). TensorFlow represents the data and calculations in the form of computation graphs."]},{"cell_type":"markdown","metadata":{"id":"gXwgaDeRFf0a"},"source":["### Installing Tensorflow\n","We Use the command pip install to install any new package in python. We can run pip install in anaconda prompt or we can directly execute the installation step in Jupyter notebook using “!pip install” command. Here in Google colab the same command is used. Below is the command to install TensorFlow. We can mention the exact version if required"]},{"cell_type":"code","metadata":{"id":"rWw7DdqV4-BS"},"source":["!pip install tensorflow"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8iZFB6V0Jype"},"source":["The above command requires an internet connection to download the TensorFlow package from the website. Below is the code for checking the installed version.\n"]},{"cell_type":"code","metadata":{"id":"_2OCT2UeDfbV"},"source":["!pip show tensorflow"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"99hso1UrLEuR"},"source":["### keyterms in tensorflow\n","TensorFlow has a different coding paradigm. We can understand TensorFlow as a sophisticated\n","version of NumPy package. Storing the data and handling the calculations is very different in\n","TensorFlow. We will see some crucial terms in TensorFlow."]},{"cell_type":"markdown","metadata":{"id":"ctGF2LbxLTLs"},"source":["#### Tensors\n","Intuitively we can understand a tensor as a multidimensional array. In TensorFlow, data is represented as tensors. An array or vector is a collection of scalar values. Matrix is a two-dimensional array. Tensor is a multidimensional array. There are two types of tensors. Constants and Variables"]},{"cell_type":"markdown","metadata":{"id":"-xPUrbDeMYTF"},"source":["##### Constant tensors\n","Tensors contain constant values. We define these using tf.constant() function. We can mention their\n","type and initialize them with value."]},{"cell_type":"code","metadata":{"id":"ps6VXC0iDpLT"},"source":["import tensorflow as tf\n","%matplotlib inline  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IPJiVrRFDsxm"},"source":["a = tf.constant([50,10])\n","print(a)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6o9zK3kZDvr8"},"source":["print('a in tensorflow ==>', a)\n","print('numpy value of a ==>', a.numpy())\n","print('dtype of a ==>', a.dtype)\n","print('shape of a ==>', a.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sle7v6yJNKaW"},"source":["We can use inbuild tf.XX() function to create constant tensors, just like numpy."]},{"cell_type":"code","metadata":{"id":"AaWwevvmDz8W"},"source":["print('Tensor of Ones: \\n',tf.ones(shape=(2, 2)))\n","print('Tensor of Zeros: \\n',tf.zeros(shape=(2, 2)))\n","print('Random normal values \\n', tf.random.normal(shape=(3, 2),\n","                                                  mean=5, \n","                                                  stddev=1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y6ytiJlWOa6r"},"source":["##### Variables\n","The data types that can allow us to change the previously assigned values and work as trainable\n","parameters. We can define them using tf.Variable(), with the type and initial value. We generally\n","create a variable and initialize it with a value. Convert this constant tensor into a variable and then\n","mutate the variable by using functions. Below are a few examples of variables"]},{"cell_type":"markdown","metadata":{"id":"56kIR9kiOwUG"},"source":["**Simple variables**"]},{"cell_type":"code","metadata":{"id":"JnogoJVhD4Gt"},"source":["x = tf.Variable(5) \n","print(x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hXqwHBzPO4NS"},"source":["**randomly initialized variable, like we need for our weights in NN**"]},{"cell_type":"code","metadata":{"id":"FzIr89OFD-Vg"},"source":["w = tf.Variable(tf.random.normal(shape=(2, 2))) \n","print(w)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UAMyR6FcEAqx"},"source":["m = tf.Variable(5) \n","print(m)\n","\n","m = tf.Variable(5) \n","print('New value', m.assign(2))\n","\n","m = tf.Variable(5) \n","print('increment by 1', m.assign_add(1))\n","\n","m = tf.Variable(5) \n","print('Decrement by 2', m.assign_sub(2))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9rKe0z1LPdrv"},"source":["### Model Building in TensorFlow\n","To work in TensorFlow, we need to follow some basic steps. First, we define the model equation.\n","Initialize the weights and define the cost function. Finally run the parameter training iterations using\n","an optimization algorithm."]},{"cell_type":"markdown","metadata":{"id":"IjJ3_9MiQQEN"},"source":["#### Regression Model building in TensorFlow\n","Below is the code for building a regression model in TensorFlow. We will create some sample data\n","and use it for solving it using TensorFlow"]},{"cell_type":"code","metadata":{"id":"B-NEhVrIEGRb"},"source":["#This step is for data creation, x, and y\n","import numpy as np\n","x_train= np.array(range(5000,5100)).reshape(-1,1)\n","y_train=[3*i+np.random.normal(500, 10) for i in x_train]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2qpxUOivTc4S"},"source":["We multiplied x_train by three and added some randomness to create the data. After building the\n","regression model, we expect the final weight of x to be “3”."]},{"cell_type":"code","metadata":{"id":"D8VLkiaqTYHb"},"source":["import matplotlib.pyplot as plt\n","plt.title(\"x_train vs y_train data\")\n","plt.plot(x_train, y_train, 'b.')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U0-d6k_zT6iY"},"source":["Below code is used for building the model"]},{"cell_type":"code","metadata":{"id":"OPC89ovgEJ5R"},"source":["#Model y=X*W + b\n","#Model function\n","def output(x):\n","    return W*x + b\n","\n","#Loss function Reduce mean square\n","def loss_function(y_pred, y_true):\n","    return tf.reduce_mean(tf.square(y_pred - y_true))\n","\n","#Initialize Weights\n","W = tf.Variable(tf.random.uniform(shape=(1, 1)))\n","b = tf.Variable(tf.ones(shape=(1,)))\n","\n","#Optimization\n","learning_rate = 0.000000001\n","steps = 200 \n","\n","for i in range(steps):\n","    with tf.GradientTape() as tape:\n","        predictions = output(x_train)\n","        loss = loss_function(predictions,y_train)\n","        dloss_dw, dloss_db = tape.gradient(loss, [W, b])\n","    W.assign_sub(learning_rate * dloss_dw)\n","    b.assign_sub(learning_rate * dloss_db)\n","    print(f\"epoch : {i}, loss  {loss.numpy()},  W : {W.numpy()}, b  {b.numpy()}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sbcmtN7yUiDv"},"source":["The critical function that we need to understand in the above code is tf.GradientTape(). This function\n","calculates the gradients. We take these weights to multiply them with the learning rate and update the weights in each epoch. Even if we do not understand the syntax, it is fine. Later in Keas section,\n","we will discuss on why it is fine not not to master TensorFlow syntax."]},{"cell_type":"code","metadata":{"id":"nT9HTenjEYRz"},"source":["print('w ', W)\n","print('b ', b)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"McMLxYbBUvBW"},"source":["As expected, the weight is 3. We did not have any bias in the data. The expected bias is 0, and it has been estimated as one by TensorFlow. The overall accuracy of the model is good. We can print and visualize how the overall model has converged."]},{"cell_type":"code","metadata":{"id":"rtMwtyv6EbQj"},"source":["W = tf.Variable(tf.random.uniform(shape=(1, 1)))\n","b = tf.Variable(tf.ones(shape=(1,)))\n","\n","learning_rate = 0.000000001\n","steps = 200 \n","\n","for i in range(steps):\n","    with tf.GradientTape() as tape:\n","        predictions = output(x_train)\n","        loss = loss_function(predictions,y_train)\n","        dloss_dw, dloss_db = tape.gradient(loss, [W, b])\n","    W.assign_sub(learning_rate * dloss_dw)\n","    b.assign_sub(learning_rate * dloss_db)\n","    if i%30 == 0:\n","        print(f\"epoch is: {i}, loss is {loss.numpy()},  W is: {W.numpy()}, b is {b.numpy()}\")\n","        plt.title([\"epoch\", i])\n","        plt.plot(x_train, y_train, 'b.')\n","        plt.plot(x_train, output(x_train), c='r')\n","        plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RcfjJApEVJw9"},"source":["#### Logistic Regression Model building in TensorFlow\n","Below code is used for building a logistic regression model in TensorFlow"]},{"cell_type":"code","metadata":{"id":"ps5EkVdBEg9l"},"source":["x_train= np.random.rand(100,1)\n","y_train=np.array([0 if i < 0.5 else 1 for i in x_train]).reshape(-1,1)\n","\n","import matplotlib.pyplot as plt\n","plt.title(\"x_train vs y_train data\")\n","plt.plot(x_train, y_train, 'b.',)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ToSGASU4d7fy"},"source":["Below is the code for building the model. The model equation changes, rest all code remains the\n","same."]},{"cell_type":"code","metadata":{"id":"Sppm3QFBEkGq"},"source":["# same as the linear regression just sigmoid wrapped around the linear equation\n","def output(x): \n","    return tf.sigmoid(W*x + b)\n","\n","#Loss function : sum of squares\n","def loss_function(y_pred, y_true):\n","    return tf.reduce_sum(tf.square(y_pred - y_true))\n","\n","W = tf.Variable(tf.random.uniform(shape=(1, 1)))\n","b = tf.Variable(tf.zeros(shape=(1,)))\n","\n","learning_rate = 0.1\n","steps = 300 \n","\n","for i in range(steps):\n","    with tf.GradientTape() as tape:\n","        predictions = output(x_train)\n","        loss = loss_function(y_train, predictions)\n","        dloss_dw, dloss_db = tape.gradient(loss, [W, b])\n","    W.assign_sub(learning_rate * dloss_dw)\n","    b.assign_sub(learning_rate * dloss_db)\n","    print(f\"epoch : {i}, loss  {loss.numpy()},  W : {W.numpy()}, b  {b.numpy()}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uTQ_RU0ghjX0"},"source":["We can print and visualize how the overall model has converged."]},{"cell_type":"code","metadata":{"id":"KIU0BCZiEqpk"},"source":["def output(x): \n","    return tf.sigmoid(W*x + b)\n","\n","def loss_function(y_pred, y_true):\n","    return tf.reduce_sum(tf.square(y_pred - y_true))\n","\n","W = tf.Variable(tf.random.uniform(shape=(1, 1)))\n","b = tf.Variable(tf.zeros(shape=(1,)))\n","\n","learning_rate = 0.1\n","steps = 300 \n","\n","for i in range(steps):\n","    with tf.GradientTape() as tape:\n","        predictions = output(x_train)\n","        loss = loss_function(y_train, predictions)\n","        dloss_dw, dloss_db = tape.gradient(loss, [W, b])\n","    W.assign_sub(learning_rate * dloss_dw)\n","    b.assign_sub(learning_rate * dloss_db)\n","\n","    if i%40 == 0:\n","        print(f\"epoch is: {i}, loss is {loss.numpy()},  W is: {W.numpy()}, b is {b.numpy()}\")\n","        plt.title([\"epoch\", i])\n","        plt.plot(x_train, y_train, 'b+')\n","        plt.plot(x_train, output(x_train), '.', c='r')\n","        plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ebw5kAb-hp6n"},"source":["## Keras\n","Keras is a high-level API on top of TensorFlow. Keras has several functions and features that will automatically write the TensorFlow code in the background. We need not write the low-level coding in TensorFlow, Keras has a simple syntax and fewer lines of code. Analogously, using TensorFlow is like using Numpy, whereas using Keras is using the Sci-kit-Learn package. Sci-kit-Learn uses NumPy\n","internally; Keras uses TensorFlow internally. Most of the data scientists use Keras to build deep learning models. Keras has fewer lines of code and earning Keras syntax is easy. Most importantly, Keras gives a lot of useful options while building the model. We need not install Keras separately. TensorFlow2.0 onwards, Keras automatically gets installed along with TensorFlow. We can use below command to import Keras."]},{"cell_type":"code","metadata":{"id":"QY974i1gE0kG"},"source":["from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jxz23UQ-izhl"},"source":["Keras coding also tries to mimic the network structure of Neural Networks. In Keras, we configure and build the models using a sequence of layers. The sequential model is a linear stack of layers. The first layer in the stack is “Input Layer”. We mention the information on the input shape. The last layer is “Output Layer”. The model gets information on labels from the last layer. We can add all the “model layers” in between. Based on the input, hidden and output layers, the model will automatically prepare the weight parameters."]},{"cell_type":"markdown","metadata":{"id":"djXTUMhni6tW"},"source":["### MNIST on keras\n","Dataset MNIST (&quot;Modified National Institute of Standards and Technology&quot;) is the most widely used\n","dataset of computer vision. The goal is to predict the number in the image by taking image pixel\n","values as input. This dataset is available as a sample dataset inside Kear&#39;s library. We will write the\n","code to understand the data and then move to model building"]},{"cell_type":"code","metadata":{"id":"uh2cJzKlE-Oy"},"source":["# The data, shuffled and split between train and test sets\n","(X_train, Y_train), (X_test, Y_test) = keras.datasets.mnist.load_data()\n","num_classes=10\n","x_train = X_train.reshape(60000, 784)\n","x_test = X_test.reshape(10000, 784)\n","x_train = x_train.astype('float32')\n","x_test = x_test.astype('float32')\n","x_train /= 255\n","x_test /= 255\n","\n","# convert class vectors to binary class matrices\n","y_train = keras.utils.to_categorical(Y_train, num_classes)\n","y_test = keras.utils.to_categorical(Y_test, num_classes)\n","\n","print(x_train.shape, 'train input samples')\n","print(x_test.shape, 'test input samples')\n","\n","print(y_train.shape, 'train output samples')\n","print(y_test.shape, 'test output samples')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9kTAsaqojXjW"},"source":["The above code is for importing the data. This data is part of Keras sample demo datasets."]},{"cell_type":"markdown","metadata":{"id":"t6xxmk8RjoVv"},"source":["We have 60,000 images in the training data and 10,000 images in test data. There are 784 pixels in\n","each image. Let us draw a few images before starting with model building. Below code is used for\n","drawing the images"]},{"cell_type":"code","metadata":{"id":"Yaq7sB5-FERO"},"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","plt.subplot(221)\n","plt.imshow(X_train[1], cmap=plt.get_cmap('gray'))\n","plt.subplot(222)\n","plt.imshow(X_train[6], cmap=plt.get_cmap('gray'))\n","plt.subplot(223)\n","plt.imshow(X_train[7], cmap=plt.get_cmap('gray'))\n","plt.subplot(224)\n","plt.imshow(X_train[9], cmap=plt.get_cmap('gray'))\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HuPSx6arjwTi"},"source":["Now we will go ahead with model building. We need to configure the model While configuring the model, we need to mention the layers sequentially. Below is the code for building the model with two hidden layers of 20 nodes each. Our neural network is [784-input nodes; 20 nodes in H1; 20 nodes in H2; 10 nodes in output layer]. We expect 15,700(785X20) weight parameters in the first layer, 420 (21X20) weights in the second layer and 210(21X10) weights in the final layer. model.summary() gives us a summary of weight parameters in the network.\n"]},{"cell_type":"code","metadata":{"id":"K3-yHLXDFJwe"},"source":["model = keras.Sequential()\n","\n","model.add(layers.Dense(20, activation='sigmoid', input_shape=(784,)))\n","\n","model.add(layers.Dense(20, activation='sigmoid'))\n","\n","model.add(layers.Dense(10, activation='softmax'))\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oBeRyAsikBBv"},"source":["In the above code, we configure the input layer in the first step. The model needs to know what input shape it should expect. For this reason, the first layer in a Sequential model needs to receive information about its input shape. Only the first layer needs the shape information because the following layers can make automatic shape inference. The dense layer is simply a layer where each hidden node in this layer is connected to every hidden node in the next layer. In the final layer, mention the number of output classes as the number of nodes."]},{"cell_type":"markdown","metadata":{"id":"WdouIEX8kLCL"},"source":["Now we are ready to build the model. The below code is used for compiling the model and training\n","the model on input data.\n","In compile function, we need to mention the loss function and validation metrics. We discussed “mean squared error” as a loss function. There are some more options in the loss function. As of now, we can look at\n","&#39;categorical_crossentropy&#39; as a different formula for loss."]},{"cell_type":"code","metadata":{"id":"9yZf0FRCFpwI"},"source":["model.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","model.fit(x_train, y_train,epochs=10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QOJLi4hUkxc0"},"source":["The model is ready. We can print the weights and we can use the model to get the accuracy on test\n","data using the below code."]},{"cell_type":"code","metadata":{"id":"s7o2eKyrFtIr"},"source":["print(model.get_weights())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"De3cDoSAGKzf"},"source":["loss, acc = model.evaluate(x_test,  y_test, verbose=2)\n","print(\"Test Accuracy: {:5.2f}%\".format(100*acc))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z9AI2A3uk8WR"},"source":["The model has achieved nearly 94% accuracy. The same model with 16,330 weights takes much time\n","if we use NumPy based standard python packages. Keras and TensorFlow complete the task much\n","faster."]}]}