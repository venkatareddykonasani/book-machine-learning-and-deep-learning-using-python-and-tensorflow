{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Chapter-4.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"e_gznAUJeM77"},"source":["# Chapter 4: Decision trees"]},{"cell_type":"markdown","metadata":{"id":"57YfYxCD-Jgp"},"source":["## Decision tree model building\n","We have this survey data with us. We want to build a decision tree model that will predict whether a customer will be satisfied or dissatisfied. We want to predict it as soon as he makes a call before he takes a survey. Based on the customer attributes, if we predict that the customer is going to be dissatisfied, then we will route the call to top agents with a high score. If a customer has a high chance of being satisfied, then we can route him to agents with low scores or inexperienced agents. This strategy will help us in resource planning and increasing the resolution rate. "]},{"cell_type":"code","metadata":{"id":"j5QYZNN0dzuw"},"source":["import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lTuz_OT8eibk"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f_FQBaLaCf9I"},"source":["### **Data importing and basic data exploration** "]},{"cell_type":"code","metadata":{"id":"jrLy9fz-eqXy"},"source":["#survey_data = pd.read_csv('/content/drive/My Drive/DataSets/Chapter-4/datasets/Call_center_survey.csv')\n","survey_data = pd.read_csv('https://raw.githubusercontent.com/venkatareddykonasani/ML_DL_py_TF/master/Chapter4_Decison_Trees/Datasets/Call_center_survey.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8_O0kohbfLWz"},"source":["print(survey_data.shape)\n","print(survey_data.columns)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vu8ANtREfPDv"},"source":["pd.set_option('display.max_columns', None)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DbKy8tNrMq35"},"source":["When we use pd.head()\n","We see some middle colums hidden by default.\n","\n","Max_colums let us manually configure how many colums to be displayed with df.head or df.tails.\n","\n","So the above code displayes maximum columns specified by you. "]},{"cell_type":"code","metadata":{"id":"Mx_lsfhffTCe"},"source":["survey_data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OxXaDmcYfh_a"},"source":["summary=survey_data.describe()\n","round(summary,2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nbtsa7w5DWK9"},"source":["The above code gives us the summary of each column.\n","\n","From the above output, few observations are listed below:\n","\n","* Age  variable has an average value of 44.\n","* Account balance has an average of 41,177. The minimum is 4,904, and the maximum is 109,776. \n","* Personal loan indicator, Home Loan indicator and Prime\n","Customer Indicator are categorical variables. They take only two values, 0 and 1. A better measure to summaries these variables will be a frequency table using the function value_counts() \n","* Overall_Satisfaction is not shown in the above output. It also takes two values “Satisfied” and “Dis-Satisfied.” \n","We will look at the frequency counts table for these indicator and categorical variables.\n"]},{"cell_type":"code","metadata":{"id":"w--TftGwhCJJ"},"source":["print(survey_data['Overall_Satisfaction'].value_counts())\n","print(survey_data[\"Personal_loan_ind\"].value_counts())\n","print(survey_data[\"Home_loan_ind\"].value_counts())\n","print(survey_data[\"Prime_Customer_ind\"].value_counts())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GBMd9q2GILfP"},"source":["from the above output we san clearly note below things:\n","\n","* Overall, 6,707 customers are dissatisfied and rest are satisfied. More customers are dissatisfied than satisfied customers\n","* Almost 50% of the customers have personal loans \n","* Almost 50% of the customers have an existing home loan\n","* Nearly 58% of customers are prime category customers.\n","\n","Now we will build the model by using  'Age',  'Account_balance', 'Personal_loan_ind',     'Home_loan_ind', 'Prime_Customer_ind’ as predictor variables and considering ‘Overall_Satisfaction’ as the target variable. "]},{"cell_type":"markdown","metadata":{"id":"igVOadL2Jwx6"},"source":["### Model building\n","Before building a model we need to convert the data in specific format i.e we need to convert the non-numeric variables to numeric variables for building the decision tree model. If the non-numeric columns are populated with only two values, then we can easily map them to 0 and 1. \n","\n","For example, a variable like gender can be easily converted to numeric by mapping Male and Female to 0 and 1.\n","\n","If a categorical variable has several values populated in it, then we need to convert it into multiple dummy variables.\n","\n","For example, a variable like Region takes four values East, West, North and South. We can not map these values to 1,2,3 and 4. We need to create four new columns. All four columns will be binary. East_ind, West_ind, North_ind and South_ind.  \n"]},{"cell_type":"markdown","metadata":{"id":"-h4eUNqELcaU"},"source":["In this example all the predictor variables are numeric but the target variable is categorical which we need to convert to numeric. Below code is mapping non-numeric values to numeric values."]},{"cell_type":"code","metadata":{"id":"hx_2ojaFhEag"},"source":["survey_data['Overall_Satisfaction'] = survey_data['Overall_Satisfaction'].map( {'Dis Satisfied': 0, 'Satisfied': 1} ).astype(int)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bOp4_azFMXNx"},"source":["Now we will check the value_counts since we converted the non-numeric values to numeric values."]},{"cell_type":"code","metadata":{"id":"aVluLXNIhGkt"},"source":["survey_data['Overall_Satisfaction'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aFCR6TKcNos4"},"source":["We will store the predictor variables list in a list called features.  "]},{"cell_type":"code","metadata":{"id":"KJCZoX3PhIgl"},"source":["features=list(survey_data.columns[1:6])\n","print(features)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"06rBOQ3iODrs"},"source":["We can prepare the final features and target matrix using this below code. \n"]},{"cell_type":"code","metadata":{"id":"BUUX7fo6N3iv"},"source":["X=survey_data[features]\n","y = survey_data['Overall_Satisfaction']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V0XqSZfGON1M"},"source":["We are going to use these to matrices in building the model. Below is the code for configuring the model. "]},{"cell_type":"code","metadata":{"id":"0tYfpBp8hPoW"},"source":["from sklearn import tree\n","DT_Model = tree.DecisionTreeClassifier(max_depth=2)\n","DT_Model.fit(X,y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CYogxCiyOe5i"},"source":["We will try to understand the above code.\n","\n","\n","DT_model – This is the model name. It can be any name\n","\n","DecisionTreeClassifier() - The function to build the decision trees. This function will execute the decision tree algorithm. \n","\n","max_depth – This is a pruning parameter. This parameter is important.\n","\n","**\"DT_model.fit(X,y)\"**\n","\n","The above step is the model configuration. In this step, we supply the actual data of X and y. Once we call the model.fit() function, the algorithm will start the information gain calculation and other steps of building the decision tree model. \n"]},{"cell_type":"markdown","metadata":{"id":"NNjCF9ppPTu8"},"source":["This output which we got from above code is not the model output that we are expecting. It is just the function and all the parameters. We need to draw the tree to understand the model stored in DT_Model. "]},{"cell_type":"markdown","metadata":{"id":"3fe-OdBHqCdS"},"source":["### Drawing the Decision tree\n","All the measures are calculated and stored in DT_Model. We will access all the values and draw the decision tree. Below is the code for drawing the decision tree.We need two packages to draw this decision tree, “Graphviz” and “pydotplus.” We need to supply the model name. This code will extract all the values from the model and returns the tree image with all the details."]},{"cell_type":"markdown","metadata":{"id":"T44yp7E3xHyp"},"source":["#### Using GraphViz package"]},{"cell_type":"code","metadata":{"id":"ZgpxUCUcxkFq"},"source":["from IPython.display import Image\n","from sklearn.externals.six import StringIO\n","import pydotplus\n","dot_data = StringIO()\n","tree.export_graphviz(DT_Model, \n","                     out_file = dot_data,\n","                     filled=True, \n","                     rounded=True,\n","                     impurity=False,\n","                     feature_names = features)\n","graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n","Image(graph.create_png())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bPG_cma0xVEl"},"source":["#### Using plot_tree function"]},{"cell_type":"code","metadata":{"id":"jhDiTevUxsUK"},"source":["import matplotlib.pyplot as plt\n","from sklearn.tree import plot_tree, export_text\n","plt.figure(figsize=(15,7))\n","plot_tree(DT_Model, filled=True, \n","                     rounded=True,\n","                     impurity=False,\n","                     feature_names = features)\n","print(export_text(DT_Model, feature_names = features))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a1ZERpsBwNXr"},"source":["### Tree validation and accuracy\n","After building the decision tree model, we will get the decision tree rules. Before going ahead with the predictions, we need to take a note of the accuracy of the model. The actual values of the target variable are 0’s and 1’s. We can get the predicted values and create a confusion matrix to derive accuracy. "]},{"cell_type":"code","metadata":{"id":"krBl0QBux3_G"},"source":["predict1 = DT_Model.predict(X)\n","print(predict1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9tPnYS65x--n"},"source":["from sklearn.metrics import confusion_matrix \n","cm = confusion_matrix(y, predict1)\n","print(cm)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K5LuIUxqyDKs"},"source":["total = sum(sum(cm))\n","accuracy = (cm[0,0]+cm[1,1])/total\n","print(accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AZx0sw9M1k10"},"source":["sum(cm) gives us column wise sum(sum(cm)) is the actual total of the confusion matrix.\n","\n","From the output, we can see that the accuracy of our decision tree model is 92.6%\n"]},{"cell_type":"markdown","metadata":{"id":"2eDW6jqj2BGF"},"source":["## problem of overfitting\n","* If a model works really well on the training data and fails on the test data, then we call that model as an overfitted model. \n","* **Train Data**: The data set that is used for building the model is known as train data. The model tries to learn the patterns in this train data. This dataset will be completely exposed to the model. Sometimes model might memorize this dataset instead of learning the generic patterns from it.  A fully grown decision tree will return the same data points as rules, that is an example of memorizing the training data. \n","* **Test Data**: Test data is sampled from the same population but it has been kept aside while building the model. We know the actual target values in the test data. We will use this test data for validating the model. A model has very high accuracy may not always ensure that it will have high accuracy on test data. We will build the model on train data and apply it to test data. Get the accuracy of test data, as well. The model is considered to be good if it shows high accuracy on train data and almost matching accuracy on test data. \n"]},{"cell_type":"code","metadata":{"id":"aAE9wiUwylFp"},"source":["import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dkHAsS1Pyupu"},"source":["#train = pd.read_csv(\"/content/drive/My Drive/DataSets/Chapter-4/datasets/Buyers Profiles/Train_data.csv\")\n","#test = pd.read_csv(\"/content/drive/My Drive/DataSets/Chapter-4/datasets/Buyers Profiles/Test_data.csv\")\n","\n","train = pd.read_csv(\"https://raw.githubusercontent.com/venkatareddykonasani/ML_DL_py_TF/master/Chapter4_Decison_Trees/Datasets/Buyers%20Profiles/Train_data.csv\")\n","test = pd.read_csv(\"https://raw.githubusercontent.com/venkatareddykonasani/ML_DL_py_TF/master/Chapter4_Decison_Trees/Datasets/Buyers%20Profiles/Test_data.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p67VuNUTk8uy"},"source":["print(train.shape)\n","print(test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eudjWLgSlDLe"},"source":["train['Gender'] = train['Gender'].map( {'Male': 1, 'Female': 0} ).astype(int)\n","train['Bought'] = train['Bought'].map({'Yes':1, 'No':0}).astype(int)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mf9lQtWTlLkl"},"source":["test['Gender'] = test['Gender'].map( {'Male': 1, 'Female': 0} ).astype(int)\n","test['Bought'] = test['Bought'].map({'Yes':1, 'No':0}).astype(int)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VzuI6VQAzT9G"},"source":["**What is Overfitting?**\n","* A model has high accuracy on train data and significantly low accuracy on test data. \n","* A model that is learning specific patterns related to training data, instead of learning the generic patterns, the model is memorizing the training data. \n","* For small changes in the training data, the model and its parameters change a lot. For example, if the decision tree is overfitted, then small changes in the training data will cause a huge change in the final rules. Since these overfitted models have a huge variance in their parameters, they are also known as models with a lot of variance. \n","* An overcomplicated model with too many parameters. A model that needs to be simplified. If it is a decision tree, then a really large tree with too many rules, these types of trees need to be pruned. \n","* Overfitting is a generic concept. It can happen to any model, regression model or logistic regression model. Any model that shows high accuracy on train data and low accuracy on test data is called an overfitted model. \n"]},{"cell_type":"code","metadata":{"id":"3llQdXtqlPSg"},"source":["from sklearn import tree"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UES3RlzYlS4O"},"source":["features = list(train.columns[:2])\n","X_train = train[features]\n","y_train = train['Bought']\n","X_test = test[features]\n","y_test = test['Bought']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2s6CAI_WlXT0"},"source":["clf = tree.DecisionTreeClassifier()\n","clf.fit(X_train,y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AHRnzFN-lepy"},"source":["from IPython.display import Image\n","from sklearn.externals.six import StringIO\n","import pydotplus\n","dot_data = StringIO()\n","tree.export_graphviz(clf,\n","                     out_file = dot_data,\n","                     feature_names = features,\n","                     filled=True, rounded=True,\n","                     impurity=False)\n","\n","graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n","Image(graph.create_png())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qvoauZBSzyg9"},"source":["***How to detect the overfitting?***\n","\n","Find the accuracy of train data and test data. If a model has significantly lower accuracy on test data, then the model is overfitted. Any difference of more than 5% is a significant difference. If we have 90% accuracy on train data. Then we expect the model accuracy on the test data to be more than 85%. "]},{"cell_type":"code","metadata":{"id":"CuMiU5FRlqA5"},"source":["predict1 = clf.predict(X_train)\n","print(predict1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KQelS01ml7XE"},"source":["predict2 = clf.predict(X_test)\n","print(predict2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7YgHTmqsl_Qp"},"source":["from sklearn.metrics import confusion_matrix ###for using confusion matrix###\n","cm1 = confusion_matrix(y_train,predict1)\n","cm1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3PEDWDpumbEb"},"source":["total1 = sum(sum(cm1))\n","accuracy1 = (cm1[0,0]+cm1[1,1])/total1\n","accuracy1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HLsglIDemgfU"},"source":["cm2 = confusion_matrix(y_test,predict2)\n","cm2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"46W-ytBCmmns"},"source":["total2 = sum(sum(cm2))\n","accuracy2 = (cm2[0,0]+cm2[1,1])/total2\n","accuracy2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VKWEIz-W0HZA"},"source":["From the above outputs we can clearly say that the above model is overfitted since the accuracy on training dataset is 100% and on test dataset is 16.66%"]},{"cell_type":"markdown","metadata":{"id":"vz4IkpA10msz"},"source":["### Choosing optimal value of Pruning parameter\n","While building a model, we have to make sure that it should be neither overfitted nor under fitted. \n","\n","First of all, you can build and finalize the decision tree model in one attempt. You need to build several models and choose the optimal one. No one can guess what the optimal depth of a decision tree for a given data is. We have to discover it.  \n","* First start by building a really large tree.  Depending on the training data, try to get the maximum possible depth. This model will be overfitted\n","* In second attempt, build a very small tree  Very small tree with just max depth=1. This will be most probably under fitted. You can look at the training accuracy and confirm it. Model_1 is overfittedand Model_2 is underfitted but we got the boundries. Now we can search the optimal value between these boundries.\n","* Now build a model by taking the value of parameter between these boundries. If the model is overfitted then reduce the value of parameter.\n","\n"]},{"cell_type":"code","metadata":{"id":"_nsXo3XQmtbX"},"source":["dtree = tree.DecisionTreeClassifier(max_leaf_nodes = 10, \n","                                    min_samples_leaf = 5, \n","                                    max_depth= 5)\n","dtree.fit(X_train,y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FCiKuwIRnbP9"},"source":["predict3 = dtree.predict(X_train)\n","predict4 = dtree.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PlimQmi5ng18"},"source":["from sklearn.metrics import confusion_matrix ###for using confusion matrix###\n","cm1 = confusion_matrix(y_train,predict3)\n","cm1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Vha7yf1nkpK"},"source":["total1 = sum(sum(cm1))\n","accuracy1 = (cm1[0,0]+cm1[1,1])/total1\n","accuracy1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m5FOj8ponogC"},"source":["cm2 = confusion_matrix(y_test,predict4)\n","cm2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4_jGas60oGng"},"source":["total2 = sum(sum(cm2))\n","accuracy2 = (cm2[0,0]+cm2[1,1])/total2\n","accuracy2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e6ur7dur3Ypm"},"source":["Now this model is not overfitted, So we got the optimal value of pruning parameter."]}]}