{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Chapter-12.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"XszQFPLGfjZt"},"source":["# Chapter 12 - RNN and LSTM"]},{"cell_type":"code","metadata":{"id":"XsXfCqzLbu1S"},"source":["#Importing dependencies\n","import numpy as np\n","import string\n","import random\n","import re\n","import tensorflow as tf\n","import pandas as pd\n","import tensorflow.keras as keras\n","import matplotlib.pyplot as plt\n","import sklearn\n","from tensorflow.keras.models import Sequential\n","from numpy import array, argmax, random, take\n","from tensorflow.keras.layers import Dense, Activation\n","from tensorflow.keras.layers import Dropout\n","from tensorflow.keras.layers import RNN, SimpleRNN, LSTM,  Embedding, RepeatVector\n","from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","%matplotlib inline\n","#For plotting the matplotlib graphs in notebook"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"inzl6cnBf91a"},"source":["Mount your drive and write the path of your dataset folder"]},{"cell_type":"code","metadata":{"id":"U7l4S3bAeL-H"},"source":["#Data_path=\"/content/drive/My Drive/DataSets/Chapter-12/Datasets/\"\n","Data_path=\"https://raw.githubusercontent.com/venkatareddykonasani/ML_DL_py_TF/master/Chapter12_RNN_LSTM_V3/Datasets/\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vt16cq-tgyR3"},"source":["## Case Study – Word prediction\n","This case study uses a simple dataset to understand the concept of sequential ANN models."]},{"cell_type":"markdown","metadata":{"id":"a16Z__MkiUrE"},"source":["### Objective and Data\n","We would like to predict the third word based on the sequence of two words. The dataset contains a list of three words. This type of data is also known as three-gram data. If we consider the most occurring N-word sequence, then it is known as N-gram data. This dataset is freely available on COCA ( Corpus of Contemporary American English) website https://www.english-corpora.org/coca/ . We are using a subset in this example. In this subset of the data, we mostly considered the word sequences starting with love or hate."]},{"cell_type":"markdown","metadata":{"id":"A0pdCUNyipF3"},"source":["The final goal is to take the first two words as input and predict the final word. Below is the code for\n","importing the data."]},{"cell_type":"code","metadata":{"id":"FQVIbjPfgO_4"},"source":["import pandas as pd\n","column_names = ['word1', 'word2', 'word3']\n","\n","input_3gram = pd.read_csv(Data_path+ \"3Gram_love_data.txt\", delimiter='\\t', names=column_names) #Importing csv file with column names\n","print(\"shape of data\", input_3gram.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MjV7tBMugTef"},"source":["print(\"Few sample records from data \\n\", input_3gram.sample(10))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fbXd0dq0ixnj"},"source":["There are a total of 5351 rows, and each row has three words, here three words constitute three\n","columns. The below code is used to find the frequency of distinct words in coulmn1 and column2."]},{"cell_type":"code","metadata":{"id":"xbstcqrtgq8S"},"source":["print(\"\\nFrequency of word1 values \\n\", input_3gram[\"word1\"].value_counts())\n","print(\"\\nFrequency of word2 values \\n\", input_3gram[\"word2\"].value_counts())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dRfnPyofi4eX"},"source":["From the above output table, we can see that column-1 has all the words related to love and hate.\n","More than 90% of the words are related to love. The second column has a few more unique words."]},{"cell_type":"markdown","metadata":{"id":"9MUAx47Mi7JG"},"source":["### Data processing\n","Before building the model, we need to perform three steps.\n","1. Find all the unique words from three columns\n","2. The model can not be built on string data. Create a words_indices dictionary. Map the words to numbers and crate a dictionary with words as keys and numbers as values. Later we will create one-hot encoded variables for all the unique numbers. This conversion is necessary for building the model.\n","3. We can not give the predictions as numbers. Create one more dictionary from numbers to words. We need to finally give the predicted output in the form of words only. We need to maintain a second dictionary that has numbers as keys and words as values.\n","\n","Let us see the code for each of these steps. First step is to fins all the unique words from three\n","columns"]},{"cell_type":"code","metadata":{"id":"GLSc3jZ-gsqf"},"source":["\"\"\"\n","Finding our words to create dictionary\n","Here we find unique values in each column and save each of those values .\n","Later which we will take the unique value for the entire appened columns\n","This will be our vocabulary list,which are the unique words in our data file\n","\"\"\"\n","unique_words = []\n","for i in list(input_3gram.columns.values):\n","    for j in pd.unique(input_3gram[i]):\n","        unique_words.append(j)\n","unique_words = np.unique(unique_words)\n","\n","\n","print('Count of unique words overall:', len(unique_words))\n","print('unique words list:', unique_words)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KLODDe8_jsbH"},"source":["From the above output, we can see that there are overall 139 unique words. These are unique words\n","from all three columns. In the previous output, we have already seen that column-1 has a few\n","unique words. Now we will create two dictionaries, words to indices, and indices to words."]},{"cell_type":"code","metadata":{"id":"0-JAmNM6gvLi"},"source":["\"\"\"\n","creating our word:indice pair dictionary and inverse\n","Here will be creating two dictonary values\n","word_indices : This contains each words mapped to an unique digit \n","indices_words : This contains each digits mapped to a word in the same sequence as word_indices \n","\"\"\"\n","word_indices = dict((w, i) for i, w in enumerate(unique_words))\n","indices_words = dict((i, w) for i, w in enumerate(unique_words))\n","\n","print(\"word_indices dictionary \\n\",word_indices)\n","print(\"word_indices.keys \\n\", word_indices.keys())\n","print(\"word_indices.values \\n\", word_indices.values())\n","print(\"\\n ########################################\\n\")\n","print(\"indices_words dictionary \\n\", indices_words)\n","print(\"indices_words keys \\n\",indices_words.keys())\n","print(\"indices_words values \\n\",indices_words.values())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XRb3vGBzjzm3"},"source":["From the above output, we can see the mapping for each of the words to a number. In the word_indices dictionary, words are keys, and numbers are values. The output also shows the keys and values separately; we are not displaying them here. Total of 139 numbers from 0 to 138. The\n","word &quot;love&quot; is mapped to 70, and the word &quot;way&quot; is mapped to 129. The second part of the output is\n","the numbers to words dictionary."]},{"cell_type":"markdown","metadata":{"id":"JgGAoWVtj9ME"},"source":["The above 2nd output is just a copy of the previous dictionary, where keys and values are swapped. In the indices_words dictionary, keys are numbers, and values are words. 70 is mapped to &quot;love,&quot; and 129 is mapped to &quot;way.&quot; These two dictionaries are used later. One is used before building the model, and the second dictionary is used at the time of prediction. We will now convert the values in column-1 to one-hot encoded values. There are 139 unique words. So the first word with one column will be one hot encoded with 139 columns. Below code, converts word1 into one-hot\n","encoded columns."]},{"cell_type":"code","metadata":{"id":"BbTpYxKKgxnB"},"source":["### Onehot encoding of word1\n","word1 = input_3gram['word1'].map(word_indices)\n","word1_onehot = keras.utils.to_categorical(np.array(word1), num_classes=len(word_indices))\n","print(\"word1_onehot shape is \",word1_onehot.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7-rQqKGlkQ9q"},"source":["As expected, the output has 139 columns. Each column corresponds to one unique word. Let us see a couple of examples."]},{"cell_type":"code","metadata":{"id":"n3vFb8mOgzhM"},"source":["#Lets take example of two different words\n","print(\"The word in row 0 is -->\"+input_3gram['word1'][0])\n","print(\"The one hot encoded version of the word in row 0 is \\n\",word1_onehot[0])\n","\n","print(\"\\nThe word in row 500 is --> \"+input_3gram['word1'][500])\n","print(\"The one hot encoded version of the word in row 500 is \\n\",word1_onehot[500])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kqukEvkqkY3a"},"source":["From the above output, we can see the word in first row is “hate.” The one-hot encoded value for\n","that row shows the value “1” in the $42^{nd}$ column. The word in column 500 is love, and it has value “1”\n","in the $71^{st}$ column. We will convert word2 and word3 also to a one-hot encoded format using the\n","below code."]},{"cell_type":"code","metadata":{"id":"eUTnQaDjg1sQ"},"source":["##one hot encoding for word2 and word3 \n","word2 = input_3gram['word2'].map(word_indices)\n","word2_onehot = keras.utils.to_categorical(np.array(word2), num_classes=len(word_indices))\n","print(\"word2_onehot shape is \",word2_onehot.shape)\n","\n","word3 = input_3gram['word3'].map(word_indices)\n","word3_onehot = keras.utils.to_categorical(np.array(word3), num_classes=len(word_indices))\n","print(\"word3_onehot shape is \",word3_onehot.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yFNeTpI6kdjB"},"source":["We are done with data pre-processing. Now we are ready to build the two models."]},{"cell_type":"markdown","metadata":{"id":"bTpxW9CdkebC"},"source":["### Model building\n","There will be two models. The first ANN model takes word1_onehot as input and word2_onehot as\n","output. We will extract the hidden layer output from this model and use it in the next model. Below\n","is the code for building the model."]},{"cell_type":"code","metadata":{"id":"7N5tI_m8g4C0"},"source":["ANN_model1 = Sequential()\n","ANN_model1.add(Dense(10, input_dim=word1_onehot.shape[1], activation='sigmoid'))\n","ANN_model1.add(Dense(word2_onehot.shape[1] ,activation='softmax'))\n","ANN_model1.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j5ppiBE2g6g3"},"source":["ANN_model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","# Train model\n","history = ANN_model1.fit(word1_onehot, word2_onehot, epochs=20, batch_size=64,  verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"No0kVIHEkvyR"},"source":["We are not interested in this model. We are interested in the intermediate output values for each\n","record. Since there are ten hidden nodes, the hidden nodes result in a matrix that will have 5351\n","rows, and 10 columns. Below code helps us in extracting that matrix"]},{"cell_type":"code","metadata":{"id":"xxWuP_b7g8ti"},"source":["#We will see what the 1st hidden layer output representation of the data  \n","# to predict the hidden layer activations, \n","# let's rewrite first layer of our model and give it the weights from fully trained previous model\n","model1_hidden = Sequential()\n","model1_hidden.add(Dense(10, input_dim=word1_onehot.shape[1], weights=ANN_model1.layers[0].get_weights()))\n","model1_hidden.add(Activation('sigmoid'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wFLArsW3hCJp"},"source":["# Getting the hidden layer activations\n","model1_hidden_output = model1_hidden.predict(word1_onehot)\n","#peak into our hidden layer activations\n","print(\"The hidden layer output for every record - Shape of it \\n\", model1_hidden_output.shape)\n","print(\"Few five records from hidden layer \\n\",model1_hidden_output[:5])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GCKKG84Ok3Qi"},"source":["As expected, the shape of model1 hidden layer output is (5351,10). The output also shows the\n","hidden node outputs for the first five records; each record has ten values calculated from ten output\n","nodes. Now we will append this to the word2_onehot and build the second ANN model."]},{"cell_type":"code","metadata":{"id":"QvaWq936hEC_"},"source":["\"\"\"\n","We append the input words of the words2 column in the output of the h1 layer,this gives us the combined input representation\n","\"\"\"\n","word2_hidden_append = np.append(model1_hidden_output,word2_onehot, axis=1)\n","print(\"word2_hidden_append Shape\", word2_hidden_append.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AmL6vQnRhF_i"},"source":["ANN_model2 = Sequential()\n","ANN_model2.add(Dense(10, input_dim=word2_hidden_append.shape[1], activation='sigmoid'))\n","ANN_model2.add(Dense(word3_onehot.shape[1], activation='softmax'))\n","ANN_model2.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2orZdActhH4r"},"source":["ANN_model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","# Train model\n","history = ANN_model2.fit(word2_hidden_append, word3_onehot, epochs=20, batch_size=64,  verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3CPYDh7Rk_Qe"},"source":["We are now done with the sequential ANN models. The predictions will not be the same as standard\n","ANN models; in the next section, we will see how to use these models to get predicted values on\n","new data points."]},{"cell_type":"markdown","metadata":{"id":"quwMwdK0lCpy"},"source":["### Prediction\n","We need to write a custom predict function. This function takes a sequence of two words. These two words will be converted to numbers using the word_indices dictionary, followed by one-hot encoding. The function uses the first word and using ANN_model1, and we extract the hidden layer output values. These values will need then we appended to the second word, and final prediction will be made using the second model. The prediction will be a number; we then convert it to word using indices_words dictionary. Below is the code for writing the custom predict function."]},{"cell_type":"code","metadata":{"id":"HQtqG37zhJlQ"},"source":["# A predict function that takes input word1 and word2; and predict word3 \n","#1. take the input word , and represent them using digits from the word_indices dictonary values\n","#2. getting the intermediate hidden nodes for word1\n","#3. appending hidden activations with word2 as final test set\n","#4. prediction on this test set\n","def two_step_pred(words_in):\n","\n","    index_input=word_indices[words_in[0]]\n","    indices_in = keras.utils.to_categorical(index_input, num_classes=len(word_indices))\n","    indices_in=indices_in.reshape(1,len(word_indices))\n","    h1_test = model1_hidden.predict(indices_in) # getting our intermediate hidden activations from model1h\n","    \n","    \n","    index_input2=word_indices[words_in[1]]\n","    indices_in2 = keras.utils.to_categorical(index_input2, num_classes=len(word_indices))\n","    indices_in2= indices_in2.reshape(1,len(word_indices))\n","    X2_test = np.append(h1_test, indices_in2, axis=1) #preparing final test data by appending hidden with word2\n","    \n","    yhat = ANN_model2.predict_classes(X2_test) #predicting final output from model2\n","    \n","    print(\"Input words --> \", words_in)\n","    print(\"Predicted word --> \", indices_words[yhat[0]])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r4JWv0M0lRBs"},"source":["We will now use this function to get some predicted values."]},{"cell_type":"code","metadata":{"id":"qDmqIkBqhMoJ"},"source":["two_step_pred(['love', 'it'])\n","two_step_pred(['love', 'to'])\n","two_step_pred(['love', 'the'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MOfCPb6ZlXqB"},"source":["The accuracy of the model depends on the training data. The predictions are reasonable here. That concludes our case study on predicting the third word using the first two words as input. We used sequential ANN models to solve this problem of sequential dependency. Understanding of this case\n","study is vital for understanding RNN models"]},{"cell_type":"markdown","metadata":{"id":"xU7K0Q5mXcGO"},"source":["## Recurrent Neural Networks\n","To solve the sequential data problem, we manually built sequential ANN models. Recurrent Neural Networks are programmed sequential ANN models. The procedure we followed till now will be done by RNN automatically. We need to mention the number of time steps, then RNN models will automatically stack the required number of ANN models. If we have to predict the N th value in the sequence, then we need to build an RNN model with N time steps."]},{"cell_type":"markdown","metadata":{"id":"TmEcT_IrbYPt"},"source":["### Model building\n","While building RNN models, we need to mention the number of time steps along with the standard parameters like the number of hidden nodes and input shape. We need to add the SimpleRNN layer and mention these parameters."]},{"cell_type":"code","metadata":{"id":"QL5thAqzhOVv"},"source":["model = Sequential()\n","model.add(SimpleRNN(4, use_bias=False, input_shape=(2,2)))\n","model.add(Dense(3, use_bias=False, activation='softmax'))\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nhcztswCcOk-"},"source":["Since we have excluded the bias, there are a total of 36 parameters. Now we will include the bias in\n","this network and observe the number of parameters."]},{"cell_type":"code","metadata":{"id":"TrOaCbpFhR2e"},"source":["model = Sequential()\n","model.add(SimpleRNN(4, input_shape=(2,2)))\n","model.add(Dense(3, activation='softmax'))\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Mjl-dDkcVq1"},"source":["From the above outputs, we can see the number of parameters is matching to our manual\n","calculation using the formula. Since there is a concept of shared weights, the number of time steps\n","and the length of the sequence has no impact on the number of parameters. If we change the time\n","steps to 4, RNN will still result in 43 parameters."]},{"cell_type":"code","metadata":{"id":"IH7lZMnvhTq_"},"source":["model = Sequential()\n","model.add(SimpleRNN(4, input_shape=(4,2)))\n","model.add(Dense(3, activation='softmax'))\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f5A0So6fdoLp"},"source":["### Word prediction using RNN model\n","We have manually built a sequential ANN stack too solve the case study where we are predicting the third word. With RNN models, we just need to mention the number of time steps; the ANN stacks will be automatically taken care of by the RNN model. We need to supply the word1 and word2 inputs and build an RNN model with time steps=2. Below is the code for data preparation."]},{"cell_type":"code","metadata":{"id":"ZRZClTrJhVzs"},"source":["word1_word2 = input_3gram[['word1','word2']]\n","for i in list(word1_word2.columns.values):\n","    word1_word2[i] = word1_word2[i].map(word_indices)\n","\n","word1_word2=np.array(word1_word2)\n","#The same data is reshaped with similar structure but appended with 1 value to make it 3d array\n","word1_word2=np.reshape(word1_word2,(word1_word2.shape[0],2,1))\n","word1_word2_onehot = keras.utils.to_categorical(np.array(word1_word2), num_classes=len(word_indices))\n","print(\"word1_word2_onehot shape\", word1_word2_onehot.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R_6FZjdEjVtJ"},"source":["In the above code, we tried appending word1, word2 column-wise. Then reshaped them followed by one-hot encoding. Finally, the code will give us a three-dimensional array with 5351 rows, two columns, and each column has 139 dimensions(one-hot encoded).\n","\n","Now we are ready with data. We are predicting the third word, which means the time steps are two. The target variable is the third word, which is also one-hot encoded."]},{"cell_type":"code","metadata":{"id":"LYmy1CZphZwl"},"source":["print(\"time steps\" , word1_word2_onehot.shape[1])\n","print(\"Input nodes\" , word1_word2_onehot.shape[2])\n","print(\"output nodes\" , word3_onehot.shape[1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Q0AOS9ukaL0"},"source":["We will now build the RNN model using the below code."]},{"cell_type":"code","metadata":{"id":"6cy4K_Kahbs5"},"source":["model_rnn = Sequential()\n","#model.add(SimpleRNN('number of hidden nodes in each rnn cell', input_shape=(timesteps, input_data_dim)))\n","model_rnn.add(SimpleRNN(30, input_shape=(word1_word2_onehot.shape[1],word1_word2_onehot.shape[2]))) \n","model_rnn.add(Dense(word3_onehot.shape[1], activation='softmax'))\n","model_rnn.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gm4NQXg0kgdk"},"source":["In the above code, we have mentioned two-time steps and thirty hidden nodes at each time point.\n","\n","We will now compile and train the RNN model."]},{"cell_type":"code","metadata":{"id":"rLAQH2Xyhg8-"},"source":["# compile network\n","model_rnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","# fit network\n","model_rnn.fit(word1_word2_onehot, word3_onehot, epochs=20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pke7Jxu6lCKk"},"source":["The accuracy of the model depends on the strength of the training data. We have only five thousand\n","records and 139 dimensions in the data. We may not achieve a model with good accuracy. We need\n","more data for higher accuracy. But this model will be better than our previous manual sequential\n","ANN model.\n","\n","The model is now ready for predicting. Below is the prediction code."]},{"cell_type":"code","metadata":{"id":"zoxsuFqHhinL"},"source":["def rnn_word_pred(in_text):\n","    print(\"Input is - \" , in_text)\n","    encoded = [word_indices[i] for i in in_text]\n","    encoded = np.array(encoded).reshape(1,2,1)\n","    encoded =keras.utils.to_categorical(np.array(encoded), num_classes=len(word_indices))\n","    ypred = model_rnn.predict_classes(encoded, verbose=0)[0]\n","    print(\"Output is --> \" ,indices_words[ypred])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7PkFqgO4l4kI"},"source":["There are three significant steps in the above prediction function. Firstly converting words into indices, followed by reshaping them to 1row, two columns format, finally, one-hot encoding the input to bring into the shape (1,2,139). This pre-processed input will be sent to RNN to predict classes function. Finally, the numerical output will be converted as words before printing. This function takes a list of two words as input. Below are a few examples."]},{"cell_type":"code","metadata":{"id":"Spah_rDmhlBd"},"source":["rnn_word_pred(['love', 'it'])\n","rnn_word_pred(['love', 'to'])\n","rnn_word_pred(['love', 'the'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FLV1R1xAmF_6"},"source":["From the output, we can see the results are as good as our previous model. Once again, we need to\n","note that the predictions depend on our training data."]},{"cell_type":"markdown","metadata":{"id":"ag-zcDF6mmRq"},"source":["## RNN for long sequences\n","RNN models are useful for solving problems related to sequential data. In practical scenarios, RNN models seems to be failing to predict long sequences. Sequences where the number of time steps is more than ten, then the RNN algorithm is not giving accurate results.\n","\n","The RNN models, in theory, should work with a sequence of any length. But in practice, the standard RNN models don’t have long term memory property. We will see a simple numerical example to prove it. We will take an example of long term dependency and verify the performance of the RNN models."]},{"cell_type":"markdown","metadata":{"id":"jizWb9-ipPlA"},"source":["### Case study - Predicting the characters to form the word.\n","This case study is similar to the above case study of predicting the next word, but the approach is entirely different.\n"]},{"cell_type":"markdown","metadata":{"id":"4ZQkuKRBpzxF"},"source":["#### Data and Objective\n","In this example, we are considering three-gram data, but the data is formed by carefully choosing the three grams that are more than 15 characters long. The objective is to take the first 14 characters sequence as input and predict the next sequence of characters that form a word. The goal\n","is to predict the next word, but that word prediction id made by arranging the characters as a sequence. Character level input and output is the core difference between this model and model in the previous case study. Below code imports and prints a sample of the data."]},{"cell_type":"code","metadata":{"id":"Heue67BKjmMr"},"source":["import urllib.request  \n","urllib.request.urlretrieve(\"https://raw.githubusercontent.com/venkatareddykonasani/ML_DL_py_TF/master/Chapter12_RNN_LSTM_V3/Datasets/Long_sequence_3gram.csv\", \"Long_sequence_3gram.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r3BZfObGhmia"},"source":["longseq_3gram = open('Long_sequence_3gram.csv').read().lower()\n","print(longseq_3gram[495:801])\n","print(longseq_3gram[30615:31000])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IZ0FIErmql_P"},"source":["The above output shows a few examples from the data. We will now need to apply pre-processing\n","steps. We need to create a character to index the dictionary and preparation of X and y data."]},{"cell_type":"markdown","metadata":{"id":"qbpVecRkqqzH"},"source":["#### Data processing\n","There are several steps in data pre-processing. We will start by replacing the commas with space using the below code"]},{"cell_type":"code","metadata":{"id":"esN9KrhphqMr"},"source":["#Replace comma with space\n","longseq_3gram1= longseq_3gram.replace(',',' ').replace('\\r','')\n","print(longseq_3gram1[495:750])\n","print(longseq_3gram1[30615:30800])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bYQ5skFZq1NR"},"source":["In this model, we need to map each character to an index while preparing the character to index the dictionary."]},{"cell_type":"code","metadata":{"id":"4S6hG6iVhsa7"},"source":["#Unique characters in our dataset we then sort it\n","chars = sorted(list(set(longseq_3gram1)))\n","print(\"Unique Characters in the text \\n \",chars)\n","#\\n is character string for new line, we dont need that in our dictionary of chars\n","chars.remove('\\n')\n","print(\"\\n Character after removing newline symbol \\'\\\\n\\'\",chars)\n","print(\"\\n overall chars count\", len(chars))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a-Gm3kbSrAvS"},"source":["In the above code, we are trying to count all the unique characters. We will finally exclude new line symbol “\\n”.\n","\n","From the output, we can see that there are 37 unique characters. Apart from the alphabets, we have a few numbers and symbols. We will now create the char to indices and indices to char dictionaries using the below code."]},{"cell_type":"code","metadata":{"id":"m0grVlK_huDy"},"source":["char_indices = dict((c, i) for i, c in enumerate(chars))\n","print(\"characters to indices dictionary\\n\", char_indices)\n","indices_char = dict((i, c) for i, c in enumerate(chars))\n","print(\"indices to char dictionary\\n\", indices_char)\n","print('unique chars: ', {len(chars)})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OvO8wBL6rem-"},"source":["In the above code, we are creating two dictionaries.\n","\n","A quick verification will show that “a” is mapped to 11 in the char_indices dictionary, and 11 is mapped to “a” in indices to char dictionary. The next step is to apply this char_indicies dictionary on the full data and convert it from a sequence of characters into a sequence of numbers. We have\n","removed the newline symbol from the data; we need to add space at the end of every line to compensate for it."]},{"cell_type":"code","metadata":{"id":"E0_6YTRshv0H"},"source":["data = longseq_3gram1.splitlines()\n","##Adding a space at the end\n","data = [i+' ' for i in data]\n","\n","##mapping our data into numbers\n","sentences = [[char_indices[j] for j in i] for i in data ]\n","print(data[0], sentences[0])\n","print(data[10], sentences[1])\n","print(data[20], sentences[2])\n","print(data[100], sentences[3])\n","print(data[400], sentences[400])\n","print(data[4000], sentences[4000])\n","print(data[9000], sentences[9000])\n","##Number of sentences\n","print(\"Number of sentences \", len(sentences))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mB0ag6XwrrgT"},"source":["The above code simply maps each character to a number by using char_indices dictionary. Code also includes printing a few examples.\n","\n","From the output, we can see the sequence of numbers corresponding to the sequence of characters. Every sentence will end with 0, which is nothing but space. There are a total of 30,207 sentences. We need to convert this data to RNN friendly data now. In this case study, we would like to take a sequence of 14 characters to predict the next character. Our RNN will have input sequence length 14 and predict one output at a time."]},{"cell_type":"markdown","metadata":{"id":"XB64jPNnr-j3"},"source":["One sentence of length 20 has been converted into six sentences with 14 inputs vs. one output pairs. We need to repeat the same for all the sentences using the below code."]},{"cell_type":"code","metadata":{"id":"p3LLU_RUhztk"},"source":["#Since all the sentences may not be of same length,it is neccessary to make them consistent when passing to keras\n","#We select a sequence length\n","Seq_ln = 14\n","X = []\n","y = []\n","for i in sentences:\n","    for j in range(len(i)-Seq_ln):\n","        X.append(i[j:j+Seq_ln])\n","        y.append(i[j+Seq_ln])\n","len(X), len(y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s2B2LUVesFZ2"},"source":["From the output, we can see that the number of sentences in the data has increased to 142,142 from the original 30,307. Each original sentence has almost created five new pairs of X and y. Below is the example from the code."]},{"cell_type":"code","metadata":{"id":"NwPJIXX9h31B"},"source":["print(\"data[0:2]=\", data[0:2])\n","print(\"sentences[0:2]=\", sentences[0:2])\n","\n","for i in range (0,20):\n","    print(\"X[\",i,\"]=\", X[i],\"y[\",i,\"]=\", y[i])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OnrHgDecsfjO"},"source":["We are trying to print the first two sentences and their corresponding X and y value conversions using the above code.\n","\n","From the output we can see the X and y pairs. We are ready to build the model. We need to one hot encode the data and build the RNN model. Below is the code for the final steps of data processing."]},{"cell_type":"code","metadata":{"id":"meSpAq-eh8sI"},"source":["#The first row is the X's first row up to 14 character\n","#The second row is the X's first row starting from second character up to 14 character\n","#The third row is the X's first row starting from third character up to 14 character and so on \n","X=np.array(X)\n","X1=np.reshape(X,(X.shape[0],X.shape[1],1))\n","X1=keras.utils.to_categorical(np.array(X1), num_classes=len(char_indices))\n","print(X1.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UMS5tkXVh-cW"},"source":["#Target Variable\n","y[:10]\n","#Reshapig our label for model\n","y1 = np.array(y)\n","# one hot encode outputs\n","y1 = keras.utils.to_categorical(np.array(y), num_classes=len(char_indices))\n","y1.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QxdwmYyHiALq"},"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.20)\n","print(\"X_train.shape\", X_train.shape)\n","print(\"y_train.shape\", y_train.shape)\n","print(\"X_test.shape\", X_test.shape)\n","print(\"y_test.shape\", y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7qgimJKes2zw"},"source":["In the above code, we have reshaped the values in X and y, and one-hot encoded them. RNN model expects the data in a particular format, and we need to reshape the data in that format.\n","\n","We are done with data preprocessing. We will go ahead and build the RNN model."]},{"cell_type":"markdown","metadata":{"id":"kLs6xsfetHlA"},"source":["#### Model building\n","While building the RNN model, we need to mention the time steps and the number of hidden nodes. Below is the code for building the model."]},{"cell_type":"code","metadata":{"id":"P_H2WAVfiB_z"},"source":["#building the model\n","model_RNN2 = Sequential()\n","##model.add(SimpleRNN('number of hidden nodes in each rnn cell', input_shape=(timesteps, data_dim)))\n","model_RNN2.add(SimpleRNN(16, input_shape=(X_train.shape[1], X_train.shape[2]))) \n","model_RNN2.add(Dense(len(char_indices)))\n","model_RNN2.add(Activation('softmax'))\n","model_RNN2.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I_LLYuaYuOra"},"source":["From the above code, we can see that we are building the model with 16 hidden nodes.\n","\n","We will compile the model and train it."]},{"cell_type":"code","metadata":{"id":"7toFmlEziD3A"},"source":["# compile network\n","model_RNN2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","# fit network\n","model_RNN2.fit(X_train, y_train, epochs=30, verbose=1, validation_data=(X_test, y_test))\n","model_RNN2.save_weights(\"char_rnn_model_weights_v1.hdf5\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GxdrM8inuYnX"},"source":["We are training the model for 30 epochs and saving it in the weights file.\n","\n","We can see from the output that the model is not improving after reaching 51% accuracy. The model will not show any improvement even after we train it for ten more epochs. Below is the code for additional epochs using the weights file."]},{"cell_type":"code","metadata":{"id":"koQfrQ_0njKC"},"source":["import urllib.request  \n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zT8JxiVhl_KY"},"source":["auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","downloaded = drive.CreateFile({'id':\"1VBKszu-PZY1EbdVl6NblXsW9363rWmrs\"})   \n","downloaded.GetContentFile('Datasets.zip') \n","\n","!unzip -qq 'Datasets.zip'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1dcbdo-ciFlR"},"source":["weightsfile_model_RNN2= \"Pre_trained_models/char_rnn_model_weights_v1.hdf5\"\n","model_RNN2.load_weights(weightsfile_model_RNN2)\n","\n","# compile network\n","model_RNN2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","# fit network\n","model_RNN2.fit(X_train, y_train, epochs=2, verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9hLcHbqFutoe"},"source":["We will now use this model for prediction"]},{"cell_type":"markdown","metadata":{"id":"8msY-hAJuv3T"},"source":["#### Prediction\n","We need to write the predict function that takes input as characters and convert them to numbers. Use these indices to get the predictions, and finally convert them to characters to give us the output. We are going to write a predict function that will predict not just one character but a sequence of characters that will form a word. The prediction loop will continue until it hits a space, which marks the completion of the word. Below is the predict function.\n"]},{"cell_type":"code","metadata":{"id":"783jVtiMiYoU"},"source":["#function to prepare test input\n","def prepare_input(in_text):\n","    X1 = np.array([char_indices[i] for i in in_text]).reshape(1,14,1)\n","    X1=keras.utils.to_categorical(np.array(X1), num_classes=len(char_indices))\n","    return(X1)\n","#function to loop our preditions\n","def complete_pred(in_text):\n","    #original_text = in_text\n","    #generated = in_text\n","    completion = ''\n","    while True:\n","        x = prepare_input(in_text)\n","        pred = model_RNN2.predict_classes(x, verbose=0)[0]\n","\n","        next_char = indices_char[pred]\n","\n","        in_text = in_text[1:] + next_char\n","        completion += next_char\n","\n","        if len(completion)> 20 or next_char == ' ':\n","            return completion"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RfMyczPyvcRX"},"source":["From the code, we can see the two functions. One is for the usual character to index conversion and the other one for predicting the sequence of characters. We will now use this function for predictions."]},{"cell_type":"code","metadata":{"id":"5cYfDzG5iaol"},"source":["in_text = 'officials say '\n","out_word = complete_pred(in_text)\n","print(\"Input text -->\", in_text, \"\\npredicted word ---> \", out_word)\n","in_text = 'how dangerous '\n","out_word = complete_pred(in_text)\n","print(\"Input text -->\", in_text, \"\\npredicted output ---> \", out_word)\n","in_text = 'political and '\n","out_word = complete_pred(in_text)\n","print(\"Input text -->\", in_text, \"\\npredicted output ---> \", out_word)\n","in_text = 'whatever they '\n","out_word = complete_pred(in_text)\n","print(\"Input text -->\", in_text, \"\\npredicted output ---> \", out_word)\n","in_text = 'of particular '\n","out_word = complete_pred(in_text)\n","print(\"Input text -->\", in_text, \"\\npredicted output ---> \", out_word)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fe8X2v-Bv0Jr"},"source":["From the output, we can see that almost all the predictions are simple words like to, of, the, etc., These words are the most frequent in the data; they are known as stop words. RNN is merely predicting the stop words for all inputs. RNN has failed to model this data. The reason is the length of the sequence is 14. In practice, RNN models usually fail when the sequence length is more than ten."]},{"cell_type":"markdown","metadata":{"id":"oOD8pmLNyhEm"},"source":["## LSTM\n","Long Short Term Memory models are created by making several modifications to standard RNN models. RNN models do not have long term memory; we add some features to make it remember long term dependencies."]},{"cell_type":"markdown","metadata":{"id":"37Pta8hM0wLS"},"source":["### LSTM case study\n","The case study on predicting the next characters we have used the standard RNN model. It has failed. We will now use the LSTM model on the same data. Below is the code building the lstm model."]},{"cell_type":"code","metadata":{"id":"25gltKWVideN"},"source":["#building the model\n","model_LSTM = Sequential()\n","#model1.add(LSTM('number of hidden nodes in each rnn cell', input_shape=(timesteps, data_dim)))\n","model_LSTM.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]))) \n","model_LSTM.add(Dense(len(char_indices)))\n","model_LSTM.add(Activation('softmax'))\n","model_LSTM.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"28CjCUxu1Rya"},"source":["We will now compile and train the model"]},{"cell_type":"code","metadata":{"id":"orxuChTaighY"},"source":["# compile network\n","model_LSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","# fit network\n","model_LSTM.fit(X_train, y_train, epochs=30, verbose=1)\n","model_LSTM.save_weights(\"char_LSTM_model_weights_v1.hdf5\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TCEIHTLb1ZFx"},"source":["This model shows a much better accuracy of 80%; on the same dataset, RNN has given us 50% accuracy. We will now use this model for prediction. Our prediction is made by predicting one character at a time and continue the predictions until we see space. Those sequences of characters will be formed as a predicted word."]},{"cell_type":"code","metadata":{"id":"Q8WFI_JGiiMQ"},"source":["weightsfile_model_LSTM= \"Pre_trained_models/char_LSTM_model_weights_v1.hdf5\"\n","model_LSTM.load_weights( weightsfile_model_LSTM)\n","\n","# compile network\n","model_LSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","# fit network\n","model_LSTM.fit(X_train, y_train,epochs=2, verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CvrPjuFiimgs"},"source":["#function to prepare test input\n","def prepare_input1(in_text):\n","    X1 = np.array([char_indices[i] for i in in_text]).reshape(1,14,1)\n","    X1= keras.utils.to_categorical(np.array(X1), num_classes=len(char_indices))\n","    return(X1)\n","#function to loop our preditions\n","def complete_pred1(in_text):\n","    #original_text = in_text\n","    #generated = in_text\n","    completion = ''\n","    while True:\n","        x = prepare_input1(in_text)\n","        pred = model_LSTM.predict_classes(x, verbose=0)[0]\n","        next_char = indices_char[pred]\n","\n","        in_text = in_text[1:] + next_char\n","        completion += next_char\n","\n","        if len(completion)> 20 or next_char == ' ':\n","            return completion"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SrHb-3TW1jrF"},"source":["We will use the above function to predict on few test points"]},{"cell_type":"code","metadata":{"id":"Z8pTOzbyipLo"},"source":["in_text = 'the emergence '\n","out_word = complete_pred1(in_text)\n","print(\"Input text -->\", in_text, \"; predicted output ---> \", out_word)\n","in_text = 'officials say '\n","out_word = complete_pred1(in_text)\n","print(\"Input text -->\", in_text, \"; predicted output ---> \", out_word)\n","in_text = 'and sentenced '\n","out_word = complete_pred1(in_text)\n","print(\"Input text -->\", in_text, \"; predicted output ---> \", out_word)\n","in_text = 'a combination '\n","out_word = complete_pred1(in_text)\n","print(\"Input text -->\", in_text, \"; predicted output ---> \", out_word)\n","in_text = 'and according '\n","out_word = complete_pred1(in_text)\n","print(\"Input text -->\", in_text, \"; predicted output ---> \", out_word)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GzPNWiAc1oko"},"source":["We will take a few test cases and get their RNN and LSTM model predictions. That will help us in\n","comparing their performance. The below code gets the predictions using RNN and LSTM."]},{"cell_type":"code","metadata":{"id":"zBEoaqDAirWK"},"source":["in_text = 'how dangerous '\n","out_word = complete_pred1(in_text)\n","print(\"Input text -->\", in_text, \"\\nLSTM Prediction ---> \", out_word)\n","out_word1 = complete_pred(in_text)\n","print(\"RNN Prediction ---> \", out_word1)\n","\n","print(\"\\n\")\n","in_text = 'political and '\n","out_word = complete_pred1(in_text)\n","print(\"Input text -->\", in_text, \"\\nLSTM Prediction ---> \", out_word)\n","out_word1 = complete_pred(in_text)\n","print(\"RNN Prediction ---> \", out_word1)\n","\n","print(\"\\n\")\n","in_text = 'of particular '\n","out_word = complete_pred1(in_text)\n","print(\"Input text -->\", in_text, \"\\nLSTM Prediction ---> \", out_word)\n","out_word1 = complete_pred(in_text)\n","print(\"RNN Prediction ---> \", out_word1)\n","\n","print(\"\\n\")\n","in_text = 'whatever they '\n","out_word = complete_pred1(in_text)\n","print(\"Input text -->\", in_text, \"\\nLSTM Prediction ---> \", out_word)\n","out_word1 = complete_pred(in_text)\n","print(\"RNN Prediction ---> \", out_word1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DZnQ6s9d1vY1"},"source":["We can see from the results that the RNN predictions are generic words, whereas LSTM is predicting\n","more related words. We will now see other applications of LSTM."]},{"cell_type":"markdown","metadata":{"id":"Wts2SYQfHLI9"},"source":["## Case Study – Language Translation\n","LSTM models are powerful sequence models available today. One of the most useful applications of LSTM is a sequence to sequence models. Where we have a sequence as input and output is a sequence. If we are building a chatbot, we have a question as an input sequence of words, the output sequence of words will be the answer. Similarly, if we are talking about the language\n","translation model. Input will be a sequence of words from language-1, and output will be a sequence of words from language-2. Language-1 is the source language, and language-2 is the target language.\n","Like the translation model from English to French"]},{"cell_type":"markdown","metadata":{"id":"ieDYqM8qHpnz"},"source":["### Data and Objective\n","In this case study the source language is English, and the target language is French. The objective is\n","to build a machine translation model. The data set has been downloaded from the http://www.manythings.org/anki/ website. Apart from English to French, there are several other datasets too. As an example, we are considering English to French in this case study. Dataset is publically available under CC-BY 2.0 license. Below code is used for importing the data"]},{"cell_type":"code","metadata":{"id":"668Wc-XUiui2"},"source":["raw_data= open(\"fra-eng/fra.txt\", mode='rt', encoding='utf-8').read()\n","raw_data=raw_data.strip().split('\\n')\n","raw_data=[i.split('\\t') for i in raw_data]\n","lang1_lang2_data=array(raw_data)\n","print(lang1_lang2_data)\n","print(\"Overall pairs\", len(lang1_lang2_data))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ef1Bkg27IQ3H"},"source":["The above code is trying to import the text file split the file into individual lines.\n","\n","From the output, we can see that there is some text from language-1 followed by an identical corresponding from language-2. In our case, language-1 is English, and language-2 is French. From here on, we will use the generalized terminology of lang1 and lang2 terminology. It will be easy to read even if we are trying with different languages. The overall lang1 and lang2 pairs are 175,623. We need many more pairs to build a flawless model like google translate. We can build a decent model with this data."]},{"cell_type":"markdown","metadata":{"id":"h93G4_J8IivA"},"source":["### Data processing\n","We will now perform some basic data pre-processing tasks like removing punctuation, converting to lowercase"]},{"cell_type":"code","metadata":{"id":"wRFpr0PMiyRh"},"source":["# Remove punctuation\n","lang1_lang2_data[:,0] = [word.translate(str.maketrans('', '', string.punctuation)) for word in lang1_lang2_data[:,0]]\n","lang1_lang2_data[:,1] = [word.translate(str.maketrans('', '', string.punctuation)) for word in lang1_lang2_data[:,1]]\n","\n","print(lang1_lang2_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hoerDPSQtwyg"},"source":["## convert text to lowercase\n","for word in range(len(lang1_lang2_data)):\n","    lang1_lang2_data[word,0] = lang1_lang2_data[word,0].lower()\n","    lang1_lang2_data[word,1] = lang1_lang2_data[word,1].lower()\n","print(lang1_lang2_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LVl9dFgst1On"},"source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(lang1_lang2_data[:, 0])\n","lang1_tokens=tokenizer\n","lang1_vocab_size = len(lang1_tokens.word_index) + 1\n","print(\"lang1_vocab_size\", lang1_vocab_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DHBMgAxYt304"},"source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(lang1_lang2_data[:, 1])\n","lang2_tokens=tokenizer\n","lang2_vocab_size = len(lang2_tokens.word_index) + 1\n","print(\"lang2_vocab_size\", lang2_vocab_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"suoxLHpQIzSG"},"source":["In the above code, we tried removing punctuation marks, then convert everything into lowercase followed by tokenizing. Tokenizing is nothing but dividing the data into words. Till now, our datasets have less vocabulary. We did manual tokens. Here we are using the tokenizer function.\n","\n","From the output, we can see that there are 14,671 unique words in language-1 and 33,321 unique\n","words in language2. We are converting the data into a sequence of words. Later these words will be\n","converted to vectors in the word embedding layer. We will now create the train and test data."]},{"cell_type":"code","metadata":{"id":"o6EapYSEt6pk"},"source":["# split data into train and test set\n","train, test = train_test_split(lang1_lang2_data, test_size=0.1, random_state = 44)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ffgvjo9MJZRz"},"source":["We are now ready to build the model, but before that, we need to convert these words into numbers followed by padding with zeros. Padding will mark the end of the paragraph or sentence."]},{"cell_type":"code","metadata":{"id":"wCzl45CJt9rW"},"source":["lang1_seq_length=15\n","lang2_seq_length=15\n","\n","X_train_seq=lang1_tokens.texts_to_sequences(train[:, 0])\n","X_train= pad_sequences(X_train_seq,lang1_seq_length,padding='post')\n","\n","Y_train_seq=lang2_tokens.texts_to_sequences(train[:, 1])\n","Y_train= pad_sequences(Y_train_seq,lang2_seq_length,padding='post')\n","\n","X_test_seq=lang1_tokens.texts_to_sequences(test[:, 0])\n","X_test= pad_sequences(X_test_seq,lang1_seq_length,padding='post')\n","\n","Y_test_seq=lang2_tokens.texts_to_sequences(test[:, 1])\n","Y_test= pad_sequences(Y_test_seq,lang2_seq_length,padding='post')\n","\n","print(\"X_train.shape\", X_train.shape)\n","print(\"Y_train.shape\",Y_train.shape)\n","print(\"X_test.shape\",X_test.shape)\n","print(\"Y_test.shape\", Y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C00gE1AQJhSW"},"source":["The above code first maps the words to numbers using texts_to_sequnces function. Here is this example, we took the average length of a sentence as 15, if a sentence is less than 15 words long, then there will be zeros added to make every sentence of length 15. Long sentences will be cut down to 15 words. This padding is necessary to bring uniformity in the length of each sentence. If required, we can increase the length from 15 words to 20 words.\n","\n","We will now print a row from the data to see the result of padding. Below is the code for printing a sample data point"]},{"cell_type":"code","metadata":{"id":"e-snJaoTuAa-"},"source":["print(\"Text data\", [train[5, 0]])\n","print('Numbers sequence', X_train_seq[5])\n","print('Padded Sequence', X_train[5])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sM4GJm5NJ1eQ"},"source":["We can see from the output that the sentence is converted into numbers. This sentence has only 12 words; hence it has been padded with three zeros at the end. Now we are ready for building the model."]},{"cell_type":"markdown","metadata":{"id":"8Kf_-iD2Kbzk"},"source":["### Encoder and Decoder\n","The Sequence to sequence models is very different from all the models that we discussed until now. A sequence of words can not be simply converted to a sequence of words just by word to word conversion. We often see that the input and output sequences have different lengths. We need to follow Encoder and Decoder architecture to build the sequence to sequence models. The encoder is an LSTM model that will be used to understand and model the input sequence. Similarly, the decoder is another LSTM that will be used to model the output sequence."]},{"cell_type":"markdown","metadata":{"id":"stHvDXYiKm8e"},"source":["### Model building\n","There are four significant steps in this model architecture.\n","* Word embedding for language-1\n","* Encoder LSTM\n","* Repeat Vector generation from thought vector. This step is to match the decoder dimensions\n","* Decoder LSTM\n","Below is the model that covers the above points"]},{"cell_type":"code","metadata":{"id":"9asKK0DNuDFE"},"source":["model = Sequential()\n","model.add(Embedding(lang1_vocab_size, 256, input_length=lang1_seq_length, mask_zero=True))\n","model.add(LSTM(128))\n","model.add(RepeatVector(lang2_seq_length))\n","model.add(LSTM(128, return_sequences=True))\n","model.add(Dense(lang2_vocab_size, activation='softmax'))\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JZs8k-x1K1_V"},"source":["We can now compile and train this model using the below code"]},{"cell_type":"code","metadata":{"id":"_Glxj-BJuLHR"},"source":["model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n","history = model.fit(X_train, Y_train.reshape(Y_train.shape[0], Y_train.shape[1], 1),  epochs=1, verbose=1, batch_size=1024)\n","model.save_weights('Eng_fra_model.hdf5')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"31NraSWEK-r9"},"source":["The above code takes nearly four hours of execution time on a typical system. There are 8.3 million weight parameters. Once the model is built and saved, we can use it for prediction. We already have a model weight file; then, we can stop the above training process and directly load weights into the model. We executed this model and saved the weights. There is a high chance that this model training might hang the system. Readers can directly load the weights from the saved model. If required, we can run a few more epochs on top of it. Below is the code for loading the weights into\n","the model"]},{"cell_type":"code","metadata":{"id":"Eo3ieNWOuNb3"},"source":["model.load_weights(\"Pre_trained_models/Eng_fra_model.hdf5\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QVqF2EYyLGeR"},"source":["### Prediction\n","The prediction has three steps.\n","* Taking text data as input\n","* Pre-processing the text data\n","* Converting into numbers\n","* Prediction of the output sequences.\n","* Finally, Converting the output sequence of numbers into words.\n","Below is the code for pre-processing"]},{"cell_type":"code","metadata":{"id":"ZgHVrdAjRlr8"},"source":["def one_line_prediction(text1):\n","    \n","    def to_lines(text):\n","          sents = text.strip().split('\\n')\n","          sents = [i.split('\\t') for i in sents]\n","          return sents\n","    small_input = to_lines(text1)\n","    small_input = array(small_input)\n","    \n","    # Remove punctuation\n","    small_input[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in small_input[:,0]]\n","    # convert text to lowercase\n","    for i in range(len(small_input)):\n","        small_input[i,0] = small_input[i,0].lower()\n","\n","    #encode and pad sequences\n","    small_input_seq=lang1_tokens.texts_to_sequences(small_input[0])\n","    small_input= pad_sequences(small_input_seq,lang1_seq_length,padding='post')\n","   \n","\n","    #Load the model\n","    #Eng French Model\n","    #model.load_weights('/content/drive/My Drive/Training/Book/0.Chapters/Chapter12 RNN and LSTM/1.Archives/Eng_fra_model_v2.hdf5')\n","\n","    pred_seq = model.predict_classes(small_input[0:1].reshape((small_input[0:1].shape[0],small_input[0:1].shape[1])))\n","    \n","    def num_to_word(n, tokens):\n","          for word, index in tokens.word_index.items():\n","              if index == n:\n","                  return word\n","          return None\n","\n","    Lang2_text = []\n","    for word_num in pred_seq:\n","          sing_pred = []\n","          for i in range(len(word_num)):\n","                t = num_to_word(word_num[i], lang2_tokens)\n","                if i > 0:\n","                    if (t == num_to_word(word_num[i-1], lang2_tokens)) or (t == None):\n","                        sing_pred.append('')\n","                    else:\n","                        sing_pred.append(t)\n","                else:\n","                      if(t == None):\n","                              sing_pred.append('')\n","                      else:\n","                              sing_pred.append(t) \n","          Lang2_text.append(' '.join(sing_pred))\n","    return(Lang2_text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yITosHDlLnp9"},"source":["The above code looks complicated, but it is doing a simple task of mapping number sequence to\n","words. In this code, we added several if-else conditions that will take care of exceptions. Like null\n","input, end of the line, and so on. Otherwise, the below three lines are sufficient"]},{"cell_type":"markdown","metadata":{"id":"WxvNuoQsL9pP"},"source":["Usually, it is a good idea to combine all the prediction related tasks into one predict function. Below are some of the results from our model predictions."]},{"cell_type":"code","metadata":{"id":"ROu7ZIRWbiZV"},"source":["Input_sentences=[\"have a great Good day\",\n","                 \"Do you speak English\",\n","                 \"I do not know your language\",\n","                 \"I need help\",\n","                 \"Thank you very much\",\n","                 \"Where can I get this\",\n","                 \"How much does it cost\",\n","                 \"Where is the bathroom\",\n","                 \"Where is the ATM\",\n","                 \"I am a visitor here\",\n","                 \"Excuse me\",\n","                 \"What do you do for living\",\n","                 \"Here is my passport\"]\n","\n","for sent in Input_sentences:\n","  print([sent] , \" -->\",one_line_prediction(sent))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PiwqZv4dL0ex"},"source":["From the above results, we can see the model performance is not excellent. However, it can be\n","made better with more training data and a few more epochs."]}]}