{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Chapter-8.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"jkK99mFEAkQ8"},"source":["# Chapter-8: Artificial Neural Network"]},{"cell_type":"markdown","metadata":{"id":"dpachWflVSsp"},"source":["Importing required packages and libraries"]},{"cell_type":"code","metadata":{"id":"aU4KOCtjpbHj"},"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import statsmodels.formula.api as sm\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix as cm\n","import random"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YEa808gHpl9Z"},"source":["!pip install neurolab"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d332XKtTrg-z"},"source":["import neurolab as nl"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GHzfvCzRrkgx"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"blzaJh6n9oCE"},"source":["github_link=\"https://raw.githubusercontent.com/venkatareddykonasani/ML_DL_py_TF/master/Chapter8_ANN/Datasets/\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gPINszAXVZjZ"},"source":["## Decision Boundary\n","The logistic regression line gives us the predicted values between 0 and 1 as the final output. For a new point, if the predicted value is 0.95, then we can consider the predicted class as class-1 if the predicted value is 0.05, then we consider the predicted class as class-0. Usually, we set a threshold at 0.5. If the predicted values are below 0.5, then we classify them as class-0 and rest all classified as class-1. Through logistic regression line looks like an “S” shaped curve, when it comes to decision making, we use it as a decision boundary that separates class-0 and class-1."]},{"cell_type":"markdown","metadata":{"id":"aPG9GqZFasui"},"source":["We will see example of decision Boundary:\n","\n","Employee purchase data has three columns. Employee Age, Experience and whether they have purchased the product or not. The product is related to insurance. The objective is to predict the target variable “purchase” by using Age and Experience as predictor variables. We will build a logistic regression line. However, we are interested in the decision boundary after the creation of logistic regression. For this demo purpose, we will use a subset of the data. We will use the complete data in the later sections."]},{"cell_type":"code","metadata":{"id":"8fIOXfzrsGzh"},"source":["#Emp_Purchase_raw = pd.read_csv(r\"/content/drive/My Drive/DataSets/Chapter-8/Chapter-8/datasets/Emp_Purchase/Emp_Purchase.csv\")\n","Emp_Purchase_raw = pd.read_csv(github_link+\"/Emp_Purchase/Emp_Purchase.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YcstDqlespr6"},"source":["Emp_Purchase1=Emp_Purchase_raw[Emp_Purchase_raw.Sample_Set<3]\n","print(Emp_Purchase1.shape)\n","print(Emp_Purchase1.columns.values)\n","print(Emp_Purchase1.head(10))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N2ldEenasuEq"},"source":["Emp_Purchase1.Purchase.value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bySAggLybdlr"},"source":["There are 74 records in this subset. We later use Age, Experience to predict purchase. We will now draw the graph of the data that shows the relation between the predictor and target variables. The below code helps us in plotting the data of all the three columns."]},{"cell_type":"code","metadata":{"id":"ddV2IUKgswiX"},"source":["fig = plt.figure()\n","ax1 = fig.add_subplot(111)\n","plt.rcParams[\"figure.figsize\"] = (8,6)\n","plt.title('Age, Experience  vs Purchase', fontsize=20)\n","\n","ax1.scatter(Emp_Purchase1.Age[Emp_Purchase1.Purchase==0],Emp_Purchase1.Experience[Emp_Purchase1.Purchase==0], s=100, c='b', marker=\"o\", label='Purchase 0')\n","ax1.scatter(Emp_Purchase1.Age[Emp_Purchase1.Purchase==1],Emp_Purchase1.Experience[Emp_Purchase1.Purchase==1], s=100, c='r', marker=\"x\", label='Purchase 1')\n","ax1.set_xlabel('Age',fontsize=15)\n","ax1.set_ylabel('Experience',fontsize=15)\n","\n","plt.xlim(min(Emp_Purchase1.Age), max(Emp_Purchase1.Age))\n","plt.ylim(min(Emp_Purchase1.Experience), max(Emp_Purchase1.Experience))\n","plt.legend(loc='upper left');\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"heHIrOAWbnwD"},"source":["Please note that we need not draw these graphs while solving the actual problems. We are drawing\n","them here to get a better visual intuition."]},{"cell_type":"markdown","metadata":{"id":"jZDCKO7IcKKj"},"source":["From the output plot, we can see both the classes of the output. Purchase =0 and Purchase=1. We will now build the logistic regression. Derive the decision boundary then draw the decision boundary on top of this plot. We can already make a guess where the logistic regression is going to appear."]},{"cell_type":"code","metadata":{"id":"NzID36Obs1Hp"},"source":["model1 = sm.logit(formula='Purchase ~ Age+Experience', data=Emp_Purchase1)\n","fitted1 = model1.fit()\n","print(fitted1.summary2())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6E6zrPRftAy6"},"source":["predicted_values=fitted1.predict(Emp_Purchase1[[\"Age\"]+[\"Experience\"]])\n","predicted_values[1:10]\n","threshold=0.5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6uv-9dCgtEGB"},"source":["import numpy as np\n","predicted_class=np.zeros(predicted_values.shape)\n","predicted_class[predicted_values>threshold]=1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"btmLLQjutRiL"},"source":["predicted_class"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jteJnnZStTMp"},"source":["from sklearn.metrics import confusion_matrix as cm\n","ConfusionMatrix = cm(Emp_Purchase1[['Purchase']],predicted_class)\n","print(ConfusionMatrix)\n","accuracy=(ConfusionMatrix[0,0]+ConfusionMatrix[1,1])/sum(sum(ConfusionMatrix))\n","print('Accuracy : ',accuracy)\n","error=1-accuracy\n","print('Error: ',error)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1sdE_VK_ckMD"},"source":["We will find the coefficients"]},{"cell_type":"code","metadata":{"id":"3RKeuKnVtWUy"},"source":["slope1=fitted1.params[1]/(-fitted1.params[2])\n","intercept1=fitted1.params[0]/(-fitted1.params[2])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vUgiu7uqcrUW"},"source":["Now we will finally draw decision boundary for this regression model"]},{"cell_type":"code","metadata":{"id":"YVc3ZCYjtafQ"},"source":["fig = plt.figure()\n","ax1 = fig.add_subplot(111)\n","plt.rcParams[\"figure.figsize\"] = (8,6)\n","plt.title('Decision Boundary', fontsize=20)\n","\n","ax1.scatter(Emp_Purchase1.Age[Emp_Purchase1.Purchase==0],Emp_Purchase1.Experience[Emp_Purchase1.Purchase==0], s=100, c='b', marker=\"o\", label='Purchase 0')\n","ax1.scatter(Emp_Purchase1.Age[Emp_Purchase1.Purchase==1],Emp_Purchase1.Experience[Emp_Purchase1.Purchase==1], s=100, c='r', marker=\"x\", label='Purchase 1')\n","ax1.set_xlabel('Age',fontsize=15)\n","ax1.set_ylabel('Experience',fontsize=15)\n","\n","plt.xlim(min(Emp_Purchase1.Age), max(Emp_Purchase1.Age))\n","plt.ylim(min(Emp_Purchase1.Experience), max(Emp_Purchase1.Experience))\n","plt.legend(loc='upper left');\n","\n","x_min, x_max = ax1.get_xlim()\n","ax1.plot([0, x_max], [intercept1, x_max*slope1+intercept1])\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KLZPCu-Tc2GG"},"source":["From the output, we can see the decision boundary created by the logistic regression line. As expected, the decision boundary is between the two classes. That concludes this section. Finally, the takeaway is, “Every logistic regression line creates a decision boundary; it looks like a straight line between two classes.” "]},{"cell_type":"markdown","metadata":{"id":"4JJ8w9_ic_sc"},"source":["## Multiple decision boundary\n","The creation of the decision boundary works correctly when the two classes in the target variable separable with a straight line. Not every dataset has a clear separating boundary between them. Suppose if we have data which cannot be divided in two classes using single decision boundary then a logistic regression line may fail in this case. All the cases where the separating boundary is non-linear or when we need more than one decision boundary, logistic regression fails. In the above example, we considered a subset of the data. We will now look at the full data. Below code helps us in plotting the data "]},{"cell_type":"markdown","metadata":{"id":"tSRIgs9Jlo4Q"},"source":["### Problem"]},{"cell_type":"code","metadata":{"id":"QIVSYCFmteyI"},"source":["fig = plt.figure()\n","ax1 = fig.add_subplot(111)\n","plt.rcParams[\"figure.figsize\"] = (8,6)\n","plt.title('Age, Experience  vs Purchase - Overall Data', fontsize=20)\n","\n","\n","ax1.scatter(Emp_Purchase_raw.Age[Emp_Purchase_raw.Purchase==0],Emp_Purchase_raw.Experience[Emp_Purchase_raw.Purchase==0], s=100, c='b', marker=\"o\", label='Purchase 0')\n","ax1.scatter(Emp_Purchase_raw.Age[Emp_Purchase_raw.Purchase==1],Emp_Purchase_raw.Experience[Emp_Purchase_raw.Purchase==1], s=100, c='r', marker=\"x\", label='Purchase 1')\n","ax1.set_xlabel('Age',fontsize=15)\n","ax1.set_ylabel('Experience',fontsize=15)\n","\n","plt.xlim(min(Emp_Purchase_raw.Age), max(Emp_Purchase_raw.Age))\n","plt.ylim(min(Emp_Purchase_raw.Experience), max(Emp_Purchase_raw.Experience))\n","plt.legend(loc='upper left');\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W3oe79TKnSRg"},"source":["We will now force-fit a logistic regression line to this data and try to draw the decision boundary. In the earlier case, we could easily guess where the decision boundary will end up, now we can not guess it. Logistic regression does not work here. We will force-fit one and see the results."]},{"cell_type":"code","metadata":{"id":"Qddx26AstuuE"},"source":["model = sm.logit(formula='Purchase ~ Age+Experience', data=Emp_Purchase_raw)\n","fitted = model.fit()\n","print(fitted.summary2())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7WL4Jc1Rb4k6"},"source":["Getting slope and intercept of the logistic line"]},{"cell_type":"code","metadata":{"id":"5alJAwjStyNC"},"source":["slope=fitted.params[1]/(-fitted.params[2])\n","intercept=fitted.params[0]/(-fitted.params[2])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZtKawqv-t1F4"},"source":["predicted_values=fitted.predict(Emp_Purchase_raw[[\"Age\"]+[\"Experience\"]])\n","predicted_values[1:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3MFYsmskt4bR"},"source":["threshold=0.5\n","threshold"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2BlnB-qlt8Hm"},"source":["import numpy as np\n","predicted_class=np.zeros(predicted_values.shape)\n","predicted_class[predicted_values>threshold]=1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DbZraaPfuAKw"},"source":["predicted_class[1:10]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aBvI7dLLcjeq"},"source":["Creating confusion matrix"]},{"cell_type":"code","metadata":{"id":"ijuzjdlIuMni"},"source":["from sklearn.metrics import confusion_matrix as cm\n","ConfusionMatrix = cm(Emp_Purchase_raw[['Purchase']],predicted_class)\n","print(ConfusionMatrix)\n","accuracy=(ConfusionMatrix[0,0]+ConfusionMatrix[1,1])/sum(sum(ConfusionMatrix))\n","print(accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GSfnb1asuQ-6"},"source":["error=1-accuracy\n","error"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bo1BAjuSuUiB"},"source":["fig = plt.figure()\n","ax1 = fig.add_subplot(111)\n","plt.rcParams[\"figure.figsize\"] = (8,6)\n","plt.title('Decision Boundary - Overall Data', fontsize=20)\n","\n","ax1.scatter(Emp_Purchase_raw.Age[Emp_Purchase_raw.Purchase==0],Emp_Purchase_raw.Experience[Emp_Purchase_raw.Purchase==0], s=100, c='b', marker=\"o\", label='Purchase 0')\n","ax1.scatter(Emp_Purchase_raw.Age[Emp_Purchase_raw.Purchase==1],Emp_Purchase_raw.Experience[Emp_Purchase_raw.Purchase==1], s=100, c='r', marker=\"x\", label='Purchase 1')\n","plt.xlim(min(Emp_Purchase_raw.Age), max(Emp_Purchase_raw.Age))\n","plt.ylim(min(Emp_Purchase_raw.Experience), max(Emp_Purchase_raw.Experience))\n","plt.legend(loc='upper left');\n","ax1.set_xlabel('Age',fontsize=15)\n","ax1.set_ylabel('Experience',fontsize=15)\n","\n","x_min, x_max = ax1.get_xlim()\n","ax1.plot([0, x_max], [intercept, x_max*slope+intercept],linewidth=5, c='r')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bBKLHWRidwq9"},"source":["From the output, we can see that the decision boundary at the bottom left side, near the origin. It is\n","visibly apparent that this decision boundary cannot separate the classes."]},{"cell_type":"markdown","metadata":{"id":"dVyupEQ_lloI"},"source":["### Solution\n","If there are multiple decision boundaries in data, then directly predicting the target with input variables does not work. By looking at the data, we can see some patterns, and we are sure that we can have a better classification model. In the overall data plot, if we consider only region-1(R1), then logistic regression works perfectly to separate the two classes. Same way, if we consider region-2(R2), then also logistic regression does the best job of separating both the classes. We will build logistic regression model-1 for region-1. We get the predicted values from that model. These precited values will be our intermediate output h 1 . Similarly, we will build another model for region-2; this will be our second intermediate output h 2 . Finally, we will use these intermediate outputs h 1 and h 2 for prediction of y. Instead of building one logistic regression line x 1 x 2 vs y, we are now building three logistic regression lines. They are x 1, x 2 vs h 1 , x 1, x 2 vs h 2 , and finally h 1, h 2 vs y. Since we are changing the region, the values of x 1 and x 2 are different in the intermediate models h 1 and h 2"]},{"cell_type":"markdown","metadata":{"id":"zuW8u9F7ps4O"},"source":["### Building intermediate output models"]},{"cell_type":"markdown","metadata":{"id":"AzSt5lBCQH_s"},"source":["**h1 model**"]},{"cell_type":"code","metadata":{"id":"t_vgYKVrua5p"},"source":["Emp_Purchase1=Emp_Purchase_raw[Emp_Purchase_raw.Sample_Set<3]\n","model1 = sm.logit(formula='Purchase ~ Age+Experience', data=Emp_Purchase1)\n","fitted1 = model1.fit()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pxDEVg7guxls"},"source":["Emp_Purchase_raw['h1']=fitted1.predict(Emp_Purchase_raw[[\"Age\"]+[\"Experience\"]])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"02cKF3RaQPR0"},"source":["**h2 model**"]},{"cell_type":"code","metadata":{"id":"cOEB0LJLu0GL"},"source":["Emp_Purchase2=Emp_Purchase_raw[Emp_Purchase_raw.Sample_Set>1]\n","model2 = sm.logit(formula='Purchase ~ Age+Experience', data=Emp_Purchase2)\n","fitted2 = model2.fit(method=\"bfgs\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TBdfhNgsu2pO"},"source":["Emp_Purchase_raw['h2']=fitted2.predict(Emp_Purchase_raw[[\"Age\"]+[\"Experience\"]])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zfcaDnRZvLey"},"source":["print(Emp_Purchase_raw[['Age', 'Experience','h1','h2','Purchase']])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"quSHcTRMQXvY"},"source":["In the above output, we can see the values of h1 and h2. These are the predictions made from the\n","logistic regression line. Before we go ahead with model building, we will plot h1,h2 vs. target\n","variable. Below code helps us in plotting the target against h1 and h2"]},{"cell_type":"code","metadata":{"id":"Ngr17d1Rvy70"},"source":["fig = plt.figure()\n","ax = fig.add_subplot(111)\n","plt.rcParams[\"figure.figsize\"] = (8,6)\n","plt.title('h1, h2 vs target ', fontsize=20)\n","\n","ax.scatter(Emp_Purchase_raw.h1[Emp_Purchase_raw.Purchase==0],Emp_Purchase_raw.h2[Emp_Purchase_raw.Purchase==0], s=100, c='b', marker=\"o\", label='Purchase 0')\n","ax.scatter(Emp_Purchase_raw.h1[Emp_Purchase_raw.Purchase==1],Emp_Purchase_raw.h2[Emp_Purchase_raw.Purchase==1], s=100, c='r', marker=\"x\", label='Purchase 1')\n","ax.set_xlabel('h1',fontsize=15)\n","ax.set_ylabel('h2',fontsize=15)\n","\n","plt.xlim(min(Emp_Purchase_raw.h1), max(Emp_Purchase_raw.h1)+0.2)\n","plt.ylim(min(Emp_Purchase_raw.h2), max(Emp_Purchase_raw.h2)+0.2)\n","\n","plt.legend(loc='lower left');\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ji7tDBm9QgbQ"},"source":["In the plot, we can see the input variables h1 and h2 can classify the target variable y. We can draw\n","one straight line that separates clas-0 and class-1. If we build a logistic regression line that it may\n","appear on the top right corner like a diagonal line. Let us build and draw the decision boundary."]},{"cell_type":"code","metadata":{"id":"lWlV1OK6v4fK"},"source":["model_combined = sm.logit(formula='Purchase ~ h1+h2', data=Emp_Purchase_raw)\n","fitted_combined = model_combined.fit(method=\"bfgs\")\n","print(fitted_combined.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bts2ujNPQmJh"},"source":["**Logistic Regerssion model with Intermediate outputs as input**"]},{"cell_type":"code","metadata":{"id":"-Sdr9Pnx2UHw"},"source":["slope_combined=fitted_combined.params[1]/(-fitted_combined.params[2])\n","intercept_combined=fitted_combined.params[0]/(-fitted_combined.params[2])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bIilqJ73Qydk"},"source":["Finally draw the decision boundary for this logistic regression model"]},{"cell_type":"code","metadata":{"id":"Px_A2rnb2Xfd"},"source":["fig = plt.figure()\n","ax2 = fig.add_subplot(111)\n","plt.rcParams[\"figure.figsize\"] = (8,7)\n","plt.title('h1, h2 vs target ', fontsize=20)\n","\n","ax2.scatter(Emp_Purchase_raw.h1[Emp_Purchase_raw.Purchase==0],Emp_Purchase_raw.h2[Emp_Purchase_raw.Purchase==0], s=100, c='b', marker=\"o\", label='Purchase 0')\n","ax2.scatter(Emp_Purchase_raw.h1[Emp_Purchase_raw.Purchase==1],Emp_Purchase_raw.h2[Emp_Purchase_raw.Purchase==1], s=100, c='r', marker=\"x\", label='Purchase 1')\n","ax2.set_xlabel('h1',fontsize=15)\n","ax2.set_ylabel('h2',fontsize=15)\n","\n","plt.xlim(min(Emp_Purchase_raw.h1), max(Emp_Purchase_raw.h1)+0.2)\n","plt.ylim(min(Emp_Purchase_raw.h2), max(Emp_Purchase_raw.h2)+0.2)\n","\n","plt.legend(loc='lower left');\n","\n","x_min, x_max = ax2.get_xlim()\n","y_min,y_max=ax2.get_ylim()\n","ax2.plot([x_min, x_max], [x_min*slope_combined+intercept_combined, x_max*slope_combined+intercept_combined],linewidth=4)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dFpXPmzORNTO"},"source":["The decision boundary is as expected by us. We have now solved the problem of multiple decision\n","boundaries using the intermediate output models. We have transformed the x1, x2 vs. y data into a\n","different space of h1, h2 vs. y. In the first case, x1, x2 vs. y, we could not separate a straight line\n","decision boundary. When these input variables are transformed into intermediate variables, we can\n","now find the separating boundary."]},{"cell_type":"markdown","metadata":{"id":"_g6gde4fQ6zX"},"source":["**Accuracy and error of the model1**"]},{"cell_type":"code","metadata":{"id":"o0bWdD5-2eRN"},"source":["predicted_values=fitted_combined.predict(Emp_Purchase_raw[[\"h1\"]+[\"h2\"]])\n","predicted_values[1:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nHLsshhH3y7x"},"source":["threshold=0.5\n","threshold"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3WY25ITS34CS"},"source":["import numpy as np\n","predicted_class=np.zeros(predicted_values.shape)\n","predicted_class[predicted_values>threshold]=1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b4geMxni37Xw"},"source":["from sklearn.metrics import confusion_matrix as cm\n","ConfusionMatrix = cm(Emp_Purchase_raw[['Purchase']],predicted_class)\n","print(\"ConfusionMatrix\\n\", ConfusionMatrix)\n","accuracy=(ConfusionMatrix[0,0]+ConfusionMatrix[1,1])/sum(sum(ConfusionMatrix))\n","print(\"accuracy\\n\", accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C4cnSaJRTSuq"},"source":["We finally classified y with high accuracy. We could achieve above 90% accuracy with three models.\n","If the amount of non-linearity is more, then we may need more of these intermediate models in the\n","first layer. We need two decision boundaries for the classification of this data. Below code helps us\n","in visualizing the decision boundaries from the two intermediate output models h 1 and h 2"]},{"cell_type":"code","metadata":{"id":"y2Wqra5c4Lv9"},"source":["slope1=fitted1.params[1]/(-fitted1.params[2])\n","intercept1=fitted1.params[0]/(-fitted1.params[2])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O66SZFry4P9R"},"source":["slope2=fitted2.params[1]/(-fitted2.params[2])\n","intercept2=fitted2.params[0]/(-fitted2.params[2])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NOKHQfg_4SVU"},"source":["fig = plt.figure()\n","ax1 = fig.add_subplot(111)\n","plt.rcParams[\"figure.figsize\"] = (8,6)\n","plt.title('Age, Experience  vs Purchase - Overall Data', fontsize=20)\n","\n","\n","ax1.scatter(Emp_Purchase_raw.Age[Emp_Purchase_raw.Purchase==0],Emp_Purchase_raw.Experience[Emp_Purchase_raw.Purchase==0], s=100, c='b', marker=\"o\", label='Purchase 0')\n","ax1.scatter(Emp_Purchase_raw.Age[Emp_Purchase_raw.Purchase==1],Emp_Purchase_raw.Experience[Emp_Purchase_raw.Purchase==1], s=100, c='r', marker=\"x\", label='Purchase 1')\n","ax1.set_xlabel('Age',fontsize=15)\n","ax1.set_ylabel('Experience',fontsize=15)\n","\n","plt.xlim(min(Emp_Purchase_raw.Age), max(Emp_Purchase_raw.Age))\n","plt.ylim(min(Emp_Purchase_raw.Experience), max(Emp_Purchase_raw.Experience))\n","\n","x_min, x_max = ax1.get_xlim()\n","ax1.plot([0, x_max], [intercept1, x_max*slope1+intercept1],linewidth=4)\n","ax1.plot([0, x_max], [intercept2, x_max*slope2+intercept2],linewidth=4)\n","\n","plt.legend(loc='upper left');\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KvBgOru2cpp9"},"source":["From the output, we can see that the two models in the layer-1 do an excellent job of finding the\n","overall boundaries between the two classes. That completes our final point “We used layered\n","modeling approach and created intermediate outputs to solve the problem of non-linear data or a\n","dataset with multiple decision boundaries.”"]},{"cell_type":"markdown","metadata":{"id":"_iKxnuhq2vRt"},"source":["## Neural Network algorithm\n","* Step-1 Random Initialization\n","* Step-2 Activation and Feed Forward\n","* Step-3 Error Calculation and Back Propagation\n","* Step-4: Weights updating\n","* Step-5: Stopping Criteria\n"]},{"cell_type":"markdown","metadata":{"id":"UnZ6GwkK3bwE"},"source":["## Gradient Descent\n"]},{"cell_type":"code","metadata":{"id":"wIppO70K4Yvg"},"source":["def lr_gd(X, y, w1, w0, learning_rate, epochs):\n","     for i in range(epochs):\n","          y_pred = (w1 * X) + w0\n","          error = sum([k**2 for k in (y-y_pred)])\n","          \n","          ##Gradients\n","          w0_gradient = -sum(y - y_pred)\n","          w1_gradient = -sum(X * (y - y_pred))\n","          \n","          ##Weight Updating\n","          w0 = w0 - (learning_rate * w0_gradient)\n","          w1 = w1 - (learning_rate * w1_gradient)\n","          \n","          print(\"epoch\", i, \"error =>\", round(error,2), \"w0 => \", round(w0,2), \"w1 => \",round(w1,2))\n","     return error, w0, w1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L7mbUdy66y2I"},"source":["We will use the above function to solve this regression problem below. We will generate some\n","random data using the formula . This means that after solving this regression line, we should get the\n","weights as the output. Below is the code for data creation and solving it."]},{"cell_type":"code","metadata":{"id":"TaibFegM4d17"},"source":["x_data=np.random.random(10)\n","y_data= x_data*20 + 10 "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u_CV1yiQ4kIa"},"source":["w0_init=5\n","w1_init=10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q5SRiFeG4mHf"},"source":["lr_gd(X=x_data, y=y_data, w1=w1_init, w0=w0_init, learning_rate=0.01, epochs=600)\t "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q1WWsFNr8F8O"},"source":["We can see from the output that at 600 epochs the error is almost zero and the final weights given\n","by GD are [10.21,19.29]"]},{"cell_type":"markdown","metadata":{"id":"NT-58WvB8wbW"},"source":["## Recognizing Handwritten Digits\n","In this case study, we will take the images of handwritten digits and build an aneural network model\n","that takes the input as these handwritten digits and builds a model to predict the number inside that\n","image.Below is the code for importing an\n","image and printing the pixel values."]},{"cell_type":"code","metadata":{"id":"e7X12S6e4ryU"},"source":["#x=plt.imread(r'/content/drive/My Drive/DataSets/Chapter-8/Chapter-8/datasets/Sample_images/Marketvegetables.jpg')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GqZINQYKAlA5"},"source":["#Image importing\n","import matplotlib.pyplot as plt\n","import urllib.request  \n","\n","#read the image\n","urllib.request.urlretrieve((github_link+\"/Sample_images/Marketvegetables.jpg\"), \"Marketvegetables.jpg\")\n","x=plt.imread('Marketvegetables.jpg')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lMpIcItq0uQf"},"source":["plt.rcParams[\"figure.figsize\"] = (12,8)\n","plt.imshow(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5YH-1Ozv0wyn"},"source":["print('Shape of the image',x.shape) \n","print(x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VldwNh3G9qiB"},"source":["From the output, we can see that the image has 2400 rows; those are the number of pixels in the\n","height of the image. One thousand six hundred columns are the number of pixels in width. The\n","depth is 3, and this number three corresponds to RGB intensities. If we print the pixel values of the\n","image, we can see each row by row values and in each cell, there are three numbers between 0 and\n","255.\n","\n","In our dataset, we are considering grayscale images. The grayscale images have length and width but\n","they do not have three values in depth. There will be one number in the color dimension for\n","greyscale images. The objective in this case study is to take the grayscale image as input and predict\n","the number inside it using the neural network model. Taking images as input indeed means that\n","taking pixel intensity numbers as input to predict the numbers."]},{"cell_type":"markdown","metadata":{"id":"ZJWKRtfn_prE"},"source":["### Data\n","The data we are considering here has 16X16 pixels. Overall, 256 pixels in each image. Small size\n","image still we have to work with 256 input variables. In our model, we will have 256 input variables .\n","\n","Our data is a standard dataset known as USPS data; it stands for United Staes Postal Services data.\n","This data contains 7,291 scanned images of handwritten digits, and these images are converted to\n","CSV files by taking the pixel intensities.\n","\n","AT&amp;T research labs shared the USPS data set. The other standard digits dataset is MNIST data with\n","28X28 pixels and 60,000 records. We will use this data later. We will use USPS data in this case\n","study. We can download these datasets from Dr. Yann Le Cunn website\n","http://yann.lecun.com/exdb/mnist/\n","Below code helps in importing the data"]},{"cell_type":"markdown","metadata":{"id":"gsOtIEMX_64c"},"source":["USPS Data importing"]},{"cell_type":"code","metadata":{"id":"RqRGjk0W00-X"},"source":["#digits_data_raw = np.loadtxt(r\"/content/drive/My Drive/DataSets/Chapter-8/Chapter-8/datasets/USPS/USPS_train.txt\")\n","digits_data_raw = np.loadtxt(github_link+\"/USPS/USPS_train.txt\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aevimbJ2_9P6"},"source":["Input data is in nparry format. we convert it into dataframe for better handling\n"]},{"cell_type":"code","metadata":{"id":"9J7so7IP1AxQ"},"source":["digits_data=pd.DataFrame(digits_data_raw)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QKWB7-9N1kiu"},"source":["print(digits_data.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ERA2Bkxj1oo7"},"source":["print(digits_data.head())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TqIKjWUSANNV"},"source":["As expected, there are 256-pixel values. The extra column is the target column — the label\n","associated with each image. For example, the label for the image in the first row is 6. The labels are\n","stored in the first column. The code below gives us the frequency of each label in the dataset."]},{"cell_type":"code","metadata":{"id":"KXZzaIUl1rDp"},"source":["print(digits_data[0:][0].value_counts())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PshF-DKVA2u-"},"source":["From the above output, we can see that there are more than 1000 combinations of image 0 similarly\n","around 1000 combinations of image-1., We will now draw a few images. Let us see few images.\n","While creating this dataset, the 16X16 image is flattened to make it as a single row in the data. We\n","need to take a row from this data and build the pixel matrix of size 16X16. The below code helps us\n","in drawing images."]},{"cell_type":"markdown","metadata":{"id":"uGHrwXRYA8x2"},"source":["First image"]},{"cell_type":"code","metadata":{"id":"MLBvNrVw2DfY"},"source":["i=0\n","data_row=digits_data_raw[i][1:]\n","pixels = np.matrix(data_row)\n","pixels=pixels.reshape(16,16)\n","plt.title([\"Row number \", i] , fontsize=20)\n","plt.imshow(pixels, cmap='Greys')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hJSAaOkCBASX"},"source":["Second image"]},{"cell_type":"code","metadata":{"id":"VpBgfTIN2Hx0"},"source":["i=1\n","data_row=digits_data_raw[i][1:]\n","pixels = np.matrix(data_row)\n","pixels=pixels.reshape(16,16)\n","plt.title([\"Row number \", i] , fontsize=20)\n","plt.imshow(pixels, cmap='Greys')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NGPoBD5jBIe_"},"source":["In the above code, ‘i’ is the row number. By changing the value row number(i) in the above code, we\n","can draw a few more images."]},{"cell_type":"code","metadata":{"id":"dr_3-tBF2KxV"},"source":["i=5000 \n","data_row=digits_data_raw[i][1:]\n","pixels = np.matrix(data_row)\n","pixels=pixels.reshape(16,16)\n","plt.title([\"Row number \", i] , fontsize=20)\n","plt.imshow(pixels, cmap='Greys')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q_lHeFbtBUp3"},"source":["We will now prepare the data for model building"]},{"cell_type":"markdown","metadata":{"id":"UZvfvypWBZ2z"},"source":["Train and Test data creation"]},{"cell_type":"code","metadata":{"id":"AI53giAX2NTP"},"source":["X=digits_data.drop(digits_data.columns[[0]], axis=1)\n","y=digits_data[0:][0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HnfxWZd_3Qsc"},"source":["X_train, X_test, y_train, y_test = train_test_split(X, y ,test_size=0.2, random_state=33)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-WVtt3PEBe83"},"source":["Shape of the data"]},{"cell_type":"code","metadata":{"id":"K5sjzx8Z3TG6"},"source":["print(\"X_train shape\", X_train.shape)\n","print(\"y_train shape\", y_train.shape)\n","print(\"X_test shape\", X_test.shape)\n","print(\"y_test shape\", y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"afTP7nKFBj0g"},"source":["### Model building\n","Before building the model, we need an essential data transformation. Till now, we discussed just one\n","binary output in our target variable. Here our output is not binary. There are ten classes in our\n","output. An image can contain any number between [0,9]. The output is multi-class and we are going\n","to build a multi-class classification model. We can extend our binary target classification concept to\n","multiclass classification.\n","\n","There are ten classes in the target, and we are going to create one binary variable for each class. It is a\n","simple one-hot encoding on the target variable. The below code helps us in creating the one-hot\n","encoded variables for the target."]},{"cell_type":"markdown","metadata":{"id":"qcwFMdgdCrSv"},"source":["Creating multiple binary columns for multiple outputs"]},{"cell_type":"code","metadata":{"id":"g7bO5uMx3WhV"},"source":["digit_labels=pd.DataFrame()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9mBN5hV8Cv7W"},"source":["Convert target into onehot encoding"]},{"cell_type":"code","metadata":{"id":"z4jAraU_3zZc"},"source":["digit_labels = pd.get_dummies(y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rt0jYFn8C3of"},"source":["see our newly created labels data"]},{"cell_type":"code","metadata":{"id":"FDY9Omey31Mz"},"source":["digit_labels.head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K2yRc-s9DGz6"},"source":["In the above output we can see ten newly created binary variables derived from one target column.\n","Since there are ten nodes in the output layer, for each new data point, we will get ten predicted\n","values. We will assign the class that has the maximum probability. We now have the input layer with\n","256 nodes and an output layer with ten nodes. We are now ready to build the model. Before\n","building the neural network model, we need to create a list with minimum and maximum values of\n","all the input variables. We are going to use this list later on in the model. Below is the code for\n","creating a list with 256 pairs of values one pair for each of the input variables."]},{"cell_type":"markdown","metadata":{"id":"wCajST4lDfCV"},"source":["getting minimum and maximum of each column of x_train into a list"]},{"cell_type":"code","metadata":{"id":"gKGLSICp33C9"},"source":["min_max_all_cols=[[X_train[i][0:].min(), X_train[i][0:].max()] for i in range(1,X_train.shape[1]+1)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z43MaWCz36KR"},"source":["print(len(min_max_all_cols))\n","print(min_max_all_cols)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iJuqLOmADpMU"},"source":["Below is the code for building the neural network model.\n","\n","**Configure the network**"]},{"cell_type":"code","metadata":{"id":"W-Le6mhC38qI"},"source":["import neurolab as nl\n","net = nl.net.newff(minmax=min_max_all_cols,size=[20,10],transf=[nl.trans.LogSig()]*2)\n","#Training method is Resilient Backpropagation method\n","net.trainf = nl.train.train_rprop "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AghgS-GtDwSw"},"source":["**Train the network**"]},{"cell_type":"code","metadata":{"id":"o-y4l-vy3_w0"},"source":["net.train(X_train, digit_labels, show=1, epochs=300)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NQpIApueD6lD"},"source":["**Code explanation**\n","\n","**newff():** Function to configure neural networks\n","minmax: This parameter takes a list of lists as input. We need to supply\n","the minimum and maximum value of all the input variables. This helps the algorithm in weights initilialization\n","\n","**size=[20,10]:** Takes a list as input\n","Mention nodes in each layer except the input layer\n","[Nodes in hidden layer1, Nodes in hidden layer2,…, Nodes in hidden layer k, Nodes in output layer]\n","[20,10] – One hidden layer with 20 nodes and an output layer with ten nodes\n","\n","**Transf= nl.trans.LogSig():** ‘Transf’ is the activation function parameter LogSig() is the sigmoid function standard syntax. For regression output we can use “PureLin”\n","\n","**[nl.trans.LogSig()]*2:** **2 denotes the two activations from input to hidden, hidden to\n","output in this case. If we have two hidden layers, then we need to mention*3 \n","\n","**net.train:** Function to fit the model. Takes training data as input\n","**show=1:** show=1 – Shows error value in each epoch.\n","show=0 - Builds the model directly. Epochs do not show errors epochs The number of epochs. One epoch is one full run of the data. Mention epochs as a number between 50-500"]},{"cell_type":"markdown","metadata":{"id":"Utjxa-xsFXWB"},"source":["We can now print the model. The model is nothing but a set of weights. Below is the code for\n","fetching the weights from the model."]},{"cell_type":"code","metadata":{"id":"n0XOtC0U4Buz"},"source":["print(net.layers[0].np['w'])\n","print(net.layers[0].np['b'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FRmCygcxOV4H"},"source":["print(net.layers[1].np['w'])\n","print(net.layers[1].np['b'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AS-ykG5CFly0"},"source":["The code below gives us an idea of the count of weights."]},{"cell_type":"code","metadata":{"id":"L6zljiXyOYiI"},"source":["print(net.layers[0].np['w'].shape)\n","print(net.layers[0].np['b'].shape)\n","print(net.layers[1].np['w'].shape)\n","print(net.layers[1].np['b'].shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j-9TU10kFrIi"},"source":["The model building is completed. We have to use these weights to get the predictions for new data\n","points"]},{"cell_type":"markdown","metadata":{"id":"HIjY3J2ZGe46"},"source":["### Deciding hidden nodes\n","There are several hyperparameters in the neural network model. The most impactful\n","hyperparameter is the number of hidden nodes. If this number is too high, then the model will be\n","overfitted. If this is too low, then the model will be under fitted. We need to look at the train and\n","test data accuracies to finetune this parameter. We can use a binary search approach to finetune\n","this parameter."]},{"cell_type":"markdown","metadata":{"id":"H4DXwDCUGol4"},"source":["### Model prediction and tree validation\n","Each new data point in the test data gives us ten probabilities, one probability for each digit. We will\n","take the final result as the digit with maximum probability."]},{"cell_type":"markdown","metadata":{"id":"-CSu5V5xGvwp"},"source":["Prediction on test data"]},{"cell_type":"code","metadata":{"id":"33V28xz5Obmv"},"source":["predicted_values = net.sim(X_test)\n","predicted=pd.DataFrame(predicted_values)\n","print(round(predicted.head(10),3))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s88012N8G0yf"},"source":["Converting predicted probabilitis into numbers"]},{"cell_type":"code","metadata":{"id":"wE956COcOd2S"},"source":["predicted_number=predicted.idxmax(axis=1)\n","print(predicted_number.head(15))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1fjRRvItHB6W"},"source":["In the above output, we can see ten probabilities for each data point. We convert them to single\n","dights based on class with the highest probability. For example, the first record has a 0.998\n","probability for class-6. Last data point as 1.0 probability for class-3. We can now create the\n","confusion matrix and calculate the accuracy."]},{"cell_type":"code","metadata":{"id":"xLdb3iqeluWU"},"source":["ConfusionMatrix = cm(y_test,predicted_number)\n","print(\"ConfusionMatrix on test data \\n\", ConfusionMatrix)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S7qKVx1Tlz77"},"source":["accuracy=np.trace(ConfusionMatrix)/sum(sum(ConfusionMatrix))\n","print(\"Test Accuracy\", accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QWu0CyrcHIVJ"},"source":["We can see that the accuracy of the test data is 83%. Let us use this model to get some predictions\n","on a few data points from test data."]},{"cell_type":"code","metadata":{"id":"L_ik_Ox5l3Zk"},"source":["i=623\n","random_sampel_data=digits_data_raw[[i]]\n","random_sampel_data1=pd.DataFrame(random_sampel_data)\n","X_sample=random_sampel_data1.drop(random_sampel_data1.columns[[0]], axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JxcHNwcVl-Q4"},"source":["predicted_values = net.sim(X_sample)\n","predicted=pd.DataFrame(predicted_values)\n","predicted_number=predicted.idxmax(axis=1)\n","predicted_number"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3vAPyNLImBDJ"},"source":["data_row=random_sampel_data[0][1:]\n","pixels = np.matrix(data_row)\n","pixels=pixels.reshape(16,16)\n","plt.rcParams[\"figure.figsize\"] = (7,5)\n","plt.title([\"Row = \", i, \"Prediction Digit \", predicted_number[0]], fontsize=20)\n","plt.imshow(pixels, cmap='Greys')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NPZm0nmEHRvN"},"source":["The above code randomly takes a point from the data and gives us the predicted values."]}]}